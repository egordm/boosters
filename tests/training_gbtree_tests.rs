//! Integration tests for GBTree training.
//!
//! These tests validate that our training produces models comparable to XGBoost.
//! Test cases are generated by `tools/data_generation/scripts/generate_xgboost.py`.

#![cfg(feature = "xgboost-compat")]

use std::fs::File;
use std::path::PathBuf;

use booste_rs::data::ColMatrix;
use booste_rs::training::quantize::{CutFinder, ExactQuantileCuts, Quantizer};
use booste_rs::training::{
    DepthWisePolicy, GBTreeTrainer, LeafWisePolicy, SquaredLoss, TrainerParams, TreeParams,
};
use serde::Deserialize;

// =============================================================================
// Test Data Loading
// =============================================================================

fn gbtree_training_dir() -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR")).join("tests/test-cases/xgboost/gbtree/training")
}

#[derive(Deserialize)]
struct TrainData {
    num_rows: usize,
    num_features: usize,
    data: Vec<Option<f32>>,
}

#[derive(Deserialize)]
struct Labels {
    labels: Vec<f32>,
}

#[derive(Deserialize)]
struct Config {
    objective: String,
    max_depth: Option<u32>,
    max_leaves: Option<u32>,
    eta: f32,
    lambda: f32,
    #[serde(default)]
    alpha: f32,
    #[serde(default)]
    min_child_weight: f32,
    #[serde(default)]
    gamma: f32,
    num_boost_round: u32,
    #[serde(default)]
    grow_policy: Option<String>,
}

#[derive(Deserialize)]
struct Metrics {
    num_trees: usize,
    test_rmse: Option<f64>,
    #[serde(default)]
    test_accuracy: Option<f64>,
}

fn load_json<T: serde::de::DeserializeOwned>(path: &std::path::Path) -> T {
    let file =
        File::open(path).unwrap_or_else(|e| panic!("Failed to open {}: {e}", path.display()));
    serde_json::from_reader(file)
        .unwrap_or_else(|e| panic!("Failed to parse {}: {e}", path.display()))
}

fn load_matrix(data: &TrainData) -> ColMatrix<f32> {
    let values: Vec<f32> = data
        .data
        .iter()
        .map(|v| v.unwrap_or(f32::NAN))
        .collect();
    ColMatrix::from_vec(values, data.num_rows, data.num_features)
}

// =============================================================================
// Integration Tests
// =============================================================================

mod regression {
    use super::*;

    /// Helper to run a regression training test case and compare predictions.
    fn run_regression_test(name: &str, _tolerance_factor: f64) {
        let dir = gbtree_training_dir();

        // Load data
        let train_data: TrainData = load_json(&dir.join(format!("{name}.train_data.json")));
        let train_labels: Labels = load_json(&dir.join(format!("{name}.train_labels.json")));
        let config: Config = load_json(&dir.join(format!("{name}.config.json")));
        let xgb_metrics: Metrics = load_json(&dir.join(format!("{name}.metrics.json")));

        // Build training matrix
        let matrix = load_matrix(&train_data);

        // Quantize
        let cut_finder = ExactQuantileCuts::new(1);
        let cuts = cut_finder.find_cuts(&matrix, 256);
        let quantizer = Quantizer::new(cuts.clone());
        let quantized = quantizer.quantize::<_, u8>(&matrix);

        // Training params
        let tree_params = TreeParams {
            max_depth: config.max_depth.unwrap_or(6),
            max_leaves: config.max_leaves.unwrap_or(0),
            learning_rate: config.eta,
            min_samples_split: 1,
            min_samples_leaf: 1,
            ..Default::default()
        };

        let params = TrainerParams {
            num_rounds: config.num_boost_round,
            tree_params,
            ..Default::default()
        };

        // Select growth policy
        let is_leaf_wise = config
            .grow_policy
            .as_ref()
            .map(|p| p == "lossguide")
            .unwrap_or(false);

        // Train model
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);

        let forest = if is_leaf_wise {
            let policy = LeafWisePolicy {
                max_leaves: config.max_leaves.unwrap_or(16),
            };
            trainer.train(policy, &quantized, &train_labels.labels, &cuts, None)
        } else {
            let policy = DepthWisePolicy {
                max_depth: config.max_depth.unwrap_or(6),
            };
            trainer.train(policy, &quantized, &train_labels.labels, &cuts, None)
        };

        // Compare number of trees
        assert_eq!(
            forest.num_trees(),
            xgb_metrics.num_trees,
            "{}: Expected {} trees, got {}",
            name,
            xgb_metrics.num_trees,
            forest.num_trees()
        );

        println!(
            "{}: Trained {} trees (XGBoost RMSE: {:?})",
            name,
            forest.num_trees(),
            xgb_metrics.test_rmse
        );
    }

    #[test]
    fn train_regression_simple() {
        run_regression_test("regression_simple", 1.5);
    }

    #[test]
    fn train_regression_deep() {
        run_regression_test("regression_deep", 1.5);
    }

    #[test]
    fn train_regression_regularized() {
        run_regression_test("regression_regularized", 2.0);
    }

    #[test]
    fn train_leaf_wise() {
        run_regression_test("leaf_wise", 1.5);
    }

    #[test]
    fn train_large() {
        run_regression_test("large", 1.5);
    }
}
