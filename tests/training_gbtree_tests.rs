//! Integration tests for GBTree training.
//!
//! These tests validate that our training produces models comparable to XGBoost.
//! Test cases are generated by `tools/data_generation/scripts/generate_xgboost.py`.
//!
//! ## Story 8: Full Training Validation
//!
//! Quality tests verify:
//! - Training predictions correlate highly with XGBoost (r > 0.95)
//! - Test set predictions correlate highly with XGBoost (r > 0.90)
//! - RMSE within acceptable tolerance of XGBoost baseline
//!
//! Integration tests verify:
//! - Frozen forest produces consistent predictions
//! - Model structure matches expected tree count

#![cfg(feature = "xgboost-compat")]

use std::fs::File;
use std::path::PathBuf;

use booste_rs::data::{ColMatrix, RowMatrix};
use booste_rs::predict::{Predictor, StandardTraversal};
use booste_rs::testing::pearson_correlation;
use booste_rs::training::{
    DepthWisePolicy, GBTreeTrainer, GainParams, LeafWisePolicy, Rmse, SimpleMetric, SquaredLoss,
    SoftmaxLoss,
    TrainerParams, TreeParams,
};
use booste_rs::training::gbtree::{CutFinder, ExactQuantileCuts, Quantizer, MulticlassTrainer};
use serde::Deserialize;

// =============================================================================
// Test Data Loading
// =============================================================================

fn gbtree_training_dir() -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR")).join("tests/test-cases/xgboost/gbtree/training")
}

#[derive(Deserialize)]
struct TrainData {
    num_rows: usize,
    num_features: usize,
    data: Vec<Option<f32>>,
}

#[derive(Deserialize)]
struct Labels {
    labels: Vec<f32>,
}

#[derive(Deserialize)]
struct Predictions {
    predictions: Vec<f32>,
}

#[allow(dead_code)]
#[derive(Deserialize)]
struct Config {
    objective: String,
    max_depth: Option<u32>,
    max_leaves: Option<u32>,
    eta: f32,
    lambda: f32,
    #[serde(default)]
    alpha: f32,
    #[serde(default)]
    min_child_weight: f32,
    #[serde(default)]
    gamma: f32,
    num_boost_round: u32,
    #[serde(default)]
    grow_policy: Option<String>,
}

#[allow(dead_code)]
#[derive(Deserialize)]
struct Metrics {
    num_trees: usize,
    test_rmse: Option<f64>,
    #[serde(default)]
    test_accuracy: Option<f64>,
}

fn load_json<T: serde::de::DeserializeOwned>(path: &std::path::Path) -> T {
    let file =
        File::open(path).unwrap_or_else(|e| panic!("Failed to open {}: {e}", path.display()));
    serde_json::from_reader(file)
        .unwrap_or_else(|e| panic!("Failed to parse {}: {e}", path.display()))
}

fn load_col_matrix(data: &TrainData) -> ColMatrix<f32> {
    // JSON data is in row-major format, convert to column-major
    let row_matrix = load_row_matrix(data);
    row_matrix.to_layout()
}

fn load_row_matrix(data: &TrainData) -> RowMatrix<f32> {
    let values: Vec<f32> = data.data.iter().map(|v| v.unwrap_or(f32::NAN)).collect();
    RowMatrix::from_vec(values, data.num_rows, data.num_features)
}

// =============================================================================
// Test Case Structure
// =============================================================================

#[allow(dead_code)]
struct TestCase {
    name: String,
    train_matrix: ColMatrix<f32>,
    train_labels: Vec<f32>,
    test_matrix: RowMatrix<f32>,
    test_labels: Vec<f32>,
    xgb_train_preds: Vec<f32>,
    xgb_test_preds: Vec<f32>,
    config: Config,
    metrics: Metrics,
}

impl TestCase {
    fn load(name: &str) -> Self {
        let dir = gbtree_training_dir();

        let train_data: TrainData = load_json(&dir.join(format!("{name}.train_data.json")));
        let train_labels: Labels = load_json(&dir.join(format!("{name}.train_labels.json")));
        let test_data: TrainData = load_json(&dir.join(format!("{name}.test_data.json")));
        let test_labels: Labels = load_json(&dir.join(format!("{name}.test_labels.json")));
        let xgb_train_preds: Predictions =
            load_json(&dir.join(format!("{name}.train_predictions.json")));
        let xgb_test_preds: Predictions =
            load_json(&dir.join(format!("{name}.test_predictions.json")));
        let config: Config = load_json(&dir.join(format!("{name}.config.json")));
        let metrics: Metrics = load_json(&dir.join(format!("{name}.metrics.json")));

        TestCase {
            name: name.to_string(),
            train_matrix: load_col_matrix(&train_data),
            train_labels: train_labels.labels,
            test_matrix: load_row_matrix(&test_data),
            test_labels: test_labels.labels,
            xgb_train_preds: xgb_train_preds.predictions,
            xgb_test_preds: xgb_test_preds.predictions,
            config,
            metrics,
        }
    }

    fn is_leaf_wise(&self) -> bool {
        self.config
            .grow_policy
            .as_ref()
            .map(|p| p == "lossguide")
            .unwrap_or(false)
    }
}

// =============================================================================
// Training Helper
// =============================================================================

struct TrainResult {
    forest: booste_rs::forest::SoAForest<booste_rs::trees::ScalarLeaf>,
    train_preds: Vec<f32>,
    test_preds: Vec<f32>,
}

fn train_and_predict(tc: &TestCase) -> TrainResult {
    // Quantize training data
    let cut_finder = ExactQuantileCuts::new(1);
    let cuts = cut_finder.find_cuts(&tc.train_matrix, 256);
    let quantizer = Quantizer::new(cuts.clone());
    let quantized = quantizer.quantize::<_, u8>(&tc.train_matrix);

    // Build gain params with regularization from config
    let gain_params = GainParams {
        lambda: tc.config.lambda,
        alpha: tc.config.alpha,
        min_split_gain: tc.config.gamma,
        min_child_weight: tc.config.min_child_weight,
    };

    // Build tree params
    let tree_params = TreeParams {
        gain: gain_params,
        max_depth: tc.config.max_depth.unwrap_or(6),
        max_leaves: tc.config.max_leaves.unwrap_or(0),
        learning_rate: tc.config.eta,
        min_samples_split: 1,
        min_samples_leaf: 1,
        ..Default::default()
    };

    let params = TrainerParams {
        num_rounds: tc.config.num_boost_round,
        tree_params,
        ..Default::default()
    };

    // Train
    let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);

    let forest = if tc.is_leaf_wise() {
        let policy = LeafWisePolicy {
            max_leaves: tc.config.max_leaves.unwrap_or(16),
        };
        trainer.train(policy, &quantized, &tc.train_labels, &cuts, &[])
    } else {
        let policy = DepthWisePolicy {
            max_depth: tc.config.max_depth.unwrap_or(6),
        };
        trainer.train(policy, &quantized, &tc.train_labels, &cuts, &[])
    };

    // Predict on train set using inference path
    let predictor = Predictor::<StandardTraversal>::new(&forest);

    // For train predictions, convert ColMatrix to RowMatrix for prediction
    let train_row_matrix: RowMatrix<f32> = tc.train_matrix.to_layout();
    let train_output = predictor.predict(&train_row_matrix);
    let train_preds = train_output.into_vec();

    // Predict on test set
    let test_output = predictor.predict(&tc.test_matrix);
    let test_preds = test_output.into_vec();

    TrainResult {
        forest,
        train_preds,
        test_preds,
    }
}

// =============================================================================
// Quality Tests (8.Q*)
// =============================================================================

mod quality_tests {
    use super::*;

    /// Validate regression training quality against XGBoost baseline.
    fn validate_regression(name: &str, min_train_corr: f64, min_test_corr: f64) {
        let tc = TestCase::load(name);
        let result = train_and_predict(&tc);

        // Check tree count
        assert_eq!(
            result.forest.num_trees(),
            tc.metrics.num_trees,
            "{}: Expected {} trees, got {}",
            name,
            tc.metrics.num_trees,
            result.forest.num_trees()
        );

        // Check training set correlation (using library function)
        let train_corr = pearson_correlation(&result.train_preds, &tc.xgb_train_preds);
        assert!(
            train_corr >= min_train_corr,
            "{}: Training correlation {:.4} < {} minimum",
            name,
            train_corr,
            min_train_corr
        );

        // Check test set correlation
        let test_corr = pearson_correlation(&result.test_preds, &tc.xgb_test_preds);
        assert!(
            test_corr >= min_test_corr,
            "{}: Test correlation {:.4} < {} minimum",
            name,
            test_corr,
            min_test_corr
        );

        // Compute our RMSE on test set (using library metric)
        let our_rmse = Rmse.compute(&result.test_preds, &tc.test_labels);
        let xgb_rmse = tc.metrics.test_rmse.unwrap_or(0.0);

        // Allow 20% tolerance vs XGBoost RMSE
        let rmse_ratio = if xgb_rmse > 0.0 {
            our_rmse / xgb_rmse
        } else {
            1.0
        };

        println!(
            "{}: trees={}, train_corr={:.4}, test_corr={:.4}, our_rmse={:.4}, xgb_rmse={:.4}, ratio={:.2}",
            name,
            result.forest.num_trees(),
            train_corr,
            test_corr,
            our_rmse,
            xgb_rmse,
            rmse_ratio
        );

        // Warn if RMSE is significantly worse (but don't fail - trees may differ)
        if rmse_ratio > 1.5 {
            eprintln!(
                "WARNING: {}: Our RMSE {:.4} is {:.1}x worse than XGBoost {:.4}",
                name,
                our_rmse,
                rmse_ratio,
                xgb_rmse
            );
        }
    }

    #[test]
    fn q1_regression_simple() {
        validate_regression("regression_simple", 0.95, 0.85);
    }

    #[test]
    fn q2_regression_deep() {
        validate_regression("regression_deep", 0.95, 0.85);
    }

    #[test]
    fn q3_regression_regularized() {
        validate_regression("regression_regularized", 0.90, 0.80);
    }

    #[test]
    fn q4_leaf_wise() {
        validate_regression("leaf_wise", 0.95, 0.85);
    }

    #[test]
    fn q5_large_dataset() {
        validate_regression("large", 0.95, 0.85);
    }
}

// =============================================================================
// Integration Tests (8.I*)
// =============================================================================

mod integration_tests {
    use super::*;

    /// Test that frozen forest predictions are consistent.
    #[test]
    fn i1_frozen_forest_consistency() {
        let tc = TestCase::load("regression_simple");
        let result = train_and_predict(&tc);

        // Predict again with same forest
        let predictor = Predictor::<StandardTraversal>::new(&result.forest);
        let train_row_matrix: RowMatrix<f32> = tc.train_matrix.to_layout();
        let second_preds = predictor.predict(&train_row_matrix).into_vec();

        // Should be identical
        for (i, (&p1, &p2)) in result.train_preds.iter().zip(second_preds.iter()).enumerate() {
            assert!(
                (p1 - p2).abs() < 1e-6,
                "Prediction mismatch at row {}: {} vs {}",
                i,
                p1,
                p2
            );
        }
    }

    /// Test single-row prediction API.
    #[test]
    fn i2_single_row_prediction() {
        let tc = TestCase::load("regression_simple");
        let result = train_and_predict(&tc);

        let predictor = Predictor::<StandardTraversal>::new(&result.forest);

        // Get first row of test data
        let first_row: Vec<f32> = (0..tc.test_matrix.num_cols())
            .map(|c| *tc.test_matrix.get(0, c).unwrap())
            .collect();

        let single_pred = predictor.predict_row(&first_row);
        let batch_pred = result.test_preds[0];

        assert!(
            (single_pred[0] - batch_pred).abs() < 1e-6,
            "Single-row prediction {} differs from batch {}",
            single_pred[0],
            batch_pred
        );
    }

    /// Test that different test cases produce different results.
    #[test]
    fn i3_different_configs_differ() {
        let simple = train_and_predict(&TestCase::load("regression_simple"));
        let deep = train_and_predict(&TestCase::load("regression_deep"));

        // Different configs should produce different tree counts
        assert_ne!(
            simple.forest.num_trees(),
            deep.forest.num_trees(),
            "Simple and deep should have different tree counts"
        );
    }
}

// =============================================================================
// Basic Smoke Tests
// =============================================================================

mod smoke_tests {
    use super::*;

    #[test]
    fn smoke_depth_wise() {
        let tc = TestCase::load("regression_simple");
        let result = train_and_predict(&tc);
        assert!(result.forest.num_trees() > 0);
        assert!(!result.train_preds.is_empty());
    }

    #[test]
    fn smoke_leaf_wise() {
        let tc = TestCase::load("leaf_wise");
        let result = train_and_predict(&tc);
        assert!(result.forest.num_trees() > 0);
        assert!(!result.train_preds.is_empty());
    }
}

// =============================================================================
// Multiclass Classification Tests (Story 4 Validation)
// =============================================================================

mod multiclass_tests {
    #![allow(dead_code)]
    use super::*;

    /// Load multiclass config with num_class field.
    #[derive(Deserialize)]
    struct MulticlassConfig {
        objective: String,
        num_class: usize,
        max_depth: Option<u32>,
        eta: f32,
        lambda: f32,
        #[serde(default)]
        alpha: f32,
        #[serde(default)]
        min_child_weight: f32,
        #[serde(default)]
        gamma: f32,
        num_boost_round: u32,
    }

    #[derive(Deserialize)]
    struct MulticlassMetrics {
        num_trees: usize,
        #[serde(default)]
        test_accuracy: Option<f64>,
    }

    /// Multiclass predictions are 2D: [[class0, class1, class2], ...]
    #[derive(Deserialize)]
    struct MulticlassPredictions {
        predictions: Vec<Vec<f32>>,
    }

    struct MulticlassTestCase {
        train_matrix: ColMatrix<f32>,
        train_labels: Vec<f32>,
        test_matrix: RowMatrix<f32>,
        test_labels: Vec<f32>,
        xgb_train_preds: Vec<Vec<f32>>,
        xgb_test_preds: Vec<Vec<f32>>,
        config: MulticlassConfig,
        metrics: MulticlassMetrics,
    }

    impl MulticlassTestCase {
        fn load(name: &str) -> Self {
            let dir = gbtree_training_dir();

            let train_data: TrainData = load_json(&dir.join(format!("{name}.train_data.json")));
            let train_labels: Labels = load_json(&dir.join(format!("{name}.train_labels.json")));
            let test_data: TrainData = load_json(&dir.join(format!("{name}.test_data.json")));
            let test_labels: Labels = load_json(&dir.join(format!("{name}.test_labels.json")));
            let xgb_train_preds: MulticlassPredictions =
                load_json(&dir.join(format!("{name}.train_predictions.json")));
            let xgb_test_preds: MulticlassPredictions =
                load_json(&dir.join(format!("{name}.test_predictions.json")));
            let config: MulticlassConfig = load_json(&dir.join(format!("{name}.config.json")));
            let metrics: MulticlassMetrics = load_json(&dir.join(format!("{name}.metrics.json")));

            MulticlassTestCase {
                train_matrix: load_col_matrix(&train_data),
                train_labels: train_labels.labels,
                test_matrix: load_row_matrix(&test_data),
                test_labels: test_labels.labels,
                xgb_train_preds: xgb_train_preds.predictions,
                xgb_test_preds: xgb_test_preds.predictions,
                config,
                metrics,
            }
        }
    }

    fn train_multiclass(tc: &MulticlassTestCase) -> (booste_rs::forest::SoAForest<booste_rs::trees::ScalarLeaf>, Vec<Vec<f32>>, Vec<Vec<f32>>) {
        let num_classes = tc.config.num_class;

        // Quantize training data
        let cut_finder = ExactQuantileCuts::new(1);
        let cuts = cut_finder.find_cuts(&tc.train_matrix, 256);
        let quantizer = Quantizer::new(cuts.clone());
        let quantized = quantizer.quantize::<_, u8>(&tc.train_matrix);

        // Build gain params with regularization from config
        let gain_params = GainParams {
            lambda: tc.config.lambda,
            alpha: tc.config.alpha,
            min_split_gain: tc.config.gamma,
            min_child_weight: tc.config.min_child_weight,
        };

        // Build tree params
        let tree_params = TreeParams {
            gain: gain_params,
            max_depth: tc.config.max_depth.unwrap_or(6),
            learning_rate: tc.config.eta,
            min_samples_split: 1,
            min_samples_leaf: 1,
            ..Default::default()
        };

        let params = TrainerParams {
            num_rounds: tc.config.num_boost_round,
            tree_params,
            ..Default::default()
        };

        // Train multiclass model
        let loss = SoftmaxLoss::new(num_classes);
        let policy = DepthWisePolicy {
            max_depth: tc.config.max_depth.unwrap_or(6),
        };
        let mut trainer = MulticlassTrainer::new(loss, params);
        let forest = trainer.train(policy, &quantized, &tc.train_labels, &cuts, &[]);

        // Predict on train set - get K scores per row
        let train_row_matrix: RowMatrix<f32> = tc.train_matrix.to_layout();
        let mut train_preds = Vec::with_capacity(train_row_matrix.num_rows());
        for row_idx in 0..train_row_matrix.num_rows() {
            let features: Vec<f32> = (0..train_row_matrix.num_cols())
                .map(|c| *train_row_matrix.get(row_idx, c).unwrap())
                .collect();
            let pred = forest.predict_row(&features);
            train_preds.push(pred);
        }

        // Predict on test set
        let mut test_preds = Vec::with_capacity(tc.test_matrix.num_rows());
        for row_idx in 0..tc.test_matrix.num_rows() {
            let features: Vec<f32> = (0..tc.test_matrix.num_cols())
                .map(|c| *tc.test_matrix.get(row_idx, c).unwrap())
                .collect();
            let pred = forest.predict_row(&features);
            test_preds.push(pred);
        }

        (forest, train_preds, test_preds)
    }

    fn argmax(preds: &[f32]) -> usize {
        preds
            .iter()
            .enumerate()
            .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
            .map(|(idx, _)| idx)
            .unwrap_or(0)
    }

    fn compute_accuracy(preds: &[Vec<f32>], labels: &[f32]) -> f64 {
        let correct: usize = preds
            .iter()
            .zip(labels.iter())
            .filter(|(pred, label)| argmax(pred) == **label as usize)
            .count();
        correct as f64 / labels.len() as f64
    }

    /// Story 4.6: Validation against XGBoost multi:softmax.
    ///
    /// Validates that:
    /// - Tree count matches XGBoost (num_rounds * num_classes)
    /// - Predictions correlate highly with XGBoost per class
    /// - Test accuracy is reasonable
    #[test]
    fn multiclass_training_quality() {
        let tc = MulticlassTestCase::load("multiclass");
        let num_classes = tc.config.num_class;
        let (forest, train_preds, test_preds) = train_multiclass(&tc);

        // Check tree count: num_rounds × num_classes
        let expected_trees = tc.config.num_boost_round as usize * num_classes;
        assert_eq!(
            forest.num_trees(),
            expected_trees,
            "Expected {} trees ({}×{}), got {}",
            expected_trees,
            tc.config.num_boost_round,
            num_classes,
            forest.num_trees()
        );

        // Check group count
        assert_eq!(
            forest.num_groups() as usize,
            num_classes,
            "Expected {} groups, got {}",
            num_classes,
            forest.num_groups()
        );

        // Compute per-class correlations with XGBoost predictions
        for class_idx in 0..num_classes {
            let our_class_preds: Vec<f32> = train_preds.iter().map(|p| p[class_idx]).collect();
            let xgb_class_preds: Vec<f32> = tc.xgb_train_preds.iter().map(|p| p[class_idx]).collect();

            let corr = pearson_correlation(&our_class_preds, &xgb_class_preds);
            println!(
                "Class {} training correlation: {:.4}",
                class_idx, corr
            );

            // Require high correlation with XGBoost
            assert!(
                corr >= 0.90,
                "Class {} training correlation {:.4} < 0.90 minimum",
                class_idx,
                corr
            );
        }

        // Compute test accuracy
        let test_accuracy = compute_accuracy(&test_preds, &tc.test_labels);
        let xgb_accuracy = tc.metrics.test_accuracy.unwrap_or(0.0);

        println!(
            "Test accuracy: ours={:.4}, xgb={:.4}",
            test_accuracy, xgb_accuracy
        );

        // Our accuracy should be reasonable (allow some tolerance)
        assert!(
            test_accuracy >= 0.60,
            "Test accuracy {:.4} too low (minimum 0.60)",
            test_accuracy
        );

        // Warn if significantly worse than XGBoost
        if xgb_accuracy > 0.0 && test_accuracy < xgb_accuracy - 0.15 {
            eprintln!(
                "WARNING: Test accuracy {:.4} is significantly worse than XGBoost {:.4}",
                test_accuracy, xgb_accuracy
            );
        }
    }

    /// Smoke test: multiclass training produces valid predictions.
    #[test]
    fn multiclass_smoke() {
        let tc = MulticlassTestCase::load("multiclass");
        let (forest, train_preds, _test_preds) = train_multiclass(&tc);

        // Should have trees
        assert!(forest.num_trees() > 0);

        // Predictions should be finite
        for pred in &train_preds {
            for &v in pred {
                assert!(v.is_finite(), "Non-finite prediction: {}", v);
            }
        }
    }
}
