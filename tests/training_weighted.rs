//! Integration tests for weighted GBTree training.
//!
//! These tests validate that sample weighting works correctly and produces
//! models comparable to XGBoost when trained with the same weights.
//!
//! Test cases are generated by `tools/data_generation/scripts/generate_xgboost.py`.
//!
//! ## Story 5: XGBoost Compatibility Tests
//!
//! Tests verify:
//! - Training with weights produces predictions that correlate highly with XGBoost
//! - Different weight distributions are handled correctly
//! - Zero-weight samples are effectively excluded
//! - Class imbalance weighting improves minority class performance

#![cfg(feature = "xgboost-compat")]

use std::fs::File;
use std::path::PathBuf;

use booste_rs::data::{ColMatrix, RowMatrix};
use booste_rs::predict::{Predictor, StandardTraversal};
use booste_rs::testing::pearson_correlation;
use booste_rs::training::{GBTreeTrainer, GrowthMode, LossFunction, Verbosity};
use serde::Deserialize;

// =============================================================================
// Test Data Loading
// =============================================================================

fn weighted_training_dir() -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("tests/test-cases/xgboost/gbtree/training/weighted")
}

#[derive(Deserialize)]
struct TrainData {
    num_rows: usize,
    num_features: usize,
    data: Vec<Option<f32>>,
}

#[derive(Deserialize)]
struct Labels {
    labels: Vec<f32>,
}

#[derive(Deserialize)]
struct Weights {
    weights: Vec<f32>,
}

#[derive(Deserialize)]
struct Predictions {
    predictions: Vec<f32>,
}

#[derive(Deserialize)]
struct MulticlassPredictions {
    predictions: Vec<Vec<f32>>,
}

#[allow(dead_code)]
#[derive(Deserialize)]
struct Config {
    objective: String,
    max_depth: Option<u32>,
    eta: f32,
    lambda: f32,
    #[serde(default)]
    alpha: f32,
    #[serde(default)]
    min_child_weight: f32,
    #[serde(default)]
    gamma: f32,
    num_boost_round: u32,
}

#[allow(dead_code)]
#[derive(Deserialize)]
struct Metrics {
    num_trees: usize,
    test_rmse: Option<f64>,
}

fn load_json<T: serde::de::DeserializeOwned>(path: &std::path::Path) -> T {
    let file =
        File::open(path).unwrap_or_else(|e| panic!("Failed to open {}: {e}", path.display()));
    serde_json::from_reader(file)
        .unwrap_or_else(|e| panic!("Failed to parse {}: {e}", path.display()))
}

fn load_col_matrix(data: &TrainData) -> ColMatrix<f32> {
    let row_matrix = load_row_matrix(data);
    row_matrix.to_layout()
}

fn load_row_matrix(data: &TrainData) -> RowMatrix<f32> {
    let values: Vec<f32> = data.data.iter().map(|v| v.unwrap_or(f32::NAN)).collect();
    RowMatrix::from_vec(values, data.num_rows, data.num_features)
}

// =============================================================================
// Test Case Structure
// =============================================================================

#[allow(dead_code)]
struct WeightedTestCase {
    name: String,
    train_matrix: ColMatrix<f32>,
    train_labels: Vec<f32>,
    train_weights: Vec<f32>,
    test_matrix: RowMatrix<f32>,
    test_labels: Vec<f32>,
    test_weights: Vec<f32>,
    xgb_train_preds: Vec<f32>,
    xgb_test_preds: Vec<f32>,
    config: Config,
    metrics: Metrics,
}

impl WeightedTestCase {
    fn load(name: &str) -> Self {
        let dir = weighted_training_dir();

        let train_data: TrainData = load_json(&dir.join(format!("{name}.train_data.json")));
        let train_labels: Labels = load_json(&dir.join(format!("{name}.train_labels.json")));
        let train_weights: Weights = load_json(&dir.join(format!("{name}.train_weights.json")));
        let test_data: TrainData = load_json(&dir.join(format!("{name}.test_data.json")));
        let test_labels: Labels = load_json(&dir.join(format!("{name}.test_labels.json")));
        let test_weights: Weights = load_json(&dir.join(format!("{name}.test_weights.json")));
        let xgb_train_preds: Predictions =
            load_json(&dir.join(format!("{name}.train_predictions.json")));
        let xgb_test_preds: Predictions =
            load_json(&dir.join(format!("{name}.test_predictions.json")));
        let config: Config = load_json(&dir.join(format!("{name}.config.json")));
        let metrics: Metrics = load_json(&dir.join(format!("{name}.metrics.json")));

        WeightedTestCase {
            name: name.to_string(),
            train_matrix: load_col_matrix(&train_data),
            train_labels: train_labels.labels,
            train_weights: train_weights.weights,
            test_matrix: load_row_matrix(&test_data),
            test_labels: test_labels.labels,
            test_weights: test_weights.weights,
            xgb_train_preds: xgb_train_preds.predictions,
            xgb_test_preds: xgb_test_preds.predictions,
            config,
            metrics,
        }
    }

    fn loss_function(&self) -> LossFunction {
        match self.config.objective.as_str() {
            "reg:squarederror" => LossFunction::SquaredError,
            "binary:logistic" => LossFunction::Logistic,
            obj if obj.starts_with("multi:softprob") => {
                // Count unique classes in labels
                let num_classes = self
                    .train_labels
                    .iter()
                    .map(|l| *l as usize)
                    .max()
                    .unwrap_or(0)
                    + 1;
                LossFunction::Softmax { num_classes }
            }
            obj => panic!("Unknown objective: {}", obj),
        }
    }
}

// =============================================================================
// Training Helper
// =============================================================================

struct TrainResult {
    forest: booste_rs::forest::SoAForest<booste_rs::trees::ScalarLeaf>,
    train_preds: Vec<f32>,
    test_preds: Vec<f32>,
}

fn train_and_predict(tc: &WeightedTestCase) -> TrainResult {
    let trainer = GBTreeTrainer::builder()
        .loss(tc.loss_function())
        .num_rounds(tc.config.num_boost_round)
        .growth_mode(GrowthMode::DepthWise)
        .max_depth(tc.config.max_depth.unwrap_or(6))
        .learning_rate(tc.config.eta)
        .reg_lambda(tc.config.lambda)
        .reg_alpha(tc.config.alpha)
        .min_split_gain(tc.config.gamma)
        .min_child_weight(tc.config.min_child_weight)
        .verbosity(Verbosity::Silent)
        .build()
        .unwrap();

    // Train with weights
    let forest = trainer.train(
        &tc.train_matrix,
        &tc.train_labels,
        Some(&tc.train_weights),
        &[],
    );

    // Predict on train set
    let predictor = Predictor::<StandardTraversal>::new(&forest);
    let train_row_matrix: RowMatrix<f32> = tc.train_matrix.to_layout();
    let train_output = predictor.predict(&train_row_matrix);
    let train_preds = train_output.into_vec();

    // Predict on test set
    let test_output = predictor.predict(&tc.test_matrix);
    let test_preds = test_output.into_vec();

    TrainResult {
        forest,
        train_preds,
        test_preds,
    }
}

// =============================================================================
// Story 5: XGBoost Compatibility Tests
// =============================================================================

mod weighted_compatibility_tests {
    use super::*;

    /// Validate weighted training produces predictions correlating with XGBoost.
    fn validate_weighted_case(name: &str, min_train_corr: f64, min_test_corr: f64) {
        let tc = WeightedTestCase::load(name);
        let result = train_and_predict(&tc);

        // Check tree count
        assert_eq!(
            result.forest.num_trees(),
            tc.metrics.num_trees,
            "{}: Expected {} trees, got {}",
            name,
            tc.metrics.num_trees,
            result.forest.num_trees()
        );

        // Check training set correlation
        let train_corr = pearson_correlation(&result.train_preds, &tc.xgb_train_preds);
        assert!(
            train_corr >= min_train_corr,
            "{}: Training correlation {:.4} < {} minimum",
            name,
            train_corr,
            min_train_corr
        );

        // Check test set correlation
        let test_corr = pearson_correlation(&result.test_preds, &tc.xgb_test_preds);
        assert!(
            test_corr >= min_test_corr,
            "{}: Test correlation {:.4} < {} minimum",
            name,
            test_corr,
            min_test_corr
        );

        println!(
            "{}: trees={}, train_corr={:.4}, test_corr={:.4}",
            name,
            result.forest.num_trees(),
            train_corr,
            test_corr
        );
    }

    /// 5.2: Weighted regression
    #[test]
    fn weighted_regression() {
        validate_weighted_case("weighted_regression", 0.90, 0.80);
    }

    /// 5.3: Weighted binary classification
    #[test]
    fn weighted_binary() {
        validate_weighted_case("weighted_binary", 0.90, 0.80);
    }

    /// 5.4: Weighted multiclass classification
    #[test]
    fn weighted_multiclass() {
        let dir = weighted_training_dir();
        let name = "weighted_multiclass";

        let train_data: TrainData = load_json(&dir.join(format!("{name}.train_data.json")));
        let train_labels: Labels = load_json(&dir.join(format!("{name}.train_labels.json")));
        let train_weights: Weights = load_json(&dir.join(format!("{name}.train_weights.json")));
        let test_data: TrainData = load_json(&dir.join(format!("{name}.test_data.json")));
        let config: Config = load_json(&dir.join(format!("{name}.config.json")));
        let metrics: Metrics = load_json(&dir.join(format!("{name}.metrics.json")));

        // Load multiclass predictions (nested arrays)
        let xgb_train_preds: MulticlassPredictions =
            load_json(&dir.join(format!("{name}.train_predictions.json")));
        let xgb_test_preds: MulticlassPredictions =
            load_json(&dir.join(format!("{name}.test_predictions.json")));

        // Flatten XGBoost predictions
        let xgb_train_flat: Vec<f32> = xgb_train_preds.predictions.into_iter().flatten().collect();
        let xgb_test_flat: Vec<f32> = xgb_test_preds.predictions.into_iter().flatten().collect();

        let train_matrix = load_col_matrix(&train_data);
        let test_matrix = load_row_matrix(&test_data);

        // Count classes from labels
        let num_classes = train_labels
            .labels
            .iter()
            .map(|l| *l as usize)
            .max()
            .unwrap_or(0)
            + 1;

        let trainer = GBTreeTrainer::builder()
            .loss(LossFunction::Softmax { num_classes })
            .num_rounds(config.num_boost_round)
            .growth_mode(GrowthMode::DepthWise)
            .max_depth(config.max_depth.unwrap_or(6))
            .learning_rate(config.eta)
            .reg_lambda(config.lambda)
            .reg_alpha(config.alpha)
            .verbosity(Verbosity::Silent)
            .build()
            .unwrap();

        // Train with weights
        let forest = trainer.train(
            &train_matrix,
            &train_labels.labels,
            Some(&train_weights.weights),
            &[],
        );

        // Check tree count
        // XGBoost metrics stores num_boosted_rounds, but actual tree count = rounds * num_classes
        let expected_trees = metrics.num_trees * num_classes;
        assert_eq!(
            forest.num_trees(),
            expected_trees,
            "weighted_multiclass: Expected {} trees ({}*{}), got {}",
            expected_trees,
            metrics.num_trees,
            num_classes,
            forest.num_trees()
        );

        // Predict
        let predictor = Predictor::<StandardTraversal>::new(&forest);
        let train_row_matrix: RowMatrix<f32> = train_matrix.to_layout();
        let train_preds = predictor.predict(&train_row_matrix).into_vec();
        let test_preds = predictor.predict(&test_matrix).into_vec();

        // Correlation should be checked on the flattened array
        let train_corr = pearson_correlation(&train_preds, &xgb_train_flat);
        let test_corr = pearson_correlation(&test_preds, &xgb_test_flat);

        println!(
            "weighted_multiclass: trees={}, train_corr={:.4}, test_corr={:.4}",
            forest.num_trees(),
            train_corr,
            test_corr
        );

        assert!(
            train_corr >= 0.85,
            "weighted_multiclass: Training correlation {:.4} < 0.85 minimum",
            train_corr
        );
        assert!(
            test_corr >= 0.75,
            "weighted_multiclass: Test correlation {:.4} < 0.75 minimum",
            test_corr
        );
    }

    /// 5.5: Class imbalance weighting
    ///
    /// Verifies that weighting minority class improves predictions for
    /// imbalanced datasets.
    #[test]
    fn class_imbalance_weighted() {
        let tc = WeightedTestCase::load("class_imbalance_weighted");
        let result = train_and_predict(&tc);

        // Check tree count
        assert_eq!(
            result.forest.num_trees(),
            tc.metrics.num_trees,
            "class_imbalance_weighted: Expected {} trees, got {}",
            tc.metrics.num_trees,
            result.forest.num_trees()
        );

        // Check correlation with XGBoost
        let train_corr = pearson_correlation(&result.train_preds, &tc.xgb_train_preds);
        let test_corr = pearson_correlation(&result.test_preds, &tc.xgb_test_preds);

        println!(
            "class_imbalance_weighted: trees={}, train_corr={:.4}, test_corr={:.4}",
            result.forest.num_trees(),
            train_corr,
            test_corr
        );

        // Use relaxed thresholds - imbalanced data can have more variance
        assert!(
            train_corr >= 0.85,
            "class_imbalance_weighted: Training correlation {:.4} < 0.85",
            train_corr
        );
        assert!(
            test_corr >= 0.70,
            "class_imbalance_weighted: Test correlation {:.4} < 0.70",
            test_corr
        );
    }

    /// 5.6: Zero-weight samples
    ///
    /// Verifies that zero-weight samples are effectively excluded from training.
    #[test]
    fn zero_weights() {
        validate_weighted_case("zero_weights", 0.90, 0.80);
    }
}

// =============================================================================
// Story 6: Quality Validation
// =============================================================================

mod quality_validation {
    use super::*;

    /// 6.1: Verify that weighting improves minority class recall.
    ///
    /// Compares weighted vs unweighted training on the same imbalanced dataset.
    #[test]
    fn weighting_improves_minority_recall() {
        let tc = WeightedTestCase::load("class_imbalance_weighted");
        let dir = weighted_training_dir();

        // Load unweighted predictions from XGBoost
        let unweighted_preds: Predictions =
            load_json(&dir.join("class_imbalance_unweighted.test_predictions.json"));

        // Train with weights
        let weighted_result = train_and_predict(&tc);

        // Compute recall for minority class (class 1) for both models
        let threshold = 0.0; // logit threshold (positive = class 1)

        let (weighted_tp, weighted_fn) = compute_minority_tp_fn(
            &weighted_result.test_preds,
            &tc.test_labels,
            threshold,
        );
        let (unweighted_tp, unweighted_fn) = compute_minority_tp_fn(
            &unweighted_preds.predictions,
            &tc.test_labels,
            threshold,
        );

        let weighted_recall = if weighted_tp + weighted_fn > 0 {
            weighted_tp as f64 / (weighted_tp + weighted_fn) as f64
        } else {
            0.0
        };

        let unweighted_recall = if unweighted_tp + unweighted_fn > 0 {
            unweighted_tp as f64 / (unweighted_tp + unweighted_fn) as f64
        } else {
            0.0
        };

        println!(
            "Minority class recall - weighted: {:.4}, unweighted: {:.4}",
            weighted_recall, unweighted_recall
        );

        // Weighted model should have better or equal recall on minority class
        // Allow 5% tolerance since models may differ slightly
        assert!(
            weighted_recall >= unweighted_recall - 0.05,
            "Weighted recall ({:.4}) should be >= unweighted recall ({:.4}) - 0.05",
            weighted_recall,
            unweighted_recall
        );
    }

    /// Compute true positives and false negatives for minority class (label=1).
    fn compute_minority_tp_fn(preds: &[f32], labels: &[f32], threshold: f32) -> (usize, usize) {
        let mut tp = 0;
        let mut fn_count = 0;

        for (pred, label) in preds.iter().zip(labels.iter()) {
            if *label > 0.5 {
                // Minority class (label=1)
                if *pred > threshold {
                    tp += 1;
                } else {
                    fn_count += 1;
                }
            }
        }

        (tp, fn_count)
    }

    /// 6.2: Verify training quality doesn't degrade with weights.
    ///
    /// When all weights are 1.0, results should match unweighted training.
    #[test]
    fn uniform_weights_match_unweighted() {
        let tc = WeightedTestCase::load("weighted_regression");

        // Train with uniform weights (all 1.0)
        let uniform_weights: Vec<f32> = vec![1.0; tc.train_labels.len()];

        let trainer = GBTreeTrainer::builder()
            .loss(tc.loss_function())
            .num_rounds(tc.config.num_boost_round)
            .growth_mode(GrowthMode::DepthWise)
            .max_depth(tc.config.max_depth.unwrap_or(6))
            .learning_rate(tc.config.eta)
            .reg_lambda(tc.config.lambda)
            .reg_alpha(tc.config.alpha)
            .verbosity(Verbosity::Silent)
            .build()
            .unwrap();

        // Train with uniform weights
        let forest_weighted = trainer.train(
            &tc.train_matrix,
            &tc.train_labels,
            Some(&uniform_weights),
            &[],
        );

        // Train without weights
        let forest_unweighted =
            trainer.train(&tc.train_matrix, &tc.train_labels, None, &[]);

        // Both should produce identical trees
        assert_eq!(
            forest_weighted.num_trees(),
            forest_unweighted.num_trees(),
            "Tree count should match"
        );

        // Predictions should be identical
        let predictor_weighted = Predictor::<StandardTraversal>::new(&forest_weighted);
        let predictor_unweighted = Predictor::<StandardTraversal>::new(&forest_unweighted);

        let train_row_matrix: RowMatrix<f32> = tc.train_matrix.to_layout();
        let preds_weighted = predictor_weighted.predict(&train_row_matrix).into_vec();
        let preds_unweighted = predictor_unweighted.predict(&train_row_matrix).into_vec();

        // Should be very close (essentially identical)
        let corr = pearson_correlation(&preds_weighted, &preds_unweighted);
        assert!(
            corr > 0.9999,
            "Uniform weights should produce near-identical predictions, got correlation {:.6}",
            corr
        );
    }
}
