"""Type stubs for boosters._boosters_rs Rust extension module.

This file provides type hints for the native Rust extension.
It is auto-generated by pyo3-stub-gen but may be manually augmented.
"""

import numpy as np
from numpy.typing import NDArray

# Module version
__version__: str

# =============================================================================
# Config Types (Base)
# =============================================================================

class TreeConfig:
    """Configuration for tree structure."""

    max_depth: int
    n_leaves: int
    min_samples_leaf: int
    min_gain_to_split: float
    growth_strategy: str

    def __init__(
        self,
        max_depth: int = -1,
        n_leaves: int = 31,
        min_samples_leaf: int = 1,
        min_gain_to_split: float = 0.0,
        growth_strategy: str = "depthwise",
    ) -> None: ...

class RegularizationConfig:
    """Configuration for L1/L2 regularization."""

    l1: float
    l2: float
    min_hessian: float

    def __init__(
        self,
        l1: float = 0.0,
        l2: float = 1.0,
        min_hessian: float = 1.0,
    ) -> None: ...

class SamplingConfig:
    """Configuration for row and column subsampling."""

    subsample: float
    colsample: float
    colsample_bylevel: float
    goss_alpha: float
    goss_beta: float

    def __init__(
        self,
        subsample: float = 1.0,
        colsample: float = 1.0,
        colsample_bylevel: float = 1.0,
        goss_alpha: float = 0.0,
        goss_beta: float = 0.0,
    ) -> None: ...

class CategoricalConfig:
    """Configuration for categorical feature handling."""

    max_categories: int
    min_category_count: int
    max_onehot: int

    def __init__(
        self,
        max_categories: int = 256,
        min_category_count: int = 10,
        max_onehot: int = 4,
    ) -> None: ...

class EFBConfig:
    """Configuration for Exclusive Feature Bundling."""

    enable: bool
    max_conflict_rate: float

    def __init__(
        self,
        enable: bool = True,
        max_conflict_rate: float = 0.0,
    ) -> None: ...

class LinearLeavesConfig:
    """Configuration for linear models in leaf nodes."""

    enable: bool
    l2: float
    l1: float
    max_iter: int
    tolerance: float
    min_samples: int

    def __init__(
        self,
        enable: bool = False,
        l2: float = 0.01,
        l1: float = 0.0,
        max_iter: int = 10,
        tolerance: float = 1e-6,
        min_samples: int = 50,
    ) -> None: ...

# =============================================================================
# Objective Types
# =============================================================================

class SquaredLoss:
    """Squared error loss for regression (L2 loss)."""

    def __init__(self) -> None: ...

class AbsoluteLoss:
    """Absolute error loss for regression (L1 loss)."""

    def __init__(self) -> None: ...

class PoissonLoss:
    """Poisson regression loss for count data."""

    def __init__(self) -> None: ...

class LogisticLoss:
    """Logistic loss for binary classification."""

    def __init__(self) -> None: ...

class HingeLoss:
    """Hinge loss for binary classification (SVM-style)."""

    def __init__(self) -> None: ...

class HuberLoss:
    """Huber loss for robust regression.

    Combines L2 loss for small errors and L1 for large errors.
    """

    delta: float

    def __init__(self, delta: float = 1.0) -> None: ...

class PinballLoss:
    """Pinball loss for quantile regression.

    The alpha parameter can be a single float or a list of floats.
    Internally, alpha is always stored as a list for consistency.
    """

    alpha: list[float]

    def __init__(self, alpha: float | list[float] = 0.5) -> None: ...

class ArctanLoss:
    """Arctan loss for robust regression.

    A smooth approximation to absolute loss.
    """

    alpha: float

    def __init__(self, alpha: float = 0.5) -> None: ...

class SoftmaxLoss:
    """Softmax cross-entropy loss for multiclass classification.

    Note: n_classes is required and has no default value.
    """

    n_classes: int

    def __init__(self, n_classes: int) -> None: ...

class LambdaRankLoss:
    """LambdaRank loss for learning-to-rank tasks."""

    ndcg_at: int

    def __init__(self, ndcg_at: int = 10) -> None: ...

# =============================================================================
# Metric Types
# =============================================================================

class Rmse:
    """Root Mean Squared Error for regression."""

    def __init__(self) -> None: ...

class Mae:
    """Mean Absolute Error for regression."""

    def __init__(self) -> None: ...

class Mape:
    """Mean Absolute Percentage Error for regression."""

    def __init__(self) -> None: ...

class LogLoss:
    """Binary Log Loss (cross-entropy) for classification."""

    def __init__(self) -> None: ...

class Auc:
    """Area Under ROC Curve for binary classification."""

    def __init__(self) -> None: ...

class Accuracy:
    """Classification accuracy (binary or multiclass)."""

    def __init__(self) -> None: ...

class Ndcg:
    """Normalized Discounted Cumulative Gain for ranking."""

    at: int

    def __init__(self, at: int = 10) -> None: ...

# =============================================================================
# Type Aliases (after class definitions)
# =============================================================================

type Objective = (
    SquaredLoss
    | AbsoluteLoss
    | PoissonLoss
    | LogisticLoss
    | HingeLoss
    | HuberLoss
    | PinballLoss
    | ArctanLoss
    | SoftmaxLoss
    | LambdaRankLoss
)

type Metric = Rmse | Mae | Mape | LogLoss | Auc | Accuracy | Ndcg

# =============================================================================
# Main Config Types (depend on type aliases)
# =============================================================================

class GBDTConfig:
    """Main configuration for GBDT model.

    This is the primary configuration class for gradient boosted decision trees.
    It accepts nested configuration objects for tree structure, regularization,
    sampling, etc.
    """

    n_estimators: int
    learning_rate: float
    objective: Objective
    metric: Metric | None
    tree: TreeConfig
    regularization: RegularizationConfig
    sampling: SamplingConfig
    categorical: CategoricalConfig
    efb: EFBConfig
    linear_leaves: LinearLeavesConfig | None
    early_stopping_rounds: int | None
    seed: int

    def __init__(
        self,
        n_estimators: int = 100,
        learning_rate: float = 0.3,
        objective: Objective | None = None,
        metric: Metric | None = None,
        tree: TreeConfig | None = None,
        regularization: RegularizationConfig | None = None,
        sampling: SamplingConfig | None = None,
        categorical: CategoricalConfig | None = None,
        efb: EFBConfig | None = None,
        linear_leaves: LinearLeavesConfig | None = None,
        early_stopping_rounds: int | None = None,
        seed: int = 42,
    ) -> None: ...

class GBLinearConfig:
    """Main configuration for GBLinear model.

    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    """

    n_estimators: int
    learning_rate: float
    objective: Objective
    metric: Metric | None
    l1: float
    l2: float
    early_stopping_rounds: int | None
    seed: int

    def __init__(
        self,
        n_estimators: int = 100,
        learning_rate: float = 0.5,
        objective: Objective | None = None,
        metric: Metric | None = None,
        l1: float = 0.0,
        l2: float = 1.0,
        early_stopping_rounds: int | None = None,
        seed: int = 42,
    ) -> None: ...

# =============================================================================
# Data Types
# =============================================================================

class Dataset:
    """Dataset holding features, labels, and optional metadata.

    This class wraps NumPy arrays or pandas DataFrames for use with boosters models.
    It keeps references to the original Python objects to ensure zero-copy access
    where possible.

    Data Layout:
        - C-contiguous (row-major) float32 arrays provide zero-copy access
        - F-contiguous arrays are automatically converted to C-order
        - float64 arrays are supported but may require conversion

    Categorical Features:
        Categorical features can be:
        1. Auto-detected from pandas categorical dtype
        2. Specified explicitly via `categorical_features` parameter

    Missing Values:
        NaN values in features are treated as missing (like XGBoost).
        Inf values in features or NaN/Inf in labels raise errors.

    Example:
        >>> import numpy as np
        >>> from boosters import Dataset
        >>>
        >>> X = np.random.rand(100, 10).astype(np.float32)
        >>> y = np.random.rand(100).astype(np.float32)
        >>>
        >>> dataset = Dataset(X, y)
        >>> print(f"Samples: {dataset.n_samples}, Features: {dataset.n_features}")
    """

    n_samples: int
    n_features: int
    has_labels: bool
    has_weights: bool
    has_groups: bool
    feature_names: list[str] | None
    categorical_features: list[int]
    was_converted: bool
    shape: tuple[int, int]

    def __init__(
        self,
        features: NDArray[np.float32] | NDArray[np.float64],
        labels: NDArray[np.float32] | None = None,
        weights: NDArray[np.float32] | None = None,
        groups: NDArray[np.int32] | None = None,
        feature_names: list[str] | None = None,
        categorical_features: list[int] | None = None,
    ) -> None: ...

class EvalSet:
    """Named evaluation set for model training.

    An EvalSet wraps a Dataset with a name, which is used to identify
    the evaluation set in training logs and `eval_results`.

    Example:
        >>> from boosters import Dataset, EvalSet
        >>> import numpy as np
        >>>
        >>> X_val = np.random.rand(50, 10).astype(np.float32)
        >>> y_val = np.random.rand(50).astype(np.float32)
        >>>
        >>> val_data = Dataset(X_val, y_val)
        >>> eval_set = EvalSet("validation", val_data)
        >>>
        >>> # Use in training:
        >>> # model.fit(train_data, valid=[eval_set])
    """

    name: str
    dataset: Dataset

    def __init__(self, name: str, dataset: Dataset) -> None: ...
