# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import builtins
import typing

@typing.final
class AbsoluteLoss:
    r"""
    Absolute error loss (L1) for robust regression.
    
    Examples
    --------
    >>> from boosters import AbsoluteLoss
    >>> obj = AbsoluteLoss()
    """
    def __new__(cls) -> AbsoluteLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class Accuracy:
    r"""
    Classification accuracy (binary or multiclass).
    
    Examples
    --------
    >>> from boosters import Accuracy
    >>> metric = Accuracy()
    """
    def __new__(cls) -> Accuracy: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class ArctanLoss:
    r"""
    Arctan loss for bounded regression.
    
    Parameters
    ----------
    alpha : float, default=0.5
        Scale parameter. Must be in (0, 1).
    
    Examples
    --------
    >>> from boosters import ArctanLoss
    >>> obj = ArctanLoss(alpha=0.3)
    """
    @property
    def alpha(self) -> builtins.float: ...
    def __new__(cls, alpha: builtins.float = 0.5) -> ArctanLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class Auc:
    r"""
    Area Under ROC Curve for binary classification.
    
    Examples
    --------
    >>> from boosters import Auc
    >>> metric = Auc()
    """
    def __new__(cls) -> Auc: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class CategoricalConfig:
    r"""
    Configuration for categorical feature handling.
    
    Boosters supports native categorical splits (like LightGBM) using
    bitset-based multi-way splits rather than one-hot encoding.
    
    Examples
    --------
    >>> from boosters import CategoricalConfig
    >>> config = CategoricalConfig(max_categories=256)
    >>> config.max_categories
    256
    """
    @property
    def max_categories(self) -> builtins.int:
        r"""
        Maximum number of categories for native categorical splits.
        Categories beyond this are treated as continuous.
        """
    @max_categories.setter
    def max_categories(self, value: builtins.int) -> None:
        r"""
        Maximum number of categories for native categorical splits.
        Categories beyond this are treated as continuous.
        """
    @property
    def min_category_count(self) -> builtins.int:
        r"""
        Minimum count for a category to be considered.
        Categories with fewer samples are grouped into "other".
        """
    @min_category_count.setter
    def min_category_count(self, value: builtins.int) -> None:
        r"""
        Minimum count for a category to be considered.
        Categories with fewer samples are grouped into "other".
        """
    @property
    def max_onehot(self) -> builtins.int:
        r"""
        Maximum categories for one-hot encoding fallback.
        If a feature has more categories, use bitset splits.
        """
    @max_onehot.setter
    def max_onehot(self, value: builtins.int) -> None:
        r"""
        Maximum categories for one-hot encoding fallback.
        If a feature has more categories, use bitset splits.
        """
    def __new__(cls, max_categories: builtins.int = 256, min_category_count: builtins.int = 10, max_onehot: builtins.int = 4) -> CategoricalConfig:
        r"""
        Create a new CategoricalConfig.
        
        Parameters
        ----------
        max_categories : int, default=256
            Maximum categories for native categorical splits.
        min_category_count : int, default=10
            Minimum samples per category.
        max_onehot : int, default=4
            Maximum categories for one-hot encoding.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Dataset:
    r"""
    Dataset holding features, labels, and optional metadata.
    
    This class wraps NumPy arrays or pandas DataFrames for use with boosters models.
    Data is converted to an internal representation on construction for efficient
    training and prediction.
    
    # Data Layout
    
    - C-contiguous (row-major) float32 arrays provide optimal performance
    - F-contiguous arrays are automatically converted to C-order
    - float64 arrays are supported but will be converted to float32
    
    # Categorical Features
    
    Categorical features can be:
    1. Auto-detected from pandas categorical dtype
    2. Specified explicitly via `categorical_features` parameter
    
    # Missing Values
    
    NaN values in features are treated as missing (like XGBoost).
    Inf values in features or NaN/Inf in labels raise errors.
    
    # Example
    
    ```python
    import numpy as np
    from boosters import Dataset
    
    X = np.random.rand(100, 10).astype(np.float32)
    y = np.random.rand(100).astype(np.float32)
    
    dataset = Dataset(X, y)
    print(f"Samples: {dataset.n_samples}, Features: {dataset.n_features}")
    ```
    """
    @property
    def n_samples(self) -> builtins.int:
        r"""
        Number of samples in the dataset.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features in the dataset.
        """
    @property
    def has_labels(self) -> builtins.bool:
        r"""
        Whether labels are present.
        """
    @property
    def has_weights(self) -> builtins.bool:
        r"""
        Whether weights are present.
        """
    @property
    def feature_names(self) -> typing.Optional[builtins.list[builtins.str]]:
        r"""
        Feature names if provided.
        """
    @property
    def categorical_features(self) -> builtins.list[builtins.int]:
        r"""
        Indices of categorical features.
        """
    @property
    def shape(self) -> tuple[builtins.int, builtins.int]:
        r"""
        Shape of the features array as (n_samples, n_features).
        """
    def __new__(cls, features: typing.Any, labels: typing.Optional[typing.Any] = None, weights: typing.Optional[typing.Any] = None, groups: typing.Optional[typing.Any] = None, feature_names: typing.Optional[typing.Sequence[builtins.str]] = None, categorical_features: typing.Optional[typing.Sequence[builtins.int]] = None) -> Dataset:
        r"""
        Create a new Dataset from features and optional labels.
        
        Args:
            features: 2D NumPy array or pandas DataFrame of shape (n_samples, n_features)
            labels: Optional 1D array of shape (n_samples,)
            weights: Optional 1D array of sample weights (n_samples,)
            groups: Optional 1D array of group labels for ranking (not yet implemented)
            feature_names: Optional list of feature names
            categorical_features: Optional list of categorical feature indices
        
        Returns:
            Dataset ready for training or prediction
        
        Raises:
            ValueError: If data is invalid (shape mismatch, Inf values, etc.)
            TypeError: If data types are unsupported
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class EFBConfig:
    r"""
    Configuration for Exclusive Feature Bundling.
    
    EFB bundles mutually exclusive features to reduce memory and computation,
    similar to LightGBM's implementation.
    
    Examples
    --------
    >>> from boosters import EFBConfig
    >>> config = EFBConfig(enable=True, max_conflict_rate=0.0)
    >>> config.enable
    True
    """
    @property
    def enable(self) -> builtins.bool:
        r"""
        Whether to enable Exclusive Feature Bundling.
        """
    @enable.setter
    def enable(self, value: builtins.bool) -> None:
        r"""
        Whether to enable Exclusive Feature Bundling.
        """
    @property
    def max_conflict_rate(self) -> builtins.float:
        r"""
        Maximum conflict rate allowed when bundling features.
        0.0 means only truly exclusive features are bundled.
        """
    @max_conflict_rate.setter
    def max_conflict_rate(self, value: builtins.float) -> None:
        r"""
        Maximum conflict rate allowed when bundling features.
        0.0 means only truly exclusive features are bundled.
        """
    def __new__(cls, enable: builtins.bool = True, max_conflict_rate: builtins.float = 0.0) -> EFBConfig:
        r"""
        Create a new EFBConfig.
        
        Parameters
        ----------
        enable : bool, default=True
            Whether to enable EFB.
        max_conflict_rate : float, default=0.0
            Maximum conflict rate for bundling. Must be in [0, 1).
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class EvalSet:
    r"""
    Named evaluation set for model training.
    
    An EvalSet wraps a Dataset with a name, which is used to identify
    the evaluation set in training logs and `eval_results`.
    
    # Example
    
    ```python
    from boosters import Dataset, EvalSet
    import numpy as np
    
    X_val = np.random.rand(50, 10).astype(np.float32)
    y_val = np.random.rand(50).astype(np.float32)
    
    val_data = Dataset(X_val, y_val)
    eval_set = EvalSet("validation", val_data)
    
    # Use in training:
    # model.fit(train_data, valid=[eval_set])
    ```
    """
    @property
    def name(self) -> builtins.str:
        r"""
        Name of this evaluation set.
        """
    @property
    def dataset(self) -> Dataset:
        r"""
        The underlying dataset.
        """
    def __new__(cls, name: builtins.str, dataset: typing.Any) -> EvalSet:
        r"""
        Create a new named evaluation set.
        
        Args:
            name: Name for this evaluation set (e.g., "validation", "test")
            dataset: Dataset containing features and labels. Accepts either
                the Rust Dataset from `_boosters_rs` or the Python wrapper.
        
        Returns:
            EvalSet ready for use in training
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTConfig:
    r"""
    Main configuration for GBDT model.
    
    This is the primary configuration class for gradient boosted decision trees.
    It accepts nested configuration objects for tree structure, regularization,
    sampling, etc.
    
    Parameters
    ----------
    n_estimators : int, default=100
        Number of boosting rounds (trees to train).
    learning_rate : float, default=0.3
        Step size shrinkage. Smaller values require more trees but often
        produce better models. Typical range: 0.01 - 0.3.
    objective : Objective, default=SquaredLoss()
        Loss function for training.
    metric : Metric or None, default=None
        Evaluation metric. If None, uses objective's default metric.
    tree : TreeConfig or None, default=None
        Tree structure parameters. If None, uses defaults.
    regularization : RegularizationConfig or None, default=None
        L1/L2 regularization parameters. If None, uses defaults.
    sampling : SamplingConfig or None, default=None
        Row and column subsampling parameters. If None, uses defaults.
    categorical : CategoricalConfig or None, default=None
        Categorical feature handling. If None, uses defaults.
    efb : EFBConfig or None, default=None
        Exclusive Feature Bundling config. If None, uses defaults.
    linear_leaves : LinearLeavesConfig or None, default=None
        Linear model in leaves config. If None, disabled.
    early_stopping_rounds : int or None, default=None
        Stop if no improvement for this many rounds. None disables.
    seed : int, default=42
        Random seed for reproducibility.
    
    Examples
    --------
    >>> from boosters import GBDTConfig, SquaredLoss, TreeConfig
    >>> config = GBDTConfig(
    ...     n_estimators=500,
    ...     learning_rate=0.1,
    ...     objective=SquaredLoss(),
    ...     tree=TreeConfig(max_depth=6),
    ... )
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size shrinkage).
        """
    @property
    def tree(self) -> TreeConfig:
        r"""
        Tree structure config.
        """
    @property
    def regularization(self) -> RegularizationConfig:
        r"""
        Regularization config.
        """
    @property
    def sampling(self) -> SamplingConfig:
        r"""
        Sampling config.
        """
    @property
    def categorical(self) -> CategoricalConfig:
        r"""
        Categorical feature config.
        """
    @property
    def efb(self) -> EFBConfig:
        r"""
        Exclusive Feature Bundling config.
        """
    @property
    def linear_leaves(self) -> typing.Optional[LinearLeavesConfig]:
        r"""
        Linear leaves config (None = disabled).
        """
    @property
    def early_stopping_rounds(self) -> typing.Optional[builtins.int]:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def objective(self) -> typing.Any:
        r"""
        Get the objective as a Python object.
        """
    @property
    def metric(self) -> typing.Optional[typing.Any]:
        r"""
        Get the metric as a Python object (or None).
        """
    def __new__(cls, n_estimators: builtins.int = 100, learning_rate: builtins.float = 0.3, objective: typing.Optional[typing.Any] = None, metric: typing.Optional[typing.Any] = None, tree: typing.Optional[TreeConfig] = None, regularization: typing.Optional[RegularizationConfig] = None, sampling: typing.Optional[SamplingConfig] = None, categorical: typing.Optional[CategoricalConfig] = None, efb: typing.Optional[EFBConfig] = None, linear_leaves: typing.Optional[LinearLeavesConfig] = None, early_stopping_rounds: typing.Optional[builtins.int] = None, seed: builtins.int = 42) -> GBDTConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTModel:
    r"""
    Gradient Boosted Decision Tree model.
    
    This is the main model class for training and prediction with gradient
    boosted decision trees.
    
    # Example
    
    ```python
    from boosters import GBDTModel, GBDTConfig, Dataset
    import numpy as np
    
    # Create training data
    X = np.random.rand(1000, 10).astype(np.float32)
    y = np.random.rand(1000).astype(np.float32)
    train = Dataset(X, y)
    
    # Train with default config
    model = GBDTModel().fit(train)
    
    # Or with custom config
    config = GBDTConfig(n_estimators=50, learning_rate=0.1)
    model = GBDTModel(config=config).fit(train)
    
    # Predict
    predictions = model.predict(X_test)
    ```
    """
    @property
    def is_fitted(self) -> builtins.bool:
        r"""
        Whether the model has been fitted.
        """
    @property
    def n_trees(self) -> builtins.int:
        r"""
        Number of trees in the fitted model.
        
        Raises:
            ValueError: If model has not been fitted.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        
        Raises:
            ValueError: If model has not been fitted.
        """
    @property
    def best_iteration(self) -> typing.Optional[builtins.int]:
        r"""
        Best iteration from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def best_score(self) -> typing.Optional[builtins.float]:
        r"""
        Best score from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def eval_results(self) -> typing.Optional[typing.Any]:
        r"""
        Evaluation results from training.
        
        Returns a dict mapping eval set names to dicts of metric names to lists
        of scores per iteration.
        
        Example:
            ```python
            results = model.eval_results
            # {"train": {"rmse": [0.5, 0.4, ...]}, "valid": {"rmse": [0.6, 0.5, ...]}}
            ```
        """
    @property
    def config(self) -> GBDTConfig:
        r"""
        Get the model configuration.
        """
    def __new__(cls, config: typing.Optional[GBDTConfig] = None) -> GBDTModel:
        r"""
        Create a new GBDT model.
        
        Args:
            config: Optional GBDTConfig. If not provided, uses default config.
        
        Returns:
            New GBDTModel instance (not yet fitted).
        """
    def feature_importance(self, importance_type: builtins.str = 'split') -> typing.Any:
        r"""
        Get feature importance scores.
        
        Args:
            importance_type: Type of importance to compute.
                - "split" (default): Number of times a feature is used to split.
                - "gain": Total gain achieved by splits on this feature.
        
        Returns:
            NumPy array of importance scores, one per feature.
        
        Raises:
            ValueError: If model has not been fitted.
        """
    def shap_values(self, features: typing.Any) -> typing.Any:
        r"""
        Compute SHAP values for feature contribution analysis.
        
        SHAP (SHapley Additive exPlanations) values show how each feature
        contributes to individual predictions. The values sum to the difference
        between the model's prediction and the base value.
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
        
        Returns:
            NumPy array with shape `(n_samples, n_features + 1, n_outputs)`.
            The last feature index contains the base value (expected value).
            For single-output models, the last dimension is squeezed.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            # Get SHAP values for test data
            shap_values = model.shap_values(X_test)
        
            # For a single sample, contributions sum to prediction - base_value
            sample_idx = 0
            feature_contribs = shap_values[sample_idx, :-1, 0]  # All features
            base_value = shap_values[sample_idx, -1, 0]  # Base value
            prediction = model.predict(X_test[sample_idx:sample_idx+1])[0]
            # assert np.isclose(base_value + feature_contribs.sum(), prediction)
            ```
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def predict(self, features: typing.Any, n_iterations: typing.Optional[builtins.int] = None) -> typing.Any:
        r"""
        Make predictions on features.
        
        Returns transformed predictions (e.g., probabilities for classification).
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
            n_iterations: Number of trees to use for prediction. If None, uses all trees.
        
        Returns:
            NumPy array with predictions of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            predictions = model.predict(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
    def predict_raw(self, features: typing.Any, n_iterations: typing.Optional[builtins.int] = None) -> typing.Any:
        r"""
        Make raw (untransformed) predictions on features.
        
        Returns raw margin scores without transformation.
        For classification this means logits instead of probabilities.
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
            n_iterations: Number of trees to use for prediction. If None, uses all trees.
        
        Returns:
            NumPy array with raw scores of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            raw_margins = model.predict_raw(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
    def fit(self, train: typing.Any, valid: typing.Optional[typing.Any] = None) -> GBDTModel:
        r"""
        Train the model on a dataset.
        
        Args:
            train: Training dataset containing features and labels.
            valid: Optional validation set(s) for early stopping and evaluation.
                Can be a single EvalSet or a list of EvalSets.
        
        Returns:
            Self (for method chaining).
        
        Raises:
            ValueError: If training data is invalid or labels are missing.
        
        Example:
            ```python
            model = GBDTModel().fit(train_dataset)
            model = GBDTModel().fit(train, valid=[EvalSet("val", val_data)])
            ```
        """

@typing.final
class GBLinearConfig:
    r"""
    Main configuration for GBLinear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    Parameters
    ----------
    n_estimators : int, default=100
        Number of boosting rounds.
    learning_rate : float, default=0.5
        Step size for weight updates. Higher values mean faster convergence
        but risk overshooting.
    objective : Objective, default=SquaredLoss()
        Loss function for training.
    metric : Metric or None, default=None
        Evaluation metric. If None, uses objective's default metric.
    l1 : float, default=0.0
        L1 regularization (alpha). Encourages sparse weights.
    l2 : float, default=1.0
        L2 regularization (lambda). Prevents large weights.
    early_stopping_rounds : int or None, default=None
        Stop if no improvement for this many rounds. None disables.
    seed : int, default=42
        Random seed for reproducibility.
    
    Examples
    --------
    >>> from boosters import GBLinearConfig, SquaredLoss
    >>> config = GBLinearConfig(
    ...     n_estimators=200,
    ...     learning_rate=0.3,
    ...     objective=SquaredLoss(),
    ...     l2=0.1,
    ... )
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size).
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization (alpha).
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization (lambda).
        """
    @property
    def early_stopping_rounds(self) -> typing.Optional[builtins.int]:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def objective(self) -> typing.Any:
        r"""
        Get the objective as a Python object.
        """
    @property
    def metric(self) -> typing.Optional[typing.Any]:
        r"""
        Get the metric as a Python object (or None).
        """
    def __new__(cls, n_estimators: builtins.int = 100, learning_rate: builtins.float = 0.5, objective: typing.Optional[typing.Any] = None, metric: typing.Optional[typing.Any] = None, l1: builtins.float = 0.0, l2: builtins.float = 1.0, early_stopping_rounds: typing.Optional[builtins.int] = None, seed: builtins.int = 42) -> GBLinearConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBLinearModel:
    r"""
    Gradient Boosted Linear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    # Example
    
    ```python
    from boosters import GBLinearModel, GBLinearConfig, Dataset
    import numpy as np
    
    # Create training data
    X = np.random.rand(1000, 10).astype(np.float32)
    y = np.random.rand(1000).astype(np.float32)
    train = Dataset(X, y)
    
    # Train with default config
    model = GBLinearModel().fit(train)
    
    # Or with custom config
    config = GBLinearConfig(n_estimators=50, learning_rate=0.3, l2=0.1)
    model = GBLinearModel(config=config).fit(train)
    
    # Predict
    predictions = model.predict(X_test)
    
    # Access weights
    print(model.coef_)      # shape: (n_features,) or (n_features, n_outputs)
    print(model.intercept_)  # shape: () or (n_outputs,)
    ```
    """
    @property
    def is_fitted(self) -> builtins.bool:
        r"""
        Whether the model has been fitted.
        """
    @property
    def n_features_in_(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        
        Raises:
            RuntimeError: If model has not been fitted.
        """
    @property
    def coef_(self) -> typing.Any:
        r"""
        Model coefficients (weights).
        
        Returns:
            NumPy array with shape:
            - `(n_features,)` for single-output models
            - `(n_features, n_outputs)` for multi-output models
        
        Raises:
            RuntimeError: If model has not been fitted.
        """
    @property
    def intercept_(self) -> typing.Any:
        r"""
        Model intercept (bias).
        
        Returns:
            NumPy array with shape:
            - `()` (scalar) for single-output models
            - `(n_outputs,)` for multi-output models
        
        Raises:
            RuntimeError: If model has not been fitted.
        """
    @property
    def best_iteration(self) -> typing.Optional[builtins.int]:
        r"""
        Best iteration from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def best_score(self) -> typing.Optional[builtins.float]:
        r"""
        Best score from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def eval_results(self) -> typing.Optional[typing.Any]:
        r"""
        Evaluation results from training.
        
        Returns a dict mapping eval set names to dicts of metric names to lists
        of scores per iteration.
        """
    @property
    def config(self) -> GBLinearConfig:
        r"""
        Get the model configuration.
        """
    def __new__(cls, config: typing.Optional[GBLinearConfig] = None) -> GBLinearModel:
        r"""
        Create a new GBLinear model.
        
        Args:
            config: Optional GBLinearConfig. If not provided, uses default config.
        
        Returns:
            New GBLinearModel instance (not yet fitted).
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def predict(self, features: typing.Any) -> typing.Any:
        r"""
        Make predictions on features.
        
        Returns transformed predictions (e.g., probabilities for classification).
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
        
        Returns:
            NumPy array with predictions of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            predictions = model.predict(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
    def predict_raw(self, features: typing.Any) -> typing.Any:
        r"""
        Make raw (untransformed) predictions on features.
        
        Returns raw margin scores without transformation.
        For classification this means logits instead of probabilities.
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
        
        Returns:
            NumPy array with raw scores of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            raw_margins = model.predict_raw(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
    def fit(self, train: typing.Any, eval_set: typing.Optional[typing.Any] = None) -> GBLinearModel:
        r"""
        Train the model on a dataset.
        
        Args:
            train: Training dataset containing features and labels.
            eval_set: Optional validation set(s) for early stopping and evaluation.
        
        Returns:
            Self (for method chaining).
        
        Raises:
            ValueError: If training data is invalid or labels are missing.
        """

@typing.final
class HingeLoss:
    r"""
    Hinge loss for binary classification (SVM-style).
    
    Examples
    --------
    >>> from boosters import HingeLoss
    >>> obj = HingeLoss()
    """
    def __new__(cls) -> HingeLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class HuberLoss:
    r"""
    Pseudo-Huber loss for robust regression.
    
    Parameters
    ----------
    delta : float, default=1.0
        Transition point between quadratic and linear loss.
    
    Examples
    --------
    >>> from boosters import HuberLoss
    >>> obj = HuberLoss(delta=1.5)
    """
    @property
    def delta(self) -> builtins.float: ...
    def __new__(cls, delta: builtins.float = 1.0) -> HuberLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class LambdaRankLoss:
    r"""
    LambdaRank loss for learning to rank.
    
    Parameters
    ----------
    ndcg_at : int, default=10
        Truncation point for NDCG calculation.
    
    Examples
    --------
    >>> from boosters import LambdaRankLoss
    >>> obj = LambdaRankLoss(ndcg_at=5)
    """
    @property
    def ndcg_at(self) -> builtins.int: ...
    def __new__(cls, ndcg_at: builtins.int = 10) -> LambdaRankLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class LinearLeavesConfig:
    r"""
    Configuration for linear models in leaf nodes.
    
    When enabled, each leaf fits a linear regression model on its samples
    instead of using a constant value.
    
    Examples
    --------
    >>> from boosters import LinearLeavesConfig
    >>> config = LinearLeavesConfig(enable=True, l2=0.01)
    >>> config.enable
    True
    """
    @property
    def enable(self) -> builtins.bool:
        r"""
        Whether to enable linear models in leaves.
        """
    @enable.setter
    def enable(self, value: builtins.bool) -> None:
        r"""
        Whether to enable linear models in leaves.
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization for linear coefficients.
        """
    @l2.setter
    def l2(self, value: builtins.float) -> None:
        r"""
        L2 regularization for linear coefficients.
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization for linear coefficients.
        """
    @l1.setter
    def l1(self, value: builtins.float) -> None:
        r"""
        L1 regularization for linear coefficients.
        """
    @property
    def max_iter(self) -> builtins.int:
        r"""
        Maximum coordinate descent iterations per leaf.
        """
    @max_iter.setter
    def max_iter(self, value: builtins.int) -> None:
        r"""
        Maximum coordinate descent iterations per leaf.
        """
    @property
    def tolerance(self) -> builtins.float:
        r"""
        Convergence tolerance.
        """
    @tolerance.setter
    def tolerance(self, value: builtins.float) -> None:
        r"""
        Convergence tolerance.
        """
    @property
    def min_samples(self) -> builtins.int:
        r"""
        Minimum samples required to fit a linear model.
        """
    @min_samples.setter
    def min_samples(self, value: builtins.int) -> None:
        r"""
        Minimum samples required to fit a linear model.
        """
    def __new__(cls, enable: builtins.bool = False, l2: builtins.float = 0.01, l1: builtins.float = 0.0, max_iter: builtins.int = 10, tolerance: builtins.float = 1e-06, min_samples: builtins.int = 50) -> LinearLeavesConfig:
        r"""
        Create a new LinearLeavesConfig.
        
        Parameters
        ----------
        enable : bool, default=False
            Whether to enable linear leaves.
        l2 : float, default=0.01
            L2 regularization.
        l1 : float, default=0.0
            L1 regularization.
        max_iter : int, default=10
            Maximum coordinate descent iterations.
        tolerance : float, default=1e-6
            Convergence tolerance.
        min_samples : int, default=50
            Minimum samples to fit linear model.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class LogLoss:
    r"""
    Binary Log Loss (cross-entropy) for classification.
    
    Examples
    --------
    >>> from boosters import LogLoss
    >>> metric = LogLoss()
    """
    def __new__(cls) -> LogLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class LogisticLoss:
    r"""
    Logistic loss for binary classification.
    
    Examples
    --------
    >>> from boosters import LogisticLoss
    >>> obj = LogisticLoss()
    """
    def __new__(cls) -> LogisticLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class Mae:
    r"""
    Mean Absolute Error for regression.
    
    Examples
    --------
    >>> from boosters import Mae
    >>> metric = Mae()
    """
    def __new__(cls) -> Mae: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class Mape:
    r"""
    Mean Absolute Percentage Error for regression.
    
    Examples
    --------
    >>> from boosters import Mape
    >>> metric = Mape()
    """
    def __new__(cls) -> Mape: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class Ndcg:
    r"""
    Normalized Discounted Cumulative Gain for ranking.
    
    Parameters
    ----------
    at : int, default=10
        Truncation point for NDCG calculation (NDCG@k).
    
    Examples
    --------
    >>> from boosters import Ndcg
    >>> metric = Ndcg(at=5)  # NDCG@5
    """
    @property
    def at(self) -> builtins.int: ...
    def __new__(cls, at: builtins.int = 10) -> Ndcg: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class PinballLoss:
    r"""
    Pinball loss for quantile regression.
    
    Parameters
    ----------
    alpha : float or list of float, default=0.5
        Quantile(s) to predict. Each value must be in (0, 1).
    
    Examples
    --------
    >>> from boosters import PinballLoss
    >>> obj = PinballLoss(alpha=0.5)  # median
    >>> obj = PinballLoss(alpha=[0.1, 0.5, 0.9])  # multiple quantiles
    """
    @property
    def alpha(self) -> builtins.list[builtins.float]:
        r"""
        Quantile values (always stored as Vec for consistency).
        """
    def __new__(cls, alpha: typing.Optional[typing.Any] = None) -> PinballLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class PoissonLoss:
    r"""
    Poisson loss for count regression.
    
    Examples
    --------
    >>> from boosters import PoissonLoss
    >>> obj = PoissonLoss()
    """
    def __new__(cls) -> PoissonLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class RegularizationConfig:
    r"""
    Configuration for L1/L2 regularization.
    
    Examples
    --------
    >>> from boosters import RegularizationConfig
    >>> config = RegularizationConfig(l1=0.1, l2=1.0)
    >>> config.l2
    1.0
    """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 (Lasso) regularization term on leaf weights.
        """
    @l1.setter
    def l1(self, value: builtins.float) -> None:
        r"""
        L1 (Lasso) regularization term on leaf weights.
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 (Ridge) regularization term on leaf weights.
        """
    @l2.setter
    def l2(self, value: builtins.float) -> None:
        r"""
        L2 (Ridge) regularization term on leaf weights.
        """
    @property
    def min_hessian(self) -> builtins.float:
        r"""
        Minimum sum of hessians required in a leaf.
        """
    @min_hessian.setter
    def min_hessian(self, value: builtins.float) -> None:
        r"""
        Minimum sum of hessians required in a leaf.
        """
    def __new__(cls, l1: builtins.float = 0.0, l2: builtins.float = 1.0, min_hessian: builtins.float = 1.0) -> RegularizationConfig:
        r"""
        Create a new RegularizationConfig.
        
        Parameters
        ----------
        l1 : float, default=0.0
            L1 regularization term.
        l2 : float, default=1.0
            L2 regularization term.
        min_hessian : float, default=1.0
            Minimum sum of hessians in a leaf.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Rmse:
    r"""
    Root Mean Squared Error for regression.
    
    Examples
    --------
    >>> from boosters import Rmse
    >>> metric = Rmse()
    """
    def __new__(cls) -> Rmse: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class SamplingConfig:
    r"""
    Configuration for row and column subsampling.
    
    Examples
    --------
    >>> from boosters import SamplingConfig
    >>> config = SamplingConfig(subsample=0.8, colsample=0.8)
    >>> config.subsample
    0.8
    """
    @property
    def subsample(self) -> builtins.float:
        r"""
        Row subsampling ratio (per tree). Value in (0, 1].
        """
    @subsample.setter
    def subsample(self, value: builtins.float) -> None:
        r"""
        Row subsampling ratio (per tree). Value in (0, 1].
        """
    @property
    def colsample(self) -> builtins.float:
        r"""
        Column subsampling ratio (per tree). Value in (0, 1].
        """
    @colsample.setter
    def colsample(self, value: builtins.float) -> None:
        r"""
        Column subsampling ratio (per tree). Value in (0, 1].
        """
    @property
    def colsample_bylevel(self) -> builtins.float:
        r"""
        Column subsampling ratio (per level). Value in (0, 1].
        """
    @colsample_bylevel.setter
    def colsample_bylevel(self, value: builtins.float) -> None:
        r"""
        Column subsampling ratio (per level). Value in (0, 1].
        """
    @property
    def goss_alpha(self) -> builtins.float:
        r"""
        GOSS top percentage (for gradient-based one-side sampling).
        If > 0, enables GOSS. Value in [0, 1).
        """
    @goss_alpha.setter
    def goss_alpha(self, value: builtins.float) -> None:
        r"""
        GOSS top percentage (for gradient-based one-side sampling).
        If > 0, enables GOSS. Value in [0, 1).
        """
    @property
    def goss_beta(self) -> builtins.float:
        r"""
        GOSS random percentage for small gradients.
        Value in [0, 1].
        """
    @goss_beta.setter
    def goss_beta(self, value: builtins.float) -> None:
        r"""
        GOSS random percentage for small gradients.
        Value in [0, 1].
        """
    def __new__(cls, subsample: builtins.float = 1.0, colsample: builtins.float = 1.0, colsample_bylevel: builtins.float = 1.0, goss_alpha: builtins.float = 0.0, goss_beta: builtins.float = 0.0) -> SamplingConfig:
        r"""
        Create a new SamplingConfig.
        
        Parameters
        ----------
        subsample : float, default=1.0
            Row subsampling ratio. Must be in (0, 1].
        colsample : float, default=1.0
            Column subsampling ratio per tree. Must be in (0, 1].
        colsample_bylevel : float, default=1.0
            Column subsampling ratio per level. Must be in (0, 1].
        goss_alpha : float, default=0.0
            GOSS top percentage. 0 disables GOSS.
        goss_beta : float, default=0.0
            GOSS random percentage for small gradients.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class SoftmaxLoss:
    r"""
    Softmax loss for multiclass classification.
    
    Parameters
    ----------
    n_classes : int
        Number of classes. Must be >= 2.
    
    Examples
    --------
    >>> from boosters import SoftmaxLoss
    >>> obj = SoftmaxLoss(n_classes=10)
    """
    @property
    def n_classes(self) -> builtins.int: ...
    def __new__(cls, n_classes: builtins.int) -> SoftmaxLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class SquaredLoss:
    r"""
    Squared error loss (L2) for regression.
    
    Examples
    --------
    >>> from boosters import SquaredLoss
    >>> obj = SquaredLoss()
    """
    def __new__(cls) -> SquaredLoss: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class TreeConfig:
    r"""
    Configuration for tree structure.
    
    Controls tree depth, number of leaves, and split constraints.
    
    Examples
    --------
    >>> from boosters import TreeConfig
    >>> config = TreeConfig(max_depth=6, n_leaves=31)
    >>> config.max_depth
    6
    """
    @property
    def max_depth(self) -> builtins.int:
        r"""
        Maximum depth of tree. -1 means unlimited (controlled by n_leaves).
        """
    @max_depth.setter
    def max_depth(self, value: builtins.int) -> None:
        r"""
        Maximum depth of tree. -1 means unlimited (controlled by n_leaves).
        """
    @property
    def n_leaves(self) -> builtins.int:
        r"""
        Maximum number of leaves. Only used when max_depth is -1.
        """
    @n_leaves.setter
    def n_leaves(self, value: builtins.int) -> None:
        r"""
        Maximum number of leaves. Only used when max_depth is -1.
        """
    @property
    def min_samples_leaf(self) -> builtins.int:
        r"""
        Minimum number of samples required in a leaf node.
        """
    @min_samples_leaf.setter
    def min_samples_leaf(self, value: builtins.int) -> None:
        r"""
        Minimum number of samples required in a leaf node.
        """
    @property
    def min_gain_to_split(self) -> builtins.float:
        r"""
        Minimum gain required to make a split.
        """
    @min_gain_to_split.setter
    def min_gain_to_split(self, value: builtins.float) -> None:
        r"""
        Minimum gain required to make a split.
        """
    @property
    def growth_strategy(self) -> builtins.str:
        r"""
        Growth strategy: "depthwise" or "leafwise".
        """
    @growth_strategy.setter
    def growth_strategy(self, value: builtins.str) -> None:
        r"""
        Growth strategy: "depthwise" or "leafwise".
        """
    def __new__(cls, max_depth: builtins.int = -1, n_leaves: builtins.int = 31, min_samples_leaf: builtins.int = 1, min_gain_to_split: builtins.float = 0.0, growth_strategy: builtins.str = 'depthwise') -> TreeConfig:
        r"""
        Create a new TreeConfig.
        
        Parameters
        ----------
        max_depth : int, default=-1
            Maximum depth of tree. -1 means unlimited.
        n_leaves : int, default=31
            Maximum number of leaves (only used when max_depth=-1).
        min_samples_leaf : int, default=1
            Minimum samples required in a leaf.
        min_gain_to_split : float, default=0.0
            Minimum gain required to make a split.
        growth_strategy : str, default="depthwise"
            Tree growth strategy: "depthwise" or "leafwise".
        """
    def __repr__(self) -> builtins.str: ...

