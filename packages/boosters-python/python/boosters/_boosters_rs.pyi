# This file is automatically generated by pyo3_stub_gen

import builtins
import enum
import typing

import numpy
import numpy.typing

class Dataset:
    r"""
    Internal dataset holding features, labels, and optional metadata.

    This is a low-level binding that accepts pre-validated numpy arrays.
    The Python `Dataset` class extends this to provide user-friendly
    constructors with DataFrame support, type conversion, and validation.

    Note:
        This class expects C-contiguous float32 arrays. The Python subclass
        handles all type conversion, validation, and DataFrame support.

    Attributes:
        n_samples: Number of samples in the dataset.
        n_features: Number of features in the dataset.
        has_labels: Whether labels are present.
        has_weights: Whether weights are present.
        feature_names: Feature names if provided.
        categorical_features: Indices of categorical features.
        shape: Shape as (n_samples, n_features).
    """
    @property
    def n_samples(self) -> builtins.int:
        r"""
        Number of samples in the dataset.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features in the dataset.
        """
    @property
    def has_labels(self) -> builtins.bool:
        r"""
        Whether labels are present.
        """
    @property
    def has_weights(self) -> builtins.bool:
        r"""
        Whether weights are present.
        """
    @property
    def feature_names(self) -> builtins.list[builtins.str] | None:
        r"""
        Feature names if provided.
        """
    @property
    def categorical_features(self) -> builtins.list[builtins.int]:
        r"""
        Indices of categorical features.
        """
    @property
    def shape(self) -> tuple[builtins.int, builtins.int]:
        r"""
        Shape of the features array as (n_samples, n_features).
        """
    def __new__(
        cls,
        features: numpy.typing.NDArray[numpy.float32],
        labels: numpy.typing.NDArray[numpy.float32] | None = None,
        weights: numpy.typing.NDArray[numpy.float32] | None = None,
        feature_names: typing.Sequence[builtins.str] | None = None,
        categorical_features: typing.Sequence[builtins.int] | None = None,
    ) -> Dataset:
        r"""
        Create a new Dataset from pre-validated numpy arrays.

        Args:
            features: C-contiguous float32 array of shape (n_samples, n_features).
            labels: C-contiguous float32 array of shape (n_outputs, n_samples), or None.
            weights: C-contiguous float32 array of shape (n_samples,), or None.
            feature_names: List of feature names, or None.
            categorical_features: List of categorical feature indices, or None.

        Returns:
            Dataset ready for training or prediction.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class EvalSet:
    r"""
    Named evaluation set for model training.

    An EvalSet wraps a Dataset with a name, which is used to identify
    the evaluation set in training logs and `eval_results`.

    # Example

    ```python
    from boosters import Dataset, EvalSet
    import numpy as np

    X_val = np.random.rand(50, 10).astype(np.float32)
    y_val = np.random.rand(50).astype(np.float32)

    val_data = Dataset(X_val, y_val)
    eval_set = EvalSet("validation", val_data)

    # Use in training:
    # model.fit(train_data, valid=[eval_set])
    ```
    """
    @property
    def name(self) -> builtins.str:
        r"""
        Name of this evaluation set.
        """
    @property
    def dataset(self) -> Dataset:
        r"""
        The underlying dataset.
        """
    def __new__(cls, dataset: Dataset, name: builtins.str) -> EvalSet:
        r"""
        Create a new named evaluation set.

        Args:
            name: Name for this evaluation set (e.g., "validation", "test")
            dataset: Dataset containing features and labels.

        Returns:
            EvalSet ready for use in training
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTConfig:
    r"""
    Main configuration for GBDT model.

    This is the primary configuration class for gradient boosted decision trees.
    All parameters are flat (no nested config objects) matching the core Rust API.

    # Arguments

    * `n_estimators` - Number of boosting rounds (trees to train). Default: 100.
    * `learning_rate` - Step size shrinkage (0.01 - 0.3 typical). Default: 0.3.
    * `objective` - Loss function for training. Default: Objective.Squared().
    * `metric` - Evaluation metric. None uses objective's default.
    * `growth_strategy` - Tree growth strategy. Default: GrowthStrategy.Depthwise.
    * `max_depth` - Maximum tree depth (only for depthwise). Default: 6.
    * `n_leaves` - Maximum leaves (only for leafwise). Default: 31.
    * `max_onehot_cats` - Max categories for one-hot encoding. Default: 4.
    * `l1` - L1 regularization on leaf weights. Default: 0.0.
    * `l2` - L2 regularization on leaf weights. Default: 1.0.
    * `min_gain_to_split` - Minimum gain required to make a split. Default: 0.0.
    * `min_child_weight` - Minimum sum of hessians in a leaf. Default: 1.0.
    * `min_samples_leaf` - Minimum samples in a leaf. Default: 1.
    * `subsample` - Row subsampling ratio per tree. Default: 1.0.
    * `colsample_bytree` - Column subsampling per tree. Default: 1.0.
    * `colsample_bylevel` - Column subsampling per level. Default: 1.0.
    * `linear_leaves` - Enable linear models in leaves (experimental). Default: False.
    * `linear_l2` - L2 regularization for linear coefficients. Default: 0.01.
    * `linear_l1` - L1 regularization for linear coefficients. Default: 0.0.
    * `early_stopping_rounds` - Stop if no improvement for this many rounds.
    * `seed` - Random seed for reproducibility. Default: 42.

    # Example (Python)

    ```text
    config = GBDTConfig(
        n_estimators=500,
        learning_rate=0.1,
        objective=Objective.logistic(),
        max_depth=6,
        l2=1.0,
    )
    ```
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size shrinkage).
        """
    @property
    def growth_strategy(self) -> GrowthStrategy:
        r"""
        Growth strategy for tree building.
        """
    @property
    def max_depth(self) -> builtins.int:
        r"""
        Maximum depth of tree (for depthwise growth).
        """
    @property
    def n_leaves(self) -> builtins.int:
        r"""
        Maximum number of leaves (for leafwise growth).
        """
    @property
    def max_onehot_cats(self) -> builtins.int:
        r"""
        Maximum categories for one-hot encoding categorical splits.
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization on leaf weights.
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization on leaf weights.
        """
    @property
    def min_gain_to_split(self) -> builtins.float:
        r"""
        Minimum gain required to make a split.
        """
    @property
    def min_child_weight(self) -> builtins.float:
        r"""
        Minimum sum of hessians required in a leaf.
        """
    @property
    def min_samples_leaf(self) -> builtins.int:
        r"""
        Minimum number of samples required in a leaf.
        """
    @property
    def subsample(self) -> builtins.float:
        r"""
        Row subsampling ratio per tree.
        """
    @property
    def colsample_bytree(self) -> builtins.float:
        r"""
        Column subsampling ratio per tree.
        """
    @property
    def colsample_bylevel(self) -> builtins.float:
        r"""
        Column subsampling ratio per level.
        """
    @property
    def linear_leaves(self) -> builtins.bool:
        r"""
        Enable linear models in leaves.
        """
    @property
    def linear_l2(self) -> builtins.float:
        r"""
        L2 regularization for linear coefficients.
        """
    @property
    def linear_l1(self) -> builtins.float:
        r"""
        L1 regularization for linear coefficients.
        """
    @property
    def early_stopping_rounds(self) -> builtins.int | None:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def objective(self) -> Objective:
        r"""
        Get the objective function.
        """
    @property
    def metric(self) -> Metric | None:
        r"""
        Get the evaluation metric (or None).
        """
    def __new__(
        cls,
        n_estimators: builtins.int = 100,
        learning_rate: builtins.float = 0.3,
        objective: Objective | None = None,
        metric: Metric | None = None,
        growth_strategy: GrowthStrategy = GrowthStrategy.Depthwise,
        max_depth: builtins.int = 6,
        n_leaves: builtins.int = 31,
        max_onehot_cats: builtins.int = 4,
        l1: builtins.float = 0.0,
        l2: builtins.float = 1.0,
        min_gain_to_split: builtins.float = 0.0,
        min_child_weight: builtins.float = 1.0,
        min_samples_leaf: builtins.int = 1,
        subsample: builtins.float = 1.0,
        colsample_bytree: builtins.float = 1.0,
        colsample_bylevel: builtins.float = 1.0,
        linear_leaves: builtins.bool = False,
        linear_l2: builtins.float = 0.01,
        linear_l1: builtins.float = 0.0,
        early_stopping_rounds: builtins.int | None = None,
        seed: builtins.int = 42,
    ) -> GBDTConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTModel:
    r"""
    Gradient Boosted Decision Tree model.

    This is the main model class for training and prediction with gradient
    boosted decision trees.

    Attributes:
        is_fitted: Whether the model has been fitted.
        n_trees: Number of trees in the fitted model.
        n_features: Number of features the model was trained on.
        best_iteration: Best iteration from early stopping.
        best_score: Best score from early stopping.
        eval_results: Evaluation results from training.
        config: Model configuration.

    Examples:
        >>> from boosters import GBDTModel, Dataset
        >>> train = Dataset(X, y)
        >>> model = GBDTModel().fit(train)
        >>> predictions = model.predict(train)
    """
    @property
    def is_fitted(self) -> builtins.bool:
        r"""
        Whether the model has been fitted.
        """
    @property
    def n_trees(self) -> builtins.int:
        r"""
        Number of trees in the fitted model.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        """
    @property
    def best_iteration(self) -> builtins.int | None:
        r"""
        Best iteration from early stopping.
        """
    @property
    def best_score(self) -> builtins.float | None:
        r"""
        Best score from early stopping.
        """
    @property
    def eval_results(self) -> dict[str, dict[str, list[float]]] | None:
        r"""
        Evaluation results from training.
        """
    @property
    def config(self) -> GBDTConfig:
        r"""
        Get the model configuration.
        """
    def __new__(cls, config: GBDTConfig | None = None) -> GBDTModel:
        r"""
        Create a new GBDT model.

        Args:
            config: Optional GBDTConfig. If not provided, uses default config.

        Returns:
            New GBDTModel instance (not yet fitted).
        """
    def feature_importance(
        self, importance_type: ImportanceType = ImportanceType.Split
    ) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Get feature importance scores.

        Args:
            importance_type: Type of importance (ImportanceType.Split or ImportanceType.Gain).

        Returns:
            Array of importance scores, one per feature.
        """
    def shap_values(self, data: Dataset) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Compute SHAP values for feature contribution analysis.

        Args:
            data: Dataset containing features for SHAP computation.

        Returns:
            Array with shape (n_samples, n_features + 1, n_outputs).
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def predict(
        self, data: Dataset, n_threads: builtins.int = 0
    ) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make predictions on data.

        Returns transformed predictions (e.g., probabilities for classification).
        Output shape is (n_samples, n_outputs) - sklearn convention.

        Args:
            data: Dataset containing features for prediction.
            n_threads: Number of threads for parallel prediction (0 = auto).

        Returns:
            Predictions array with shape (n_samples, n_outputs).
        """
    def predict_raw(
        self, data: Dataset, n_threads: builtins.int = 0
    ) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make raw (untransformed) predictions on data.

        Returns raw margin scores without transformation.
        Output shape is (n_samples, n_outputs) - sklearn convention.

        Args:
            data: Dataset containing features for prediction.
            n_threads: Number of threads for parallel prediction (0 = auto).

        Returns:
            Raw scores array with shape (n_samples, n_outputs).
        """
    def fit(
        self,
        train: Dataset,
        valid: typing.Sequence[EvalSet] | None = None,
        n_threads: builtins.int = 0,
    ) -> GBDTModel:
        r"""
        Train the model on a dataset.

        Args:
            train: Training dataset containing features and labels.
            valid: Validation set(s) for early stopping and evaluation.
            n_threads: Number of threads for parallel training (0 = auto).

        Returns:
            Self (for method chaining).
        """

@typing.final
class GBLinearConfig:
    r"""
    Main configuration for GBLinear model.

    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.

    Args:
        n_estimators: Number of boosting rounds. Default: 100.
        learning_rate: Step size for weight updates. Default: 0.5.
        objective: Loss function for training. Default: Objective.Squared().
        metric: Evaluation metric. None uses objective's default.
        l1: L1 regularization (alpha). Encourages sparse weights. Default: 0.0.
        l2: L2 regularization (lambda). Prevents large weights. Default: 1.0.
        early_stopping_rounds: Stop if no improvement for this many rounds.
        seed: Random seed for reproducibility. Default: 42.

    Examples:
        >>> config = GBLinearConfig(
        ...     n_estimators=200,
        ...     learning_rate=0.3,
        ...     objective=Objective.logistic(),
        ...     l2=0.1,
        ... )
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size).
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization (alpha).
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization (lambda).
        """
    @property
    def early_stopping_rounds(self) -> builtins.int | None:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def objective(self) -> Objective:
        r"""
        Get the objective function.
        """
    @property
    def metric(self) -> Metric | None:
        r"""
        Get the evaluation metric (or None).
        """
    def __new__(
        cls,
        n_estimators: builtins.int = 100,
        learning_rate: builtins.float = 0.5,
        objective: Objective | None = None,
        metric: Metric | None = None,
        l1: builtins.float = 0.0,
        l2: builtins.float = 1.0,
        early_stopping_rounds: builtins.int | None = None,
        seed: builtins.int = 42,
    ) -> GBLinearConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBLinearModel:
    r"""
    Gradient Boosted Linear model.

    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.

    Attributes:
        coef_: Model coefficients after fitting.
        intercept_: Model intercept after fitting.
        is_fitted: Whether the model has been trained.
        n_features_in_: Number of features seen during fit.

    Examples:
        >>> config = GBLinearConfig(n_estimators=50, learning_rate=0.3)
        >>> model = GBLinearModel(config=config).fit(train)
        >>> predictions = model.predict(X_test)
    """
    @property
    def is_fitted(self) -> builtins.bool:
        r"""
        Whether the model has been fitted.
        """
    @property
    def n_features_in_(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        """
    @property
    def coef_(self) -> typing.Any:
        r"""
        Model coefficients (weights).

        Returns:
            Array with shape (n_features,) for single-output models
            or (n_features, n_outputs) for multi-output models.
        """
    @property
    def intercept_(self) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Model intercept (bias).

        Returns:
            Array of shape (n_outputs,).
        """
    @property
    def best_iteration(self) -> builtins.int | None:
        r"""
        Best iteration from early stopping.
        """
    @property
    def best_score(self) -> builtins.float | None:
        r"""
        Best score from early stopping.
        """
    @property
    def eval_results(self) -> dict[str, dict[str, list[float]]] | None:
        r"""
        Evaluation results from training.
        """
    @property
    def config(self) -> GBLinearConfig:
        r"""
        Get the model configuration.
        """
    def __new__(cls, config: GBLinearConfig | None = None) -> GBLinearModel:
        r"""
        Create a new GBLinear model.

        Args:
            config: Optional GBLinearConfig. If not provided, uses default config.

        Returns:
            New GBLinearModel instance (not yet fitted).
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def predict(
        self, data: Dataset, n_threads: builtins.int = 0
    ) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make predictions on features.

        Returns transformed predictions (e.g., probabilities for classification).
        Output shape is (n_samples, n_outputs) - sklearn convention.

        Args:
            data: Dataset containing features.
            n_threads: Number of threads (unused, for API consistency).

        Returns:
            Predictions array with shape (n_samples, n_outputs).
        """
    def predict_raw(
        self, data: Dataset, n_threads: builtins.int = 0
    ) -> numpy.typing.NDArray[numpy.float32]:
        r"""
        Make raw (untransformed) predictions on features.

        Returns raw margin scores without transformation.
        Output shape is (n_samples, n_outputs) - sklearn convention.

        Args:
            data: Dataset containing features.
            n_threads: Number of threads (unused, for API consistency).

        Returns:
            Raw scores array with shape (n_samples, n_outputs).
        """
    def fit(
        self,
        train: Dataset,
        eval_set: typing.Sequence[EvalSet] | None = None,
        n_threads: builtins.int = 0,
    ) -> GBLinearModel:
        r"""
        Train the model on a dataset.

        Args:
            train: Training dataset containing features and labels.
            eval_set: Validation set(s) for early stopping and evaluation.
            n_threads: Number of threads for parallel training (0 = auto).

        Returns:
            Self (for method chaining).
        """

@typing.final
class GrowthStrategy(enum.Enum):
    r"""
    Tree growth strategy for building decision trees.

    Determines the order in which nodes are expanded during tree construction.
    Acts like a Python StrEnum - can be compared to strings.

    Attributes:
        Depthwise: Grow tree level-by-level (like XGBoost).
            All nodes at depth d are expanded before any node at depth d+1.
            More balanced trees, better for shallow trees.
        Leafwise: Grow tree by best-first split (like LightGBM).
            Always expands the leaf with highest gain.
            Can produce deeper, more accurate trees but risks overfitting.

    Examples:
        >>> from boosters import GrowthStrategy
        >>> strategy = GrowthStrategy.Depthwise
        >>> strategy == "depthwise"
        True
        >>> str(strategy)
        'depthwise'
    """

    Depthwise = ...
    r"""
    Grow tree level-by-level (like XGBoost).
    """
    Leafwise = ...
    r"""
    Grow tree by best-first split (like LightGBM).
    """

    def __str__(self) -> builtins.str:
        r"""
        String representation like StrEnum.
        """
    def __repr__(self) -> builtins.str: ...
    def __hash__(self) -> builtins.int:
        r"""
        Hash using the string value for StrEnum-like behavior.
        """
    def __reduce__(self) -> tuple[typing.Any, tuple[builtins.str]]:
        r"""
        Pickle support: reduce to module path and variant name.
        """
    def __deepcopy__(self, _memo: typing.Any) -> GrowthStrategy:
        r"""
        Deepcopy support - return self since enum variants are singletons.
        """
    def __copy__(self) -> GrowthStrategy:
        r"""
        Copy support - return self since enum variants are singletons.
        """

@typing.final
class ImportanceType(enum.Enum):
    r"""
    Type of feature importance to compute.

    Attributes:
        Split: Number of times each feature is used in splits.
        Gain: Total gain from splits using each feature.

    Examples:
        >>> from boosters import ImportanceType
        >>> importance = model.feature_importance(ImportanceType.Gain)
    """

    Split = ...
    r"""
    Number of times each feature is used in splits.
    """
    Gain = ...
    r"""
    Total gain from splits using each feature.
    """

    def __str__(self) -> builtins.str:
        r"""
        String representation.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Metric(enum.Enum):
    r"""
    Evaluation metrics for gradient boosting.

    Each variant represents a different metric for evaluating model performance.
    Use the static constructor methods for validation.

    Regression:
        - Metric.Rmse(): Root Mean Squared Error
        - Metric.Mae(): Mean Absolute Error
        - Metric.Mape(): Mean Absolute Percentage Error

    Classification:
        - Metric.LogLoss(): Binary cross-entropy
        - Metric.Auc(): Area Under ROC Curve
        - Metric.Accuracy(): Classification accuracy

    Ranking:
        - Metric.Ndcg(at): Normalized Discounted Cumulative Gain@k

    Examples
    --------
    >>> from boosters import Metric
    >>> metric = Metric.rmse()  # Regression
    >>> metric = Metric.auc()  # Binary classification
    >>> metric = Metric.ndcg(at=5)  # Ranking

    Pattern matching:
    >>> match metric:
    ...     case Metric.Rmse():
    ...         print("RMSE")
    ...     case Metric.Ndcg(at=k):
    ...         print(f"NDCG@{k}")
    """

    Rmse = ...
    r"""
    Root Mean Squared Error for regression.
    """
    Mae = ...
    r"""
    Mean Absolute Error for regression.
    """
    Mape = ...
    r"""
    Mean Absolute Percentage Error for regression.
    """
    LogLoss = ...
    r"""
    Binary Log Loss (cross-entropy) for classification.
    """
    Auc = ...
    r"""
    Area Under ROC Curve for binary classification.
    """
    Accuracy = ...
    r"""
    Classification accuracy (binary or multiclass).
    """
    Ndcg = ...
    r"""
    Normalized Discounted Cumulative Gain for ranking.

    Parameters:
        at: Truncation point for NDCG calculation (NDCG@k). Default: 10.
    """

    @staticmethod
    def rmse() -> Metric:
        r"""
        Create RMSE metric.
        """
    @staticmethod
    def mae() -> Metric:
        r"""
        Create MAE metric.
        """
    @staticmethod
    def mape() -> Metric:
        r"""
        Create MAPE metric.
        """
    @staticmethod
    def logloss() -> Metric:
        r"""
        Create log loss metric.
        """
    @staticmethod
    def auc() -> Metric:
        r"""
        Create AUC metric.
        """
    @staticmethod
    def accuracy() -> Metric:
        r"""
        Create accuracy metric.
        """
    @staticmethod
    def ndcg(at: builtins.int = 10) -> Metric:
        r"""
        Create NDCG@k metric with validation.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Objective(enum.Enum):
    r"""
    Objective (loss) functions for gradient boosting.

    Each variant represents a different loss function for training GBDT and
    GBLinear models. Use the static constructor methods for validation.

    Regression:
        - Objective.Squared(): Mean squared error (L2)
        - Objective.Absolute(): Mean absolute error (L1)
        - Objective.Huber(delta): Pseudo-Huber loss (robust)
        - Objective.Pinball(alpha): Quantile regression
        - Objective.Poisson(): Poisson deviance for count data

    Classification:
        - Objective.Logistic(): Binary cross-entropy
        - Objective.Hinge(): SVM-style hinge loss
        - Objective.Softmax(n_classes): Multiclass cross-entropy

    Ranking:
        - Objective.LambdaRank(ndcg_at): LambdaMART for NDCG optimization

    Examples
    --------
    >>> from boosters import Objective
    >>> obj = Objective.squared()  # L2 regression
    >>> obj = Objective.logistic()  # Binary classification
    >>> obj = Objective.pinball([0.1, 0.5, 0.9])  # Quantile regression
    >>> obj = Objective.softmax(10)  # Multiclass classification

    Pattern matching:
    >>> match obj:
    ...     case Objective.Squared():
    ...         print("L2 loss")
    ...     case Objective.Pinball(alpha=a):
    ...         print(f"Quantile: {a}")
    """

    Squared = ...
    r"""
    Squared error loss (L2) for regression.
    """
    Absolute = ...
    r"""
    Absolute error loss (L1) for robust regression.
    """
    Poisson = ...
    r"""
    Poisson loss for count regression.
    """
    Logistic = ...
    r"""
    Logistic loss for binary classification.
    """
    Hinge = ...
    r"""
    Hinge loss for binary classification (SVM-style).
    """
    Huber = ...
    r"""
    Pseudo-Huber loss for robust regression.

    Parameters:
        delta: Transition point between quadratic and linear loss. Default: 1.0.
    """
    Pinball = ...
    r"""
    Pinball loss for quantile regression.

    Parameters:
        alpha: List of quantiles to predict. Each value must be in (0, 1).
    """
    Softmax = ...
    r"""
    Softmax loss for multiclass classification.

    Parameters:
        n_classes: Number of classes. Must be >= 2.
    """
    LambdaRank = ...
    r"""
    LambdaRank loss for learning to rank.

    Parameters:
        ndcg_at: Truncation point for NDCG calculation. Default: 10.
    """

    @staticmethod
    def squared() -> Objective:
        r"""
        Create squared error loss (L2).
        """
    @staticmethod
    def absolute() -> Objective:
        r"""
        Create absolute error loss (L1).
        """
    @staticmethod
    def poisson() -> Objective:
        r"""
        Create Poisson loss.
        """
    @staticmethod
    def logistic() -> Objective:
        r"""
        Create logistic loss for binary classification.
        """
    @staticmethod
    def hinge() -> Objective:
        r"""
        Create hinge loss for binary classification.
        """
    @staticmethod
    def huber(delta: builtins.float = 1.0) -> Objective:
        r"""
        Create Huber loss with validation.
        """
    @staticmethod
    def pinball(alpha: typing.Sequence[builtins.float]) -> Objective:
        r"""
        Create pinball loss with validation.
        """
    @staticmethod
    def softmax(n_classes: builtins.int) -> Objective:
        r"""
        Create softmax loss with validation.
        """
    @staticmethod
    def lambdarank(ndcg_at: builtins.int = 10) -> Objective:
        r"""
        Create LambdaRank loss with validation.
        """
    def __repr__(self) -> builtins.str: ...
