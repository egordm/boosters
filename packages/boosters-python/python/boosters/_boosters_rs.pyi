# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import builtins
import enum
import numpy
import numpy.typing
import typing

@typing.final
class CategoricalConfig:
    r"""
    Configuration for categorical feature handling.
    
    Boosters supports native categorical splits (like LightGBM) using
    bitset-based multi-way splits rather than one-hot encoding.
    
    Attributes:
        max_categories: Maximum categories for native categorical splits.
        min_category_count: Minimum samples per category.
        max_onehot: Maximum categories for one-hot encoding.
    
    Examples:
        >>> config = CategoricalConfig(max_categories=256)
        >>> config.max_categories
        256
    """
    @property
    def max_categories(self) -> builtins.int:
        r"""
        Maximum number of categories for native categorical splits.
        Categories beyond this are treated as continuous.
        """
    @max_categories.setter
    def max_categories(self, value: builtins.int) -> None:
        r"""
        Maximum number of categories for native categorical splits.
        Categories beyond this are treated as continuous.
        """
    @property
    def min_category_count(self) -> builtins.int:
        r"""
        Minimum count for a category to be considered.
        Categories with fewer samples are grouped into "other".
        """
    @min_category_count.setter
    def min_category_count(self, value: builtins.int) -> None:
        r"""
        Minimum count for a category to be considered.
        Categories with fewer samples are grouped into "other".
        """
    @property
    def max_onehot(self) -> builtins.int:
        r"""
        Maximum categories for one-hot encoding fallback.
        If a feature has more categories, use bitset splits.
        """
    @max_onehot.setter
    def max_onehot(self, value: builtins.int) -> None:
        r"""
        Maximum categories for one-hot encoding fallback.
        If a feature has more categories, use bitset splits.
        """
    def __new__(cls, max_categories: builtins.int = 256, min_category_count: builtins.int = 10, max_onehot: builtins.int = 4) -> CategoricalConfig:
        r"""
        Create a new CategoricalConfig.
        
        Args:
            max_categories: Maximum categories for native splits. Default: 256.
            min_category_count: Minimum samples per category. Default: 10.
            max_onehot: Maximum categories for one-hot encoding. Default: 4.
        """
    def __repr__(self) -> builtins.str: ...

class Dataset:
    r"""
    Internal dataset holding features, labels, and optional metadata.
    
    This is a low-level binding that accepts pre-validated numpy arrays.
    The Python `Dataset` class extends this to provide user-friendly
    constructors with DataFrame support, type conversion, and validation.
    
    Note:
        This class expects C-contiguous float32 arrays. The Python subclass
        handles all type conversion, validation, and DataFrame support.
    
    Attributes:
        n_samples: Number of samples in the dataset.
        n_features: Number of features in the dataset.
        has_labels: Whether labels are present.
        has_weights: Whether weights are present.
        feature_names: Feature names if provided.
        categorical_features: Indices of categorical features.
        shape: Shape as (n_samples, n_features).
    """
    @property
    def n_samples(self) -> builtins.int:
        r"""
        Number of samples in the dataset.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features in the dataset.
        """
    @property
    def has_labels(self) -> builtins.bool:
        r"""
        Whether labels are present.
        """
    @property
    def has_weights(self) -> builtins.bool:
        r"""
        Whether weights are present.
        """
    @property
    def feature_names(self) -> typing.Optional[builtins.list[builtins.str]]:
        r"""
        Feature names if provided.
        """
    @property
    def categorical_features(self) -> builtins.list[builtins.int]:
        r"""
        Indices of categorical features.
        """
    @property
    def shape(self) -> tuple[builtins.int, builtins.int]:
        r"""
        Shape of the features array as (n_samples, n_features).
        """
    def __new__(cls, features: numpy.typing.NDArray[numpy.float32], labels: typing.Optional[numpy.typing.NDArray[numpy.float32]] = None, weights: typing.Optional[numpy.typing.NDArray[numpy.float32]] = None, feature_names: typing.Optional[typing.Sequence[builtins.str]] = None, categorical_features: typing.Optional[typing.Sequence[builtins.int]] = None) -> Dataset:
        r"""
        Create a new Dataset from pre-validated numpy arrays.
        
        Args:
            features: C-contiguous float32 array of shape (n_samples, n_features).
            labels: C-contiguous float32 array of shape (n_outputs, n_samples), or None.
            weights: C-contiguous float32 array of shape (n_samples,), or None.
            feature_names: List of feature names, or None.
            categorical_features: List of categorical feature indices, or None.
        
        Returns:
            Dataset ready for training or prediction.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class EFBConfig:
    r"""
    Configuration for Exclusive Feature Bundling.
    
    EFB bundles mutually exclusive features to reduce memory and computation,
    similar to LightGBM's implementation.
    
    Attributes:
        enable: Whether to enable EFB.
        max_conflict_rate: Maximum conflict rate for bundling features.
    
    Examples:
        >>> config = EFBConfig(enable=True, max_conflict_rate=0.0)
        >>> config.enable
        True
    """
    @property
    def enable(self) -> builtins.bool:
        r"""
        Whether to enable Exclusive Feature Bundling.
        """
    @enable.setter
    def enable(self, value: builtins.bool) -> None:
        r"""
        Whether to enable Exclusive Feature Bundling.
        """
    @property
    def max_conflict_rate(self) -> builtins.float:
        r"""
        Maximum conflict rate allowed when bundling features.
        0.0 means only truly exclusive features are bundled.
        """
    @max_conflict_rate.setter
    def max_conflict_rate(self, value: builtins.float) -> None:
        r"""
        Maximum conflict rate allowed when bundling features.
        0.0 means only truly exclusive features are bundled.
        """
    def __new__(cls, enable: builtins.bool = True, max_conflict_rate: builtins.float = 0.0) -> EFBConfig:
        r"""
        Create a new EFBConfig.
        
        Args:
            enable: Whether to enable EFB. Default: True.
            max_conflict_rate: Maximum conflict rate. Must be in [0, 1). Default: 0.0.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class EvalSet:
    r"""
    Named evaluation set for model training.
    
    An EvalSet wraps a Dataset with a name, which is used to identify
    the evaluation set in training logs and `eval_results`.
    
    # Example
    
    ```python
    from boosters import Dataset, EvalSet
    import numpy as np
    
    X_val = np.random.rand(50, 10).astype(np.float32)
    y_val = np.random.rand(50).astype(np.float32)
    
    val_data = Dataset(X_val, y_val)
    eval_set = EvalSet("validation", val_data)
    
    # Use in training:
    # model.fit(train_data, valid=[eval_set])
    ```
    """
    @property
    def name(self) -> builtins.str:
        r"""
        Name of this evaluation set.
        """
    @property
    def dataset(self) -> Dataset:
        r"""
        The underlying dataset.
        """
    def __new__(cls, dataset: Dataset, name: builtins.str) -> EvalSet:
        r"""
        Create a new named evaluation set.
        
        Args:
            name: Name for this evaluation set (e.g., "validation", "test")
            dataset: Dataset containing features and labels.
        
        Returns:
            EvalSet ready for use in training
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTConfig:
    r"""
    Main configuration for GBDT model.
    
    This is the primary configuration class for gradient boosted decision trees.
    It accepts nested configuration objects for tree structure, regularization,
    sampling, etc.
    
    Args:
        n_estimators: Number of boosting rounds (trees to train). Default: 100.
        learning_rate: Step size shrinkage (0.01 - 0.3 typical). Default: 0.3.
        objective: Loss function for training. Default: Objective.Squared().
        metric: Evaluation metric. None uses objective's default.
        tree: Tree structure parameters.
        regularization: L1/L2 regularization parameters.
        sampling: Row and column subsampling parameters.
        categorical: Categorical feature handling.
        efb: Exclusive Feature Bundling config.
        linear_leaves: Linear model in leaves config. None = disabled.
        early_stopping_rounds: Stop if no improvement for this many rounds.
        seed: Random seed for reproducibility. Default: 42.
    
    Examples:
        >>> config = GBDTConfig(
        ...     n_estimators=500,
        ...     learning_rate=0.1,
        ...     objective=Objective.logistic(),
        ...     tree=TreeConfig(max_depth=6),
        ... )
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size shrinkage).
        """
    @property
    def tree(self) -> TreeConfig:
        r"""
        Tree structure config.
        """
    @property
    def regularization(self) -> RegularizationConfig:
        r"""
        Regularization config.
        """
    @property
    def sampling(self) -> SamplingConfig:
        r"""
        Sampling config.
        """
    @property
    def categorical(self) -> CategoricalConfig:
        r"""
        Categorical feature config.
        """
    @property
    def efb(self) -> EFBConfig:
        r"""
        Exclusive Feature Bundling config.
        """
    @property
    def linear_leaves(self) -> typing.Optional[LinearLeavesConfig]:
        r"""
        Linear leaves config (None = disabled).
        """
    @property
    def early_stopping_rounds(self) -> typing.Optional[builtins.int]:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def objective(self) -> Objective:
        r"""
        Get the objective function.
        """
    @property
    def metric(self) -> Metric | None:
        r"""
        Get the evaluation metric (or None).
        """
    def __new__(cls, n_estimators: builtins.int = 100, learning_rate: builtins.float = 0.3, objective: Objective | None = None, metric: Metric | None = None, tree: typing.Optional[TreeConfig] = None, regularization: typing.Optional[RegularizationConfig] = None, sampling: typing.Optional[SamplingConfig] = None, categorical: typing.Optional[CategoricalConfig] = None, efb: typing.Optional[EFBConfig] = None, linear_leaves: typing.Optional[LinearLeavesConfig] = None, early_stopping_rounds: typing.Optional[builtins.int] = None, seed: builtins.int = 42) -> GBDTConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBDTModel:
    r"""
    Gradient Boosted Decision Tree model.
    
    This is the main model class for training and prediction with gradient
    boosted decision trees.
    
    Attributes:
        is_fitted: Whether the model has been fitted.
        n_trees: Number of trees in the fitted model.
        n_features: Number of features the model was trained on.
        best_iteration: Best iteration from early stopping.
        best_score: Best score from early stopping.
        eval_results: Evaluation results from training.
        config: Model configuration.
    
    Examples:
        >>> from boosters import GBDTModel, Dataset
        >>> train = Dataset(X, y)
        >>> model = GBDTModel().fit(train)
        >>> predictions = model.predict(train)
    """
    @property
    def is_fitted(self) -> builtins.bool:
        r"""
        Whether the model has been fitted.
        """
    @property
    def n_trees(self) -> builtins.int:
        r"""
        Number of trees in the fitted model.
        
        Raises:
            ValueError: If model has not been fitted.
        """
    @property
    def n_features(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        
        Raises:
            ValueError: If model has not been fitted.
        """
    @property
    def best_iteration(self) -> typing.Optional[builtins.int]:
        r"""
        Best iteration from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def best_score(self) -> typing.Optional[builtins.float]:
        r"""
        Best score from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def eval_results(self) -> dict[str, dict[str, list[float]]] | None:
        r"""
        Evaluation results from training.
        
        Returns a dict mapping eval set names to dicts of metric names to lists
        of scores per iteration.
        
        Examples:
            >>> results = model.eval_results
            >>> # {"train": {"rmse": [0.5, 0.4, ...]}}
        """
    @property
    def config(self) -> GBDTConfig:
        r"""
        Get the model configuration.
        """
    def __new__(cls, config: typing.Optional[GBDTConfig] = None) -> GBDTModel:
        r"""
        Create a new GBDT model.
        
        Args:
            config: Optional GBDTConfig. If not provided, uses default config.
        
        Returns:
            New GBDTModel instance (not yet fitted).
        """
    def feature_importance(self, importance_type: typing.Literal['split', 'gain'] = "split") -> numpy.ndarray:
        r"""
        Get feature importance scores.
        
        Args:
            importance_type: Type of importance: "split" or "gain".
        
        Returns:
            Array of importance scores, one per feature.
        
        Raises:
            ValueError: If model has not been fitted.
        """
    def shap_values(self, data: Dataset) -> numpy.ndarray:
        r"""
        Compute SHAP values for feature contribution analysis.
        
        SHAP (SHapley Additive exPlanations) values show how each feature
        contributes to individual predictions.
        
        Args:
            data: Dataset containing features for SHAP computation.
        
        Returns:
            Array with shape (n_samples, n_features + 1, n_outputs).
            The last feature index contains the base value.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Examples:
            >>> shap_values = model.shap_values(test_data)
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def predict(self, data: Dataset, n_iterations: typing.Optional[builtins.int] = None) -> numpy.ndarray:
        r"""
        Make predictions on data.
        
        Returns transformed predictions (e.g., probabilities for classification).
        
        Args:
            data: Dataset containing features for prediction.
            n_iterations: Number of trees to use. Defaults to all trees.
        
        Returns:
            Predictions of shape (n_samples, n_outputs).
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Examples:
            >>> predictions = model.predict(test_data)
        """
    def predict_raw(self, data: Dataset, n_iterations: typing.Optional[builtins.int] = None) -> numpy.ndarray:
        r"""
        Make raw (untransformed) predictions on data.
        
        Returns raw margin scores without transformation.
        For classification this means logits instead of probabilities.
        
        Args:
            data: Dataset containing features for prediction.
            n_iterations: Number of trees to use. Defaults to all trees.
        
        Returns:
            Raw scores of shape (n_samples, n_outputs).
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Examples:
            >>> raw_margins = model.predict_raw(test_data)
        """
    def fit(self, train: Dataset, valid: typing.Optional[typing.Sequence[EvalSet]] = None) -> GBDTModel:
        r"""
        Train the model on a dataset.
        
        Args:
            train: Training dataset containing features and labels.
            valid: Validation set(s) for early stopping and evaluation.
        
        Returns:
            Self (for method chaining).
        
        Raises:
            ValueError: If training data is invalid or labels are missing.
        
        Examples:
            >>> model = GBDTModel().fit(train_dataset)
        """

@typing.final
class GBLinearConfig:
    r"""
    Main configuration for GBLinear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    Args:
        n_estimators: Number of boosting rounds. Default: 100.
        learning_rate: Step size for weight updates. Default: 0.5.
        objective: Loss function for training. Default: Objective.Squared().
        metric: Evaluation metric. None uses objective's default.
        l1: L1 regularization (alpha). Encourages sparse weights. Default: 0.0.
        l2: L2 regularization (lambda). Prevents large weights. Default: 1.0.
        early_stopping_rounds: Stop if no improvement for this many rounds.
        seed: Random seed for reproducibility. Default: 42.
    
    Examples:
        >>> config = GBLinearConfig(
        ...     n_estimators=200,
        ...     learning_rate=0.3,
        ...     objective=Objective.logistic(),
        ...     l2=0.1,
        ... )
    """
    @property
    def n_estimators(self) -> builtins.int:
        r"""
        Number of boosting rounds.
        """
    @property
    def learning_rate(self) -> builtins.float:
        r"""
        Learning rate (step size).
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization (alpha).
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization (lambda).
        """
    @property
    def early_stopping_rounds(self) -> typing.Optional[builtins.int]:
        r"""
        Early stopping rounds (None = disabled).
        """
    @property
    def seed(self) -> builtins.int:
        r"""
        Random seed.
        """
    @property
    def objective(self) -> Objective:
        r"""
        Get the objective function.
        """
    @property
    def metric(self) -> Metric | None:
        r"""
        Get the evaluation metric (or None).
        """
    def __new__(cls, n_estimators: builtins.int = 100, learning_rate: builtins.float = 0.5, objective: Objective | None = None, metric: Metric | None = None, l1: builtins.float = 0.0, l2: builtins.float = 1.0, early_stopping_rounds: typing.Optional[builtins.int] = None, seed: builtins.int = 42) -> GBLinearConfig: ...
    def __repr__(self) -> builtins.str: ...

@typing.final
class GBLinearModel:
    r"""
    Gradient Boosted Linear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    Attributes:
        coef_: Model coefficients after fitting.
        intercept_: Model intercept after fitting.
        is_fitted: Whether the model has been trained.
        n_features_in_: Number of features seen during fit.
    
    Examples:
        >>> config = GBLinearConfig(n_estimators=50, learning_rate=0.3)
        >>> model = GBLinearModel(config=config).fit(train)
        >>> predictions = model.predict(X_test)
    """
    @property
    def is_fitted(self) -> builtins.bool:
        r"""
        Whether the model has been fitted.
        """
    @property
    def n_features_in_(self) -> builtins.int:
        r"""
        Number of features the model was trained on.
        
        Raises:
            RuntimeError: If model has not been fitted.
        """
    @property
    def coef_(self) -> numpy.ndarray:
        r"""
        Model coefficients (weights).
        
        Returns:
            Array with shape (n_features,) for single-output models
            or (n_features, n_outputs) for multi-output models.
        
        Raises:
            RuntimeError: If model has not been fitted.
        """
    @property
    def intercept_(self) -> numpy.ndarray:
        r"""
        Model intercept (bias).
        
        Returns:
            Scalar for single-output models or array of shape (n_outputs,)
            for multi-output models.
        
        Raises:
            RuntimeError: If model has not been fitted.
        """
    @property
    def best_iteration(self) -> typing.Optional[builtins.int]:
        r"""
        Best iteration from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def best_score(self) -> typing.Optional[builtins.float]:
        r"""
        Best score from early stopping.
        
        Returns None if early stopping was not used or not triggered.
        """
    @property
    def eval_results(self) -> dict[str, dict[str, list[float]]] | None:
        r"""
        Evaluation results from training.
        
        Returns a dict mapping eval set names to dicts of metric names to lists
        of scores per iteration.
        """
    @property
    def config(self) -> GBLinearConfig:
        r"""
        Get the model configuration.
        """
    def __new__(cls, config: typing.Optional[GBLinearConfig] = None) -> GBLinearModel:
        r"""
        Create a new GBLinear model.
        
        Args:
            config: Optional GBLinearConfig. If not provided, uses default config.
        
        Returns:
            New GBLinearModel instance (not yet fitted).
        """
    def __repr__(self) -> builtins.str:
        r"""
        String representation.
        """
    def predict(self, data: Dataset) -> numpy.ndarray:
        r"""
        Make predictions on features.
        
        Returns transformed predictions (e.g., probabilities for classification).
        
        Args:
            data: Dataset containing features.
        
        Returns:
            Predictions of shape (n_samples, n_outputs).
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Examples:
            >>> predictions = model.predict(test_data)
        """
    def predict_raw(self, data: Dataset) -> numpy.ndarray:
        r"""
        Make raw (untransformed) predictions on features.
        
        Returns raw margin scores without transformation.
        For classification this means logits instead of probabilities.
        
        Args:
            data: Dataset containing features.
        
        Returns:
            Raw scores of shape (n_samples, n_outputs).
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Examples:
            >>> raw_margins = model.predict_raw(test_data)
        """
    def fit(self, train: Dataset, eval_set: typing.Optional[typing.Sequence[EvalSet]] = None) -> GBLinearModel:
        r"""
        Train the model on a dataset.
        
        Args:
            train: Training dataset containing features and labels.
            eval_set: Validation set(s) for early stopping and evaluation.
        
        Returns:
            Self (for method chaining).
        
        Raises:
            ValueError: If training data is invalid or labels are missing.
        """

@typing.final
class LinearLeavesConfig:
    r"""
    Configuration for linear models in leaf nodes.
    
    When enabled, each leaf fits a linear regression model on its samples
    instead of using a constant value.
    
    Attributes:
        enable: Whether to enable linear models in leaves.
        l2: L2 regularization for linear coefficients.
        l1: L1 regularization for linear coefficients.
        max_iter: Maximum coordinate descent iterations per leaf.
        tolerance: Convergence tolerance.
        min_samples: Minimum samples required to fit a linear model.
    
    Examples:
        >>> config = LinearLeavesConfig(enable=True, l2=0.01)
        >>> config.enable
        True
    """
    @property
    def enable(self) -> builtins.bool:
        r"""
        Whether to enable linear models in leaves.
        """
    @enable.setter
    def enable(self, value: builtins.bool) -> None:
        r"""
        Whether to enable linear models in leaves.
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 regularization for linear coefficients.
        """
    @l2.setter
    def l2(self, value: builtins.float) -> None:
        r"""
        L2 regularization for linear coefficients.
        """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 regularization for linear coefficients.
        """
    @l1.setter
    def l1(self, value: builtins.float) -> None:
        r"""
        L1 regularization for linear coefficients.
        """
    @property
    def max_iter(self) -> builtins.int:
        r"""
        Maximum coordinate descent iterations per leaf.
        """
    @max_iter.setter
    def max_iter(self, value: builtins.int) -> None:
        r"""
        Maximum coordinate descent iterations per leaf.
        """
    @property
    def tolerance(self) -> builtins.float:
        r"""
        Convergence tolerance.
        """
    @tolerance.setter
    def tolerance(self, value: builtins.float) -> None:
        r"""
        Convergence tolerance.
        """
    @property
    def min_samples(self) -> builtins.int:
        r"""
        Minimum samples required to fit a linear model.
        """
    @min_samples.setter
    def min_samples(self, value: builtins.int) -> None:
        r"""
        Minimum samples required to fit a linear model.
        """
    def __new__(cls, enable: builtins.bool = False, l2: builtins.float = 0.01, l1: builtins.float = 0.0, max_iter: builtins.int = 10, tolerance: builtins.float = 1e-06, min_samples: builtins.int = 50) -> LinearLeavesConfig:
        r"""
        Create a new LinearLeavesConfig.
        
        Args:
            enable: Whether to enable linear leaves. Default: False.
            l2: L2 regularization. Default: 0.01.
            l1: L1 regularization. Default: 0.0.
            max_iter: Maximum coordinate descent iterations. Default: 10.
            tolerance: Convergence tolerance. Default: 1e-6.
            min_samples: Minimum samples to fit linear model. Default: 50.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class RegularizationConfig:
    r"""
    Configuration for L1/L2 regularization.
    
    Attributes:
        l1: L1 (Lasso) regularization term on leaf weights.
        l2: L2 (Ridge) regularization term on leaf weights.
        min_hessian: Minimum sum of hessians required in a leaf.
    
    Examples:
        >>> config = RegularizationConfig(l1=0.1, l2=1.0)
        >>> config.l2
        1.0
    """
    @property
    def l1(self) -> builtins.float:
        r"""
        L1 (Lasso) regularization term on leaf weights.
        """
    @l1.setter
    def l1(self, value: builtins.float) -> None:
        r"""
        L1 (Lasso) regularization term on leaf weights.
        """
    @property
    def l2(self) -> builtins.float:
        r"""
        L2 (Ridge) regularization term on leaf weights.
        """
    @l2.setter
    def l2(self, value: builtins.float) -> None:
        r"""
        L2 (Ridge) regularization term on leaf weights.
        """
    @property
    def min_hessian(self) -> builtins.float:
        r"""
        Minimum sum of hessians required in a leaf.
        """
    @min_hessian.setter
    def min_hessian(self, value: builtins.float) -> None:
        r"""
        Minimum sum of hessians required in a leaf.
        """
    def __new__(cls, l1: builtins.float = 0.0, l2: builtins.float = 1.0, min_hessian: builtins.float = 1.0) -> RegularizationConfig:
        r"""
        Create a new RegularizationConfig.
        
        Args:
            l1: L1 regularization term. Default: 0.0.
            l2: L2 regularization term. Default: 1.0.
            min_hessian: Minimum sum of hessians in a leaf. Default: 1.0.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class SamplingConfig:
    r"""
    Configuration for row and column subsampling.
    
    Attributes:
        subsample: Row subsampling ratio (per tree). Value in (0, 1].
        colsample: Column subsampling ratio (per tree). Value in (0, 1].
        colsample_bylevel: Column subsampling ratio (per level). Value in (0, 1].
        goss_alpha: GOSS top percentage. 0 disables GOSS.
        goss_beta: GOSS random percentage for small gradients.
    
    Examples:
        >>> config = SamplingConfig(subsample=0.8, colsample=0.8)
        >>> config.subsample
        0.8
    """
    @property
    def subsample(self) -> builtins.float:
        r"""
        Row subsampling ratio (per tree). Value in (0, 1].
        """
    @subsample.setter
    def subsample(self, value: builtins.float) -> None:
        r"""
        Row subsampling ratio (per tree). Value in (0, 1].
        """
    @property
    def colsample(self) -> builtins.float:
        r"""
        Column subsampling ratio (per tree). Value in (0, 1].
        """
    @colsample.setter
    def colsample(self, value: builtins.float) -> None:
        r"""
        Column subsampling ratio (per tree). Value in (0, 1].
        """
    @property
    def colsample_bylevel(self) -> builtins.float:
        r"""
        Column subsampling ratio (per level). Value in (0, 1].
        """
    @colsample_bylevel.setter
    def colsample_bylevel(self, value: builtins.float) -> None:
        r"""
        Column subsampling ratio (per level). Value in (0, 1].
        """
    @property
    def goss_alpha(self) -> builtins.float:
        r"""
        GOSS top percentage (for gradient-based one-side sampling).
        If > 0, enables GOSS. Value in [0, 1).
        """
    @goss_alpha.setter
    def goss_alpha(self, value: builtins.float) -> None:
        r"""
        GOSS top percentage (for gradient-based one-side sampling).
        If > 0, enables GOSS. Value in [0, 1).
        """
    @property
    def goss_beta(self) -> builtins.float:
        r"""
        GOSS random percentage for small gradients.
        Value in [0, 1].
        """
    @goss_beta.setter
    def goss_beta(self, value: builtins.float) -> None:
        r"""
        GOSS random percentage for small gradients.
        Value in [0, 1].
        """
    def __new__(cls, subsample: builtins.float = 1.0, colsample: builtins.float = 1.0, colsample_bylevel: builtins.float = 1.0, goss_alpha: builtins.float = 0.0, goss_beta: builtins.float = 0.0) -> SamplingConfig:
        r"""
        Create a new SamplingConfig.
        
        Args:
            subsample: Row subsampling ratio. Must be in (0, 1]. Default: 1.0.
            colsample: Column subsampling ratio per tree. Must be in (0, 1]. Default: 1.0.
            colsample_bylevel: Column subsampling ratio per level. Default: 1.0.
            goss_alpha: GOSS top percentage. 0 disables GOSS. Default: 0.0.
            goss_beta: GOSS random percentage for small gradients. Default: 0.0.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class TreeConfig:
    r"""
    Configuration for tree structure.
    
    Controls tree depth, number of leaves, and split constraints.
    
    Attributes:
        max_depth (int): Maximum depth of tree. -1 means unlimited
            (controlled by n_leaves).
        n_leaves (int): Maximum number of leaves. Only used when max_depth=-1.
        min_samples_leaf (int): Minimum number of samples required in a leaf.
        min_gain_to_split (float): Minimum gain required to make a split.
        growth_strategy (GrowthStrategy): Growth strategy for tree building.
    
    Examples:
        >>> from boosters import TreeConfig, GrowthStrategy
        >>> config = TreeConfig(max_depth=6)
        >>> config.max_depth
        6
        >>> config = TreeConfig(max_depth=-1, n_leaves=31, growth_strategy=GrowthStrategy.Leafwise)
    """
    @property
    def max_depth(self) -> builtins.int:
        r"""
        Maximum depth of tree. -1 means unlimited (controlled by n_leaves).
        """
    @max_depth.setter
    def max_depth(self, value: builtins.int) -> None:
        r"""
        Maximum depth of tree. -1 means unlimited (controlled by n_leaves).
        """
    @property
    def n_leaves(self) -> builtins.int:
        r"""
        Maximum number of leaves. Only used when max_depth is -1.
        """
    @n_leaves.setter
    def n_leaves(self, value: builtins.int) -> None:
        r"""
        Maximum number of leaves. Only used when max_depth is -1.
        """
    @property
    def min_samples_leaf(self) -> builtins.int:
        r"""
        Minimum number of samples required in a leaf node.
        """
    @min_samples_leaf.setter
    def min_samples_leaf(self, value: builtins.int) -> None:
        r"""
        Minimum number of samples required in a leaf node.
        """
    @property
    def min_gain_to_split(self) -> builtins.float:
        r"""
        Minimum gain required to make a split.
        """
    @min_gain_to_split.setter
    def min_gain_to_split(self, value: builtins.float) -> None:
        r"""
        Minimum gain required to make a split.
        """
    @property
    def growth_strategy(self) -> GrowthStrategy:
        r"""
        Growth strategy for tree building.
        """
    @growth_strategy.setter
    def growth_strategy(self, value: GrowthStrategy) -> None:
        r"""
        Growth strategy for tree building.
        """
    def __new__(cls, max_depth: builtins.int = -1, n_leaves: builtins.int = 31, min_samples_leaf: builtins.int = 1, min_gain_to_split: builtins.float = 0.0, growth_strategy: GrowthStrategy = GrowthStrategy.Depthwise) -> TreeConfig:
        r"""
        Create a new TreeConfig.
        
        Args:
            max_depth: Maximum depth of tree. -1 means unlimited
                (controlled by n_leaves). Defaults to -1.
            n_leaves: Maximum number of leaves. Only used when max_depth=-1.
                Defaults to 31.
            min_samples_leaf: Minimum samples required in a leaf. Defaults to 1.
            min_gain_to_split: Minimum gain required to make a split.
                Defaults to 0.0.
            growth_strategy: Tree growth strategy. Defaults to Depthwise.
        
        Returns:
            A new TreeConfig instance.
        
        Examples:
            >>> from boosters import TreeConfig, GrowthStrategy
            >>> config = TreeConfig(max_depth=6)
            >>> config = TreeConfig(growth_strategy=GrowthStrategy.Leafwise)
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class GrowthStrategy(enum.Enum):
    r"""
    Tree growth strategy for building decision trees.
    
    Determines the order in which nodes are expanded during tree construction.
    Acts like a Python StrEnum - can be compared to strings.
    
    Attributes:
        Depthwise: Grow tree level-by-level (like XGBoost).
            All nodes at depth d are expanded before any node at depth d+1.
            More balanced trees, better for shallow trees.
        Leafwise: Grow tree by best-first split (like LightGBM).
            Always expands the leaf with highest gain.
            Can produce deeper, more accurate trees but risks overfitting.
    
    Examples:
        >>> from boosters import GrowthStrategy
        >>> strategy = GrowthStrategy.Depthwise
        >>> strategy == "depthwise"
        True
        >>> str(strategy)
        'depthwise'
    """
    Depthwise = ...
    r"""
    Grow tree level-by-level (like XGBoost).
    """
    Leafwise = ...
    r"""
    Grow tree by best-first split (like LightGBM).
    """

    def __str__(self) -> builtins.str:
        r"""
        String representation like StrEnum.
        """
    def __repr__(self) -> builtins.str: ...
    def __hash__(self) -> builtins.int:
        r"""
        Hash using the string value for StrEnum-like behavior.
        """
    def __reduce__(self) -> tuple[typing.Any, tuple[builtins.str]]:
        r"""
        Pickle support: reduce to module path and variant name.
        """
    def __deepcopy__(self, _memo: typing.Any) -> GrowthStrategy:
        r"""
        Deepcopy support - return self since enum variants are singletons.
        """
    def __copy__(self) -> GrowthStrategy:
        r"""
        Copy support - return self since enum variants are singletons.
        """

@typing.final
class Metric(enum.Enum):
    r"""
    Evaluation metrics for gradient boosting.
    
    Each variant represents a different metric for evaluating model performance.
    Use the static constructor methods for validation.
    
    Regression:
        - Metric.Rmse(): Root Mean Squared Error
        - Metric.Mae(): Mean Absolute Error
        - Metric.Mape(): Mean Absolute Percentage Error
    
    Classification:
        - Metric.LogLoss(): Binary cross-entropy
        - Metric.Auc(): Area Under ROC Curve
        - Metric.Accuracy(): Classification accuracy
    
    Ranking:
        - Metric.Ndcg(at): Normalized Discounted Cumulative Gain@k
    
    Examples
    --------
    >>> from boosters import Metric
    >>> metric = Metric.rmse()  # Regression
    >>> metric = Metric.auc()  # Binary classification
    >>> metric = Metric.ndcg(at=5)  # Ranking
    
    Pattern matching:
    >>> match metric:
    ...     case Metric.Rmse():
    ...         print("RMSE")
    ...     case Metric.Ndcg(at=k):
    ...         print(f"NDCG@{k}")
    """
    Rmse = ...
    r"""
    Root Mean Squared Error for regression.
    """
    Mae = ...
    r"""
    Mean Absolute Error for regression.
    """
    Mape = ...
    r"""
    Mean Absolute Percentage Error for regression.
    """
    LogLoss = ...
    r"""
    Binary Log Loss (cross-entropy) for classification.
    """
    Auc = ...
    r"""
    Area Under ROC Curve for binary classification.
    """
    Accuracy = ...
    r"""
    Classification accuracy (binary or multiclass).
    """
    Ndcg = ...
    r"""
    Normalized Discounted Cumulative Gain for ranking.
    
    Parameters:
        at: Truncation point for NDCG calculation (NDCG@k). Default: 10.
    """

    @staticmethod
    def rmse() -> Metric:
        r"""
        Create RMSE metric.
        """
    @staticmethod
    def mae() -> Metric:
        r"""
        Create MAE metric.
        """
    @staticmethod
    def mape() -> Metric:
        r"""
        Create MAPE metric.
        """
    @staticmethod
    def logloss() -> Metric:
        r"""
        Create log loss metric.
        """
    @staticmethod
    def auc() -> Metric:
        r"""
        Create AUC metric.
        """
    @staticmethod
    def accuracy() -> Metric:
        r"""
        Create accuracy metric.
        """
    @staticmethod
    def ndcg(at: builtins.int = 10) -> Metric:
        r"""
        Create NDCG@k metric with validation.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class Objective(enum.Enum):
    r"""
    Objective (loss) functions for gradient boosting.
    
    Each variant represents a different loss function for training GBDT and
    GBLinear models. Use the static constructor methods for validation.
    
    Regression:
        - Objective.Squared(): Mean squared error (L2)
        - Objective.Absolute(): Mean absolute error (L1)
        - Objective.Huber(delta): Pseudo-Huber loss (robust)
        - Objective.Pinball(alpha): Quantile regression
        - Objective.Poisson(): Poisson deviance for count data
    
    Classification:
        - Objective.Logistic(): Binary cross-entropy
        - Objective.Hinge(): SVM-style hinge loss
        - Objective.Softmax(n_classes): Multiclass cross-entropy
    
    Ranking:
        - Objective.LambdaRank(ndcg_at): LambdaMART for NDCG optimization
    
    Examples
    --------
    >>> from boosters import Objective
    >>> obj = Objective.squared()  # L2 regression
    >>> obj = Objective.logistic()  # Binary classification
    >>> obj = Objective.pinball([0.1, 0.5, 0.9])  # Quantile regression
    >>> obj = Objective.softmax(10)  # Multiclass classification
    
    Pattern matching:
    >>> match obj:
    ...     case Objective.Squared():
    ...         print("L2 loss")
    ...     case Objective.Pinball(alpha=a):
    ...         print(f"Quantile: {a}")
    """
    Squared = ...
    r"""
    Squared error loss (L2) for regression.
    """
    Absolute = ...
    r"""
    Absolute error loss (L1) for robust regression.
    """
    Poisson = ...
    r"""
    Poisson loss for count regression.
    """
    Logistic = ...
    r"""
    Logistic loss for binary classification.
    """
    Hinge = ...
    r"""
    Hinge loss for binary classification (SVM-style).
    """
    Huber = ...
    r"""
    Pseudo-Huber loss for robust regression.
    
    Parameters:
        delta: Transition point between quadratic and linear loss. Default: 1.0.
    """
    Pinball = ...
    r"""
    Pinball loss for quantile regression.
    
    Parameters:
        alpha: List of quantiles to predict. Each value must be in (0, 1).
    """
    Softmax = ...
    r"""
    Softmax loss for multiclass classification.
    
    Parameters:
        n_classes: Number of classes. Must be >= 2.
    """
    LambdaRank = ...
    r"""
    LambdaRank loss for learning to rank.
    
    Parameters:
        ndcg_at: Truncation point for NDCG calculation. Default: 10.
    """

    @staticmethod
    def squared() -> Objective:
        r"""
        Create squared error loss (L2).
        """
    @staticmethod
    def absolute() -> Objective:
        r"""
        Create absolute error loss (L1).
        """
    @staticmethod
    def poisson() -> Objective:
        r"""
        Create Poisson loss.
        """
    @staticmethod
    def logistic() -> Objective:
        r"""
        Create logistic loss for binary classification.
        """
    @staticmethod
    def hinge() -> Objective:
        r"""
        Create hinge loss for binary classification.
        """
    @staticmethod
    def huber(delta: builtins.float = 1.0) -> Objective:
        r"""
        Create Huber loss with validation.
        """
    @staticmethod
    def pinball(alpha: typing.Sequence[builtins.float]) -> Objective:
        r"""
        Create pinball loss with validation.
        """
    @staticmethod
    def softmax(n_classes: builtins.int) -> Objective:
        r"""
        Create softmax loss with validation.
        """
    @staticmethod
    def lambdarank(ndcg_at: builtins.int = 10) -> Objective:
        r"""
        Create LambdaRank loss with validation.
        """
    def __repr__(self) -> builtins.str: ...

