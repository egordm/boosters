# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import typing

class AbsoluteLoss:
    r"""
    Absolute error loss (L1) for robust regression.
    
    Examples
    --------
    >>> from boosters import AbsoluteLoss
    >>> obj = AbsoluteLoss()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class Accuracy:
    r"""
    Classification accuracy (binary or multiclass).
    
    Examples
    --------
    >>> from boosters import Accuracy
    >>> metric = Accuracy()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class ArctanLoss:
    r"""
    Arctan loss for bounded regression.
    
    Parameters
    ----------
    alpha : float, default=0.5
        Scale parameter. Must be in (0, 1).
    
    Examples
    --------
    >>> from boosters import ArctanLoss
    >>> obj = ArctanLoss(alpha=0.3)
    """
    alpha: float
    def __new__(cls,alpha = ...): ...
    def __repr__(self) -> str:
        ...


class Auc:
    r"""
    Area Under ROC Curve for binary classification.
    
    Examples
    --------
    >>> from boosters import Auc
    >>> metric = Auc()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class CategoricalConfig:
    r"""
    Configuration for categorical feature handling.
    
    Boosters supports native categorical splits (like LightGBM) using
    bitset-based multi-way splits rather than one-hot encoding.
    
    Examples
    --------
    >>> from boosters import CategoricalConfig
    >>> config = CategoricalConfig(max_categories=256)
    >>> config.max_categories
    256
    """
    max_categories: int
    min_category_count: int
    max_onehot: int
    def __new__(cls,max_categories = ...,min_category_count = ...,max_onehot = ...): ...
    def __repr__(self) -> str:
        ...


class Dataset:
    r"""
    Dataset holding features, labels, and optional metadata.
    
    This class wraps NumPy arrays or pandas DataFrames for use with boosters models.
    Data is converted to an internal representation on construction for efficient
    training and prediction.
    
    # Data Layout
    
    - C-contiguous (row-major) float32 arrays provide optimal performance
    - F-contiguous arrays are automatically converted to C-order
    - float64 arrays are supported but will be converted to float32
    
    # Categorical Features
    
    Categorical features can be:
    1. Auto-detected from pandas categorical dtype
    2. Specified explicitly via `categorical_features` parameter
    
    # Missing Values
    
    NaN values in features are treated as missing (like XGBoost).
    Inf values in features or NaN/Inf in labels raise errors.
    
    # Example
    
    ```python
    import numpy as np
    from boosters import Dataset
    
    X = np.random.rand(100, 10).astype(np.float32)
    y = np.random.rand(100).astype(np.float32)
    
    dataset = Dataset(X, y)
    print(f"Samples: {dataset.n_samples}, Features: {dataset.n_features}")
    ```
    """
    n_samples: int
    n_features: int
    has_labels: bool
    has_weights: bool
    feature_names: typing.Optional[list[str]]
    categorical_features: list[int]
    shape: tuple[int, int]
    def __new__(cls,features,labels = ...,weights = ...,groups = ...,feature_names = ...,categorical_features = ...): ...
    def __repr__(self) -> str:
        ...


class EFBConfig:
    r"""
    Configuration for Exclusive Feature Bundling.
    
    EFB bundles mutually exclusive features to reduce memory and computation,
    similar to LightGBM's implementation.
    
    Examples
    --------
    >>> from boosters import EFBConfig
    >>> config = EFBConfig(enable=True, max_conflict_rate=0.0)
    >>> config.enable
    True
    """
    enable: bool
    max_conflict_rate: float
    def __new__(cls,enable = ...,max_conflict_rate = ...): ...
    def __repr__(self) -> str:
        ...


class EvalSet:
    r"""
    Named evaluation set for model training.
    
    An EvalSet wraps a Dataset with a name, which is used to identify
    the evaluation set in training logs and `eval_results`.
    
    # Example
    
    ```python
    from boosters import Dataset, EvalSet
    import numpy as np
    
    X_val = np.random.rand(50, 10).astype(np.float32)
    y_val = np.random.rand(50).astype(np.float32)
    
    val_data = Dataset(X_val, y_val)
    eval_set = EvalSet("validation", val_data)
    
    # Use in training:
    # model.fit(train_data, valid=[eval_set])
    ```
    """
    name: str
    dataset: Dataset
    def __new__(cls,name:str, dataset:typing.Any): ...
    def __repr__(self) -> str:
        ...


class GBDTConfig:
    r"""
    Main configuration for GBDT model.
    
    This is the primary configuration class for gradient boosted decision trees.
    It accepts nested configuration objects for tree structure, regularization,
    sampling, etc.
    
    Parameters
    ----------
    n_estimators : int, default=100
        Number of boosting rounds (trees to train).
    learning_rate : float, default=0.3
        Step size shrinkage. Smaller values require more trees but often
        produce better models. Typical range: 0.01 - 0.3.
    objective : Objective, default=SquaredLoss()
        Loss function for training.
    metric : Metric or None, default=None
        Evaluation metric. If None, uses objective's default metric.
    tree : TreeConfig or None, default=None
        Tree structure parameters. If None, uses defaults.
    regularization : RegularizationConfig or None, default=None
        L1/L2 regularization parameters. If None, uses defaults.
    sampling : SamplingConfig or None, default=None
        Row and column subsampling parameters. If None, uses defaults.
    categorical : CategoricalConfig or None, default=None
        Categorical feature handling. If None, uses defaults.
    efb : EFBConfig or None, default=None
        Exclusive Feature Bundling config. If None, uses defaults.
    linear_leaves : LinearLeavesConfig or None, default=None
        Linear model in leaves config. If None, disabled.
    early_stopping_rounds : int or None, default=None
        Stop if no improvement for this many rounds. None disables.
    seed : int, default=42
        Random seed for reproducibility.
    
    Examples
    --------
    >>> from boosters import GBDTConfig, SquaredLoss, TreeConfig
    >>> config = GBDTConfig(
    ...     n_estimators=500,
    ...     learning_rate=0.1,
    ...     objective=SquaredLoss(),
    ...     tree=TreeConfig(max_depth=6),
    ... )
    """
    n_estimators: int
    learning_rate: float
    tree: TreeConfig
    regularization: RegularizationConfig
    sampling: SamplingConfig
    categorical: CategoricalConfig
    efb: EFBConfig
    linear_leaves: typing.Optional[LinearLeavesConfig]
    early_stopping_rounds: typing.Optional[int]
    seed: int
    objective: typing.Any
    metric: typing.Optional[typing.Any]
    def __new__(cls,n_estimators = ...,learning_rate = ...,objective = ...,metric = ...,tree = ...,regularization = ...,sampling = ...,categorical = ...,efb = ...,linear_leaves = ...,early_stopping_rounds = ...,seed = ...): ...
    def __repr__(self) -> str:
        ...


class GBDTModel:
    r"""
    Gradient Boosted Decision Tree model.
    
    This is the main model class for training and prediction with gradient
    boosted decision trees.
    
    # Example
    
    ```python
    from boosters import GBDTModel, GBDTConfig, Dataset
    import numpy as np
    
    # Create training data
    X = np.random.rand(1000, 10).astype(np.float32)
    y = np.random.rand(1000).astype(np.float32)
    train = Dataset(X, y)
    
    # Train with default config
    model = GBDTModel().fit(train)
    
    # Or with custom config
    config = GBDTConfig(n_estimators=50, learning_rate=0.1)
    model = GBDTModel(config=config).fit(train)
    
    # Predict
    predictions = model.predict(X_test)
    ```
    """
    is_fitted: bool
    n_trees: int
    n_features: int
    best_iteration: typing.Optional[int]
    best_score: typing.Optional[float]
    eval_results: typing.Optional[typing.Any]
    config: GBDTConfig
    def __new__(cls,config = ...): ...
    def feature_importance(self, importance_type = ...) -> typing.Any:
        r"""
        Get feature importance scores.
        
        Args:
            importance_type: Type of importance to compute.
                - "split" (default): Number of times a feature is used to split.
                - "gain": Total gain achieved by splits on this feature.
        
        Returns:
            NumPy array of importance scores, one per feature.
        
        Raises:
            ValueError: If model has not been fitted.
        """
        ...

    def shap_values(self, features) -> typing.Any:
        r"""
        Compute SHAP values for feature contribution analysis.
        
        SHAP (SHapley Additive exPlanations) values show how each feature
        contributes to individual predictions. The values sum to the difference
        between the model's prediction and the base value.
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
        
        Returns:
            NumPy array with shape `(n_samples, n_features + 1, n_outputs)`.
            The last feature index contains the base value (expected value).
            For single-output models, the last dimension is squeezed.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            # Get SHAP values for test data
            shap_values = model.shap_values(X_test)
        
            # For a single sample, contributions sum to prediction - base_value
            sample_idx = 0
            feature_contribs = shap_values[sample_idx, :-1, 0]  # All features
            base_value = shap_values[sample_idx, -1, 0]  # Base value
            prediction = model.predict(X_test[sample_idx:sample_idx+1])[0]
            # assert np.isclose(base_value + feature_contribs.sum(), prediction)
            ```
        """
        ...

    def __repr__(self) -> str:
        r"""
        String representation.
        """
        ...

    def predict(self, features,n_iterations = ...) -> typing.Any:
        r"""
        Make predictions on features.
        
        Returns transformed predictions (e.g., probabilities for classification).
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
            n_iterations: Number of trees to use for prediction. If None, uses all trees.
        
        Returns:
            NumPy array with predictions of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            predictions = model.predict(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
        ...

    def predict_raw(self, features,n_iterations = ...) -> typing.Any:
        r"""
        Make raw (untransformed) predictions on features.
        
        Returns raw margin scores without transformation.
        For classification this means logits instead of probabilities.
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
            n_iterations: Number of trees to use for prediction. If None, uses all trees.
        
        Returns:
            NumPy array with raw scores of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            raw_margins = model.predict_raw(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
        ...

    def fit(self, train,valid = ...) -> GBDTModel:
        r"""
        Train the model on a dataset.
        
        Args:
            train: Training dataset containing features and labels.
            valid: Optional validation set(s) for early stopping and evaluation.
                Can be a single EvalSet or a list of EvalSets.
        
        Returns:
            Self (for method chaining).
        
        Raises:
            ValueError: If training data is invalid or labels are missing.
        
        Example:
            ```python
            model = GBDTModel().fit(train_dataset)
            model = GBDTModel().fit(train, valid=[EvalSet("val", val_data)])
            ```
        """
        ...


class GBLinearConfig:
    r"""
    Main configuration for GBLinear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    Parameters
    ----------
    n_estimators : int, default=100
        Number of boosting rounds.
    learning_rate : float, default=0.5
        Step size for weight updates. Higher values mean faster convergence
        but risk overshooting.
    objective : Objective, default=SquaredLoss()
        Loss function for training.
    metric : Metric or None, default=None
        Evaluation metric. If None, uses objective's default metric.
    l1 : float, default=0.0
        L1 regularization (alpha). Encourages sparse weights.
    l2 : float, default=1.0
        L2 regularization (lambda). Prevents large weights.
    early_stopping_rounds : int or None, default=None
        Stop if no improvement for this many rounds. None disables.
    seed : int, default=42
        Random seed for reproducibility.
    
    Examples
    --------
    >>> from boosters import GBLinearConfig, SquaredLoss
    >>> config = GBLinearConfig(
    ...     n_estimators=200,
    ...     learning_rate=0.3,
    ...     objective=SquaredLoss(),
    ...     l2=0.1,
    ... )
    """
    n_estimators: int
    learning_rate: float
    l1: float
    l2: float
    early_stopping_rounds: typing.Optional[int]
    seed: int
    objective: typing.Any
    metric: typing.Optional[typing.Any]
    def __new__(cls,n_estimators = ...,learning_rate = ...,objective = ...,metric = ...,l1 = ...,l2 = ...,early_stopping_rounds = ...,seed = ...): ...
    def __repr__(self) -> str:
        ...


class GBLinearModel:
    r"""
    Gradient Boosted Linear model.
    
    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    
    # Example
    
    ```python
    from boosters import GBLinearModel, GBLinearConfig, Dataset
    import numpy as np
    
    # Create training data
    X = np.random.rand(1000, 10).astype(np.float32)
    y = np.random.rand(1000).astype(np.float32)
    train = Dataset(X, y)
    
    # Train with default config
    model = GBLinearModel().fit(train)
    
    # Or with custom config
    config = GBLinearConfig(n_estimators=50, learning_rate=0.3, l2=0.1)
    model = GBLinearModel(config=config).fit(train)
    
    # Predict
    predictions = model.predict(X_test)
    
    # Access weights
    print(model.coef_)      # shape: (n_features,) or (n_features, n_outputs)
    print(model.intercept_)  # shape: () or (n_outputs,)
    ```
    """
    is_fitted: bool
    n_features_in_: int
    coef_: typing.Any
    intercept_: typing.Any
    best_iteration: typing.Optional[int]
    best_score: typing.Optional[float]
    eval_results: typing.Optional[typing.Any]
    config: GBLinearConfig
    def __new__(cls,config = ...): ...
    def __repr__(self) -> str:
        r"""
        String representation.
        """
        ...

    def predict(self, features) -> typing.Any:
        r"""
        Make predictions on features.
        
        Returns transformed predictions (e.g., probabilities for classification).
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
        
        Returns:
            NumPy array with predictions of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            predictions = model.predict(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
        ...

    def predict_raw(self, features) -> typing.Any:
        r"""
        Make raw (untransformed) predictions on features.
        
        Returns raw margin scores without transformation.
        For classification this means logits instead of probabilities.
        
        Args:
            features: Feature array of shape `(n_samples, n_features)` or Dataset.
        
        Returns:
            NumPy array with raw scores of shape `(n_samples, n_outputs)`.
            For single-output models, n_outputs is 1.
        
        Raises:
            RuntimeError: If model has not been fitted.
            ValueError: If features have wrong shape.
        
        Example:
            ```python
            raw_margins = model.predict_raw(X_test)  # shape: (n_samples, n_outputs)
            ```
        """
        ...

    def fit(self, train,eval_set = ...) -> GBLinearModel:
        r"""
        Train the model on a dataset.
        
        Args:
            train: Training dataset containing features and labels.
            eval_set: Optional validation set(s) for early stopping and evaluation.
        
        Returns:
            Self (for method chaining).
        
        Raises:
            ValueError: If training data is invalid or labels are missing.
        """
        ...


class HingeLoss:
    r"""
    Hinge loss for binary classification (SVM-style).
    
    Examples
    --------
    >>> from boosters import HingeLoss
    >>> obj = HingeLoss()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class HuberLoss:
    r"""
    Pseudo-Huber loss for robust regression.
    
    Parameters
    ----------
    delta : float, default=1.0
        Transition point between quadratic and linear loss.
    
    Examples
    --------
    >>> from boosters import HuberLoss
    >>> obj = HuberLoss(delta=1.5)
    """
    delta: float
    def __new__(cls,delta = ...): ...
    def __repr__(self) -> str:
        ...


class LambdaRankLoss:
    r"""
    LambdaRank loss for learning to rank.
    
    Parameters
    ----------
    ndcg_at : int, default=10
        Truncation point for NDCG calculation.
    
    Examples
    --------
    >>> from boosters import LambdaRankLoss
    >>> obj = LambdaRankLoss(ndcg_at=5)
    """
    ndcg_at: int
    def __new__(cls,ndcg_at = ...): ...
    def __repr__(self) -> str:
        ...


class LinearLeavesConfig:
    r"""
    Configuration for linear models in leaf nodes.
    
    When enabled, each leaf fits a linear regression model on its samples
    instead of using a constant value.
    
    Examples
    --------
    >>> from boosters import LinearLeavesConfig
    >>> config = LinearLeavesConfig(enable=True, l2=0.01)
    >>> config.enable
    True
    """
    enable: bool
    l2: float
    l1: float
    max_iter: int
    tolerance: float
    min_samples: int
    def __new__(cls,enable = ...,l2 = ...,l1 = ...,max_iter = ...,tolerance = ...,min_samples = ...): ...
    def __repr__(self) -> str:
        ...


class LogLoss:
    r"""
    Binary Log Loss (cross-entropy) for classification.
    
    Examples
    --------
    >>> from boosters import LogLoss
    >>> metric = LogLoss()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class LogisticLoss:
    r"""
    Logistic loss for binary classification.
    
    Examples
    --------
    >>> from boosters import LogisticLoss
    >>> obj = LogisticLoss()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class Mae:
    r"""
    Mean Absolute Error for regression.
    
    Examples
    --------
    >>> from boosters import Mae
    >>> metric = Mae()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class Mape:
    r"""
    Mean Absolute Percentage Error for regression.
    
    Examples
    --------
    >>> from boosters import Mape
    >>> metric = Mape()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class Ndcg:
    r"""
    Normalized Discounted Cumulative Gain for ranking.
    
    Parameters
    ----------
    at : int, default=10
        Truncation point for NDCG calculation (NDCG@k).
    
    Examples
    --------
    >>> from boosters import Ndcg
    >>> metric = Ndcg(at=5)  # NDCG@5
    """
    at: int
    def __new__(cls,at = ...): ...
    def __repr__(self) -> str:
        ...


class PinballLoss:
    r"""
    Pinball loss for quantile regression.
    
    Parameters
    ----------
    alpha : float or list of float, default=0.5
        Quantile(s) to predict. Each value must be in (0, 1).
    
    Examples
    --------
    >>> from boosters import PinballLoss
    >>> obj = PinballLoss(alpha=0.5)  # median
    >>> obj = PinballLoss(alpha=[0.1, 0.5, 0.9])  # multiple quantiles
    """
    alpha: list[float]
    def __new__(cls,alpha = ...): ...
    def __repr__(self) -> str:
        ...


class PoissonLoss:
    r"""
    Poisson loss for count regression.
    
    Examples
    --------
    >>> from boosters import PoissonLoss
    >>> obj = PoissonLoss()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class RegularizationConfig:
    r"""
    Configuration for L1/L2 regularization.
    
    Examples
    --------
    >>> from boosters import RegularizationConfig
    >>> config = RegularizationConfig(l1=0.1, l2=1.0)
    >>> config.l2
    1.0
    """
    l1: float
    l2: float
    min_hessian: float
    def __new__(cls,l1 = ...,l2 = ...,min_hessian = ...): ...
    def __repr__(self) -> str:
        ...


class Rmse:
    r"""
    Root Mean Squared Error for regression.
    
    Examples
    --------
    >>> from boosters import Rmse
    >>> metric = Rmse()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class SamplingConfig:
    r"""
    Configuration for row and column subsampling.
    
    Examples
    --------
    >>> from boosters import SamplingConfig
    >>> config = SamplingConfig(subsample=0.8, colsample=0.8)
    >>> config.subsample
    0.8
    """
    subsample: float
    colsample: float
    colsample_bylevel: float
    goss_alpha: float
    goss_beta: float
    def __new__(cls,subsample = ...,colsample = ...,colsample_bylevel = ...,goss_alpha = ...,goss_beta = ...): ...
    def __repr__(self) -> str:
        ...


class SoftmaxLoss:
    r"""
    Softmax loss for multiclass classification.
    
    Parameters
    ----------
    n_classes : int
        Number of classes. Must be >= 2.
    
    Examples
    --------
    >>> from boosters import SoftmaxLoss
    >>> obj = SoftmaxLoss(n_classes=10)
    """
    n_classes: int
    def __new__(cls,n_classes:int): ...
    def __repr__(self) -> str:
        ...


class SquaredLoss:
    r"""
    Squared error loss (L2) for regression.
    
    Examples
    --------
    >>> from boosters import SquaredLoss
    >>> obj = SquaredLoss()
    """
    def __new__(cls,): ...
    def __repr__(self) -> str:
        ...


class TreeConfig:
    r"""
    Configuration for tree structure.
    
    Controls tree depth, number of leaves, and split constraints.
    
    Examples
    --------
    >>> from boosters import TreeConfig
    >>> config = TreeConfig(max_depth=6, n_leaves=31)
    >>> config.max_depth
    6
    """
    max_depth: int
    n_leaves: int
    min_samples_leaf: int
    min_gain_to_split: float
    growth_strategy: str
    def __new__(cls,max_depth = ...,n_leaves = ...,min_samples_leaf = ...,min_gain_to_split = ...,growth_strategy = ...): ...
    def __repr__(self) -> str:
        ...


