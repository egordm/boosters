"""Type stubs for boosters._boosters_rs Rust extension module.

This file provides type hints for the native Rust extension.
It is auto-generated by pyo3-stub-gen but may be manually augmented.
"""

# Module version
__version__: str

# =============================================================================
# Config Types (Base)
# =============================================================================

class TreeConfig:
    """Configuration for tree structure."""

    max_depth: int
    n_leaves: int
    min_samples_leaf: int
    min_gain_to_split: float
    growth_strategy: str

    def __init__(
        self,
        max_depth: int = -1,
        n_leaves: int = 31,
        min_samples_leaf: int = 1,
        min_gain_to_split: float = 0.0,
        growth_strategy: str = "depthwise",
    ) -> None: ...

class RegularizationConfig:
    """Configuration for L1/L2 regularization."""

    l1: float
    l2: float
    min_hessian: float

    def __init__(
        self,
        l1: float = 0.0,
        l2: float = 1.0,
        min_hessian: float = 1.0,
    ) -> None: ...

class SamplingConfig:
    """Configuration for row and column subsampling."""

    subsample: float
    colsample: float
    colsample_bylevel: float
    goss_alpha: float
    goss_beta: float

    def __init__(
        self,
        subsample: float = 1.0,
        colsample: float = 1.0,
        colsample_bylevel: float = 1.0,
        goss_alpha: float = 0.0,
        goss_beta: float = 0.0,
    ) -> None: ...

class CategoricalConfig:
    """Configuration for categorical feature handling."""

    max_categories: int
    min_category_count: int
    max_onehot: int

    def __init__(
        self,
        max_categories: int = 256,
        min_category_count: int = 10,
        max_onehot: int = 4,
    ) -> None: ...

class EFBConfig:
    """Configuration for Exclusive Feature Bundling."""

    enable: bool
    max_conflict_rate: float

    def __init__(
        self,
        enable: bool = True,
        max_conflict_rate: float = 0.0,
    ) -> None: ...

class LinearLeavesConfig:
    """Configuration for linear models in leaf nodes."""

    enable: bool
    l2: float
    l1: float
    max_iter: int
    tolerance: float
    min_samples: int

    def __init__(
        self,
        enable: bool = False,
        l2: float = 0.01,
        l1: float = 0.0,
        max_iter: int = 10,
        tolerance: float = 1e-6,
        min_samples: int = 50,
    ) -> None: ...

# =============================================================================
# Objective Types
# =============================================================================

class SquaredLoss:
    """Squared error loss for regression (L2 loss)."""

    def __init__(self) -> None: ...

class AbsoluteLoss:
    """Absolute error loss for regression (L1 loss)."""

    def __init__(self) -> None: ...

class PoissonLoss:
    """Poisson regression loss for count data."""

    def __init__(self) -> None: ...

class LogisticLoss:
    """Logistic loss for binary classification."""

    def __init__(self) -> None: ...

class HingeLoss:
    """Hinge loss for binary classification (SVM-style)."""

    def __init__(self) -> None: ...

class HuberLoss:
    """Huber loss for robust regression.

    Combines L2 loss for small errors and L1 for large errors.
    """

    delta: float

    def __init__(self, delta: float = 1.0) -> None: ...

class PinballLoss:
    """Pinball loss for quantile regression.

    The alpha parameter can be a single float or a list of floats.
    Internally, alpha is always stored as a list for consistency.
    """

    alpha: list[float]

    def __init__(self, alpha: float | list[float] = 0.5) -> None: ...

class ArctanLoss:
    """Arctan loss for robust regression.

    A smooth approximation to absolute loss.
    """

    alpha: float

    def __init__(self, alpha: float = 0.5) -> None: ...

class SoftmaxLoss:
    """Softmax cross-entropy loss for multiclass classification.

    Note: n_classes is required and has no default value.
    """

    n_classes: int

    def __init__(self, n_classes: int) -> None: ...

class LambdaRankLoss:
    """LambdaRank loss for learning-to-rank tasks."""

    ndcg_at: int

    def __init__(self, ndcg_at: int = 10) -> None: ...

# =============================================================================
# Metric Types
# =============================================================================

class Rmse:
    """Root Mean Squared Error for regression."""

    def __init__(self) -> None: ...

class Mae:
    """Mean Absolute Error for regression."""

    def __init__(self) -> None: ...

class Mape:
    """Mean Absolute Percentage Error for regression."""

    def __init__(self) -> None: ...

class LogLoss:
    """Binary Log Loss (cross-entropy) for classification."""

    def __init__(self) -> None: ...

class Auc:
    """Area Under ROC Curve for binary classification."""

    def __init__(self) -> None: ...

class Accuracy:
    """Classification accuracy (binary or multiclass)."""

    def __init__(self) -> None: ...

class Ndcg:
    """Normalized Discounted Cumulative Gain for ranking."""

    at: int

    def __init__(self, at: int = 10) -> None: ...

# =============================================================================
# Type Aliases (after class definitions)
# =============================================================================

type Objective = (
    SquaredLoss
    | AbsoluteLoss
    | PoissonLoss
    | LogisticLoss
    | HingeLoss
    | HuberLoss
    | PinballLoss
    | ArctanLoss
    | SoftmaxLoss
    | LambdaRankLoss
)

type Metric = Rmse | Mae | Mape | LogLoss | Auc | Accuracy | Ndcg

# =============================================================================
# Main Config Types (depend on type aliases)
# =============================================================================

class GBDTConfig:
    """Main configuration for GBDT model.

    This is the primary configuration class for gradient boosted decision trees.
    It accepts nested configuration objects for tree structure, regularization,
    sampling, etc.
    """

    n_estimators: int
    learning_rate: float
    objective: Objective
    metric: Metric | None
    tree: TreeConfig
    regularization: RegularizationConfig
    sampling: SamplingConfig
    categorical: CategoricalConfig
    efb: EFBConfig
    linear_leaves: LinearLeavesConfig | None
    early_stopping_rounds: int | None
    seed: int

    def __init__(
        self,
        n_estimators: int = 100,
        learning_rate: float = 0.3,
        objective: Objective | None = None,
        metric: Metric | None = None,
        tree: TreeConfig | None = None,
        regularization: RegularizationConfig | None = None,
        sampling: SamplingConfig | None = None,
        categorical: CategoricalConfig | None = None,
        efb: EFBConfig | None = None,
        linear_leaves: LinearLeavesConfig | None = None,
        early_stopping_rounds: int | None = None,
        seed: int = 42,
    ) -> None: ...

class GBLinearConfig:
    """Main configuration for GBLinear model.

    GBLinear uses gradient boosting to train a linear model via coordinate
    descent. Simpler than GBDT but can be effective for linear relationships.
    """

    n_estimators: int
    learning_rate: float
    objective: Objective
    metric: Metric | None
    l1: float
    l2: float
    early_stopping_rounds: int | None
    seed: int

    def __init__(
        self,
        n_estimators: int = 100,
        learning_rate: float = 0.5,
        objective: Objective | None = None,
        metric: Metric | None = None,
        l1: float = 0.0,
        l2: float = 1.0,
        early_stopping_rounds: int | None = None,
        seed: int = 42,
    ) -> None: ...
