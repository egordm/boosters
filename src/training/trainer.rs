//! Gradient boosting trainer for tree ensembles.
//!
//! This module implements the full boosting loop for training GBTree models.
//! It coordinates tree building, gradient computation, and prediction updates.
//!
//! # Example
//!
//! ```ignore
//! use booste_rs::training::{GBTreeTrainer, TrainerParams, DepthWisePolicy};
//! use booste_rs::training::{SquaredLoss, Rmse};
//!
//! let loss = Box::new(SquaredLoss);
//! let params = TrainerParams::default();
//!
//! let mut trainer = GBTreeTrainer::new(loss, params);
//! let forest = trainer.train(&quantized, &labels, &cuts);
//! ```
//!
//! See RFC-0015 for design rationale.

use crate::forest::SoAForest;
use crate::trees::{ScalarLeaf, SoATreeStorage, TreeBuilder as SoATreeBuilder};

use super::buffer::GradientBuffer;
use super::callback::EarlyStopping;
use super::logger::{TrainingLogger, Verbosity};
use super::loss::Loss;
use super::metric::Metric;
use super::partition::RowPartitioner;
use super::quantize::{BinCuts, BinIndex, QuantizedMatrix};
use super::tree::{BuildingTree, GrowthPolicy, TreeGrower, TreeParams};

// ============================================================================
// TrainerParams
// ============================================================================

/// Parameters for the gradient boosting trainer.
#[derive(Debug, Clone)]
pub struct TrainerParams {
    /// Parameters for individual tree building
    pub tree_params: TreeParams,
    /// Number of boosting rounds (trees to build)
    pub num_rounds: u32,
    /// Base score initialization strategy
    pub base_score: BaseScore,
    /// Verbosity level for logging
    pub verbosity: Verbosity,
}

impl Default for TrainerParams {
    fn default() -> Self {
        Self {
            tree_params: TreeParams::default(),
            num_rounds: 100,
            base_score: BaseScore::Mean,
            verbosity: Verbosity::Info,
        }
    }
}

/// Strategy for initializing base score.
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum BaseScore {
    /// Use mean of labels (good for regression)
    Mean,
    /// Use a fixed value
    Fixed(f32),
    /// Use zero (raw model starts from 0)
    Zero,
}

// ============================================================================
// GBTreeTrainer
// ============================================================================

/// Gradient boosting trainer for tree ensembles.
///
/// Coordinates the full boosting loop:
/// 1. Initialize predictions with base score
/// 2. For each round:
///    - Compute gradients from loss
///    - Build a tree using TreeGrower
///    - Update predictions with tree output
///    - Optionally check early stopping
/// 3. Convert trees to inference format
pub struct GBTreeTrainer {
    /// Loss function for gradient computation
    loss: Box<dyn Loss>,
    /// Training parameters
    params: TrainerParams,
    /// Training logger
    logger: TrainingLogger,
    /// Early stopping callback (optional)
    early_stopping: Option<EarlyStopping>,
}

impl GBTreeTrainer {
    /// Create a new trainer.
    pub fn new(loss: Box<dyn Loss>, params: TrainerParams) -> Self {
        let logger = TrainingLogger::new(params.verbosity);
        Self {
            loss,
            params,
            logger,
            early_stopping: None,
        }
    }

    /// Enable early stopping with the given metric and patience.
    pub fn with_early_stopping(mut self, metric: Box<dyn Metric>, patience: usize) -> Self {
        self.early_stopping = Some(EarlyStopping::new(metric, patience));
        self
    }

    /// Train a gradient boosted forest.
    ///
    /// # Type Parameters
    ///
    /// * `G` - Growth policy (DepthWisePolicy or LeafWisePolicy)
    /// * `B` - Bin index type (u8 or u16)
    ///
    /// # Arguments
    ///
    /// * `policy` - Growth policy for tree building
    /// * `quantized` - Quantized feature matrix
    /// * `labels` - Target labels
    /// * `cuts` - Bin cuts for histogram building
    /// * `eval_set` - Optional validation set for early stopping (quantized data, labels)
    pub fn train<G, B>(
        &mut self,
        policy: G,
        quantized: &QuantizedMatrix<B>,
        labels: &[f32],
        cuts: &BinCuts,
        eval_set: Option<(&QuantizedMatrix<B>, &[f32])>,
    ) -> SoAForest<ScalarLeaf>
    where
        G: GrowthPolicy + Clone,
        B: BinIndex,
    {
        let num_rows = quantized.num_rows() as usize;
        assert_eq!(labels.len(), num_rows, "labels length must match data rows");

        // Initialize base score
        let base_score = self.compute_base_score(labels);
        self.logger
            .info(&format!("Base score: {:.6}", base_score));

        // Initialize predictions
        let mut predictions = vec![base_score; num_rows];

        // Gradient buffer (single output for regression/binary)
        let mut grads = GradientBuffer::new(num_rows, 1);

        // Row partitioner (reused per tree)
        let mut partitioner = RowPartitioner::new(num_rows as u32);

        // Trees built during training
        let mut trees: Vec<BuildingTree> = Vec::with_capacity(self.params.num_rounds as usize);

        self.logger.info(&format!(
            "Starting training: {} rounds, {} samples",
            self.params.num_rounds, num_rows
        ));

        for round in 0..self.params.num_rounds {
            // Compute gradients
            self.loss
                .compute_gradients(&predictions, labels, &mut grads);

            // Reset partitioner for new tree
            partitioner.reset();

            // Build tree
            let mut grower = TreeGrower::new(policy.clone(), cuts, self.params.tree_params.clone());
            let tree = grower.build_tree(quantized, &grads, &mut partitioner);

            // Update predictions
            Self::update_predictions(&tree, quantized, &mut predictions);

            // Log progress
            if self.params.verbosity >= Verbosity::Info {
                let train_metric = self.compute_train_metric(&predictions, labels);
                let metrics = [("train".to_string(), train_metric as f64)];
                self.logger.log_round(round as usize, &metrics);
            }

            // Early stopping check
            if let Some((eval_data, eval_labels)) = eval_set {
                if let Some(early_stop) = &mut self.early_stopping {
                    let eval_preds =
                        Self::predict_with_trees(&trees, &tree, eval_data, base_score);
                    if early_stop.should_stop(&eval_preds, eval_labels) {
                        self.logger.info(&format!(
                            "Early stopping at round {} (best: {})",
                            round,
                            early_stop.best_round()
                        ));
                        trees.push(tree);
                        break;
                    }
                }
            }

            trees.push(tree);
        }

        self.logger.info(&format!(
            "Training complete: {} trees built",
            trees.len()
        ));

        // Convert to inference format
        self.freeze_forest(trees, base_score)
    }

    /// Compute base score from labels.
    fn compute_base_score(&self, labels: &[f32]) -> f32 {
        match self.params.base_score {
            BaseScore::Mean => {
                let sum: f32 = labels.iter().sum();
                sum / labels.len() as f32
            }
            BaseScore::Fixed(v) => v,
            BaseScore::Zero => 0.0,
        }
    }

    /// Update predictions by adding tree outputs.
    fn update_predictions<B: BinIndex>(
        tree: &BuildingTree,
        quantized: &QuantizedMatrix<B>,
        predictions: &mut [f32],
    ) {
        for row in 0..quantized.num_rows() as usize {
            let leaf_value = Self::predict_row(tree, quantized, row as u32);
            predictions[row] += leaf_value;
        }
    }

    /// Traverse a BuildingTree for a single row.
    ///
    /// This is used during training to update predictions.
    fn predict_row<B: BinIndex>(tree: &BuildingTree, quantized: &QuantizedMatrix<B>, row: u32) -> f32 {
        let mut node_id = 0u32;

        loop {
            let node = tree.node(node_id);
            if node.is_leaf {
                return node.weight;
            }

            let split = node.split.as_ref().expect("Non-leaf must have split");
            let bin = quantized.get(row, split.feature).to_usize();

            let goes_left = if bin == 0 {
                // Missing value (bin 0) - use default direction
                split.default_left
            } else if split.is_categorical {
                // Categorical: check if bin is in left set
                split.categories_left.contains(&(bin as u32))
            } else {
                // Numerical: bin <= split_bin goes left
                bin <= split.split_bin as usize
            };

            node_id = if goes_left { node.left } else { node.right };
        }
    }

    /// Predict with all trees so far plus the new tree.
    fn predict_with_trees<B: BinIndex>(
        trees: &[BuildingTree],
        new_tree: &BuildingTree,
        data: &QuantizedMatrix<B>,
        base_score: f32,
    ) -> Vec<f32> {
        let num_rows = data.num_rows() as usize;
        let mut predictions = vec![base_score; num_rows];

        // Add predictions from existing trees
        for tree in trees {
            for row in 0..num_rows {
                predictions[row] += Self::predict_row(tree, data, row as u32);
            }
        }

        // Add predictions from new tree
        for row in 0..num_rows {
            predictions[row] += Self::predict_row(new_tree, data, row as u32);
        }

        predictions
    }

    /// Compute training metric (simple squared error for now).
    fn compute_train_metric(&self, predictions: &[f32], labels: &[f32]) -> f64 {
        let sum_sq_err: f64 = predictions
            .iter()
            .zip(labels.iter())
            .map(|(p, l)| (p - l).powi(2) as f64)
            .sum();
        (sum_sq_err / predictions.len() as f64).sqrt()
    }

    /// Convert BuildingTrees to inference-optimized SoAForest.
    fn freeze_forest(&self, trees: Vec<BuildingTree>, base_score: f32) -> SoAForest<ScalarLeaf> {
        let mut forest = SoAForest::for_regression().with_base_score(vec![base_score]);

        for building_tree in trees {
            let soa_tree = self.convert_tree(&building_tree);
            forest.push_tree(soa_tree, 0); // All trees in group 0 for regression
        }

        forest
    }

    /// Convert a BuildingTree to SoATreeStorage.
    fn convert_tree(&self, building: &BuildingTree) -> SoATreeStorage<ScalarLeaf> {
        let mut builder = SoATreeBuilder::<ScalarLeaf>::new();

        // Process nodes in order (BFS or just iterate since nodes are already numbered)
        for node_id in 0..building.num_nodes() as u32 {
            let node = building.node(node_id);

            if node.is_leaf {
                builder.add_leaf(ScalarLeaf(node.weight));
            } else {
                let split = node.split.as_ref().expect("Non-leaf must have split");
                if split.is_categorical {
                    // Convert categories_left to bitset format
                    let bitset = categories_to_bitset(&split.categories_left);
                    builder.add_categorical_split(
                        split.feature,
                        bitset,
                        split.default_left,
                        node.left,
                        node.right,
                    );
                } else {
                    builder.add_split(
                        split.feature,
                        split.threshold,
                        split.default_left,
                        node.left,
                        node.right,
                    );
                }
            }
        }

        builder.build()
    }
}

/// Convert category list to bitset format (u32 words).
fn categories_to_bitset(categories: &[u32]) -> Vec<u32> {
    if categories.is_empty() {
        return vec![];
    }

    let max_cat = *categories.iter().max().unwrap_or(&0);
    let num_words = (max_cat / 32 + 1) as usize;
    let mut bitset = vec![0u32; num_words];

    for &cat in categories {
        let word = (cat / 32) as usize;
        let bit = cat % 32;
        bitset[word] |= 1u32 << bit;
    }

    bitset
}

// ============================================================================
// Tests
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;
    use crate::data::DenseMatrix;
    use crate::training::quantize::{CutFinder, ExactQuantileCuts, Quantizer};
    use crate::training::{DepthWisePolicy, LeafWisePolicy, SquaredLoss};

    fn make_regression_data() -> (QuantizedMatrix<u8>, BinCuts, Vec<f32>) {
        // Simple linear relationship: y = x0 + noise
        let mut data = Vec::new();
        let mut labels = Vec::new();
        for i in 0..100 {
            let x0 = i as f32 / 10.0;
            let x1 = (i % 10) as f32;
            data.push(x0);
            data.push(x1);
            labels.push(x0 + 0.1); // y â‰ˆ x0
        }

        let matrix = DenseMatrix::from_vec(data, 100, 2);
        let cuts_finder = ExactQuantileCuts::new(1);
        let cuts = cuts_finder.find_cuts(&matrix, 256);
        let quantizer = Quantizer::new(cuts.clone());
        let quantized = quantizer.quantize::<_, u8>(&matrix);

        (quantized, cuts, labels)
    }

    #[test]
    fn test_trainer_params_default() {
        let params = TrainerParams::default();
        assert_eq!(params.num_rounds, 100);
        assert_eq!(params.base_score, BaseScore::Mean);
    }

    #[test]
    fn test_base_score_mean() {
        let labels = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let params = TrainerParams {
            base_score: BaseScore::Mean,
            ..Default::default()
        };
        let trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let base = trainer.compute_base_score(&labels);
        assert!((base - 3.0).abs() < 1e-6);
    }

    #[test]
    fn test_base_score_fixed() {
        let labels = vec![1.0, 2.0, 3.0];
        let params = TrainerParams {
            base_score: BaseScore::Fixed(0.5),
            ..Default::default()
        };
        let trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let base = trainer.compute_base_score(&labels);
        assert_eq!(base, 0.5);
    }

    #[test]
    fn test_train_depth_wise() {
        let (quantized, cuts, labels) = make_regression_data();

        let params = TrainerParams {
            num_rounds: 10,
            tree_params: TreeParams {
                max_depth: 3,
                learning_rate: 0.3,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let policy = DepthWisePolicy { max_depth: 3 };
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let forest = trainer.train(policy, &quantized, &labels, &cuts, None);

        assert_eq!(forest.num_trees(), 10);
        assert_eq!(forest.num_groups(), 1);
    }

    #[test]
    fn test_train_leaf_wise() {
        let (quantized, cuts, labels) = make_regression_data();

        let params = TrainerParams {
            num_rounds: 10,
            tree_params: TreeParams {
                max_leaves: 8,
                learning_rate: 0.3,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let policy = LeafWisePolicy { max_leaves: 8 };
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let forest = trainer.train(policy, &quantized, &labels, &cuts, None);

        assert_eq!(forest.num_trees(), 10);
    }

    #[test]
    fn test_predict_row() {
        // Build a simple tree manually and verify prediction
        let (quantized, cuts, labels) = make_regression_data();

        let params = TrainerParams {
            num_rounds: 1,
            tree_params: TreeParams {
                max_depth: 2,
                learning_rate: 1.0,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let policy = DepthWisePolicy { max_depth: 2 };
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let forest = trainer.train(policy, &quantized, &labels, &cuts, None);

        // Forest should be valid and produce predictions
        assert_eq!(forest.num_trees(), 1);
    }

    #[test]
    fn test_predictions_improve() {
        let (quantized, cuts, labels) = make_regression_data();

        let params = TrainerParams {
            num_rounds: 20,
            tree_params: TreeParams {
                max_depth: 4,
                learning_rate: 0.3,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let policy = DepthWisePolicy { max_depth: 4 };
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);

        // Train for a few rounds
        let forest = trainer.train(policy, &quantized, &labels, &cuts, None);

        // Predict on training data (should fit reasonably well)
        // Since we're using the inference forest, we need to use raw features
        // For now just verify we got trees
        assert!(forest.num_trees() > 0);
    }

    #[test]
    fn test_categories_to_bitset() {
        let cats = vec![0, 1, 5, 32, 33];
        let bitset = categories_to_bitset(&cats);

        assert_eq!(bitset.len(), 2); // Need 2 words for cat 33

        // Check bits
        assert!(bitset[0] & (1 << 0) != 0); // cat 0
        assert!(bitset[0] & (1 << 1) != 0); // cat 1
        assert!(bitset[0] & (1 << 5) != 0); // cat 5
        assert!(bitset[1] & (1 << 0) != 0); // cat 32
        assert!(bitset[1] & (1 << 1) != 0); // cat 33
    }

    #[test]
    fn test_empty_categories_bitset() {
        let cats: Vec<u32> = vec![];
        let bitset = categories_to_bitset(&cats);
        assert!(bitset.is_empty());
    }

    // ========================================================================
    // Story 7 Test Cases
    // ========================================================================

    #[test]
    fn test_gradients_computed_correctly() {
        // 7.T2: Verify gradients computed using Loss trait
        let (quantized, cuts, labels) = make_regression_data();

        let params = TrainerParams {
            num_rounds: 1,
            tree_params: TreeParams {
                max_depth: 2,
                learning_rate: 1.0,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            base_score: BaseScore::Zero, // Start from 0 for predictable gradients
            ..Default::default()
        };

        let policy = DepthWisePolicy { max_depth: 2 };
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let _forest = trainer.train(policy, &quantized, &labels, &cuts, None);

        // For squared loss with predictions=0, gradients should be -labels
        // The tree should be built to reduce these gradients
        // We can verify the forest was built (training worked)
    }

    #[test]
    fn test_predictions_updated_after_tree() {
        // 7.T3: Predictions updated after each tree
        let (quantized, cuts, labels) = make_regression_data();

        // Train multiple rounds and check that more trees = better fit
        let params_1 = TrainerParams {
            num_rounds: 1,
            tree_params: TreeParams {
                max_depth: 3,
                learning_rate: 0.5,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let params_10 = TrainerParams {
            num_rounds: 10,
            tree_params: TreeParams {
                max_depth: 3,
                learning_rate: 0.5,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let policy = DepthWisePolicy { max_depth: 3 };

        let mut trainer_1 = GBTreeTrainer::new(Box::new(SquaredLoss), params_1);
        let forest_1 = trainer_1.train(policy.clone(), &quantized, &labels, &cuts, None);

        let mut trainer_10 = GBTreeTrainer::new(Box::new(SquaredLoss), params_10);
        let forest_10 = trainer_10.train(policy, &quantized, &labels, &cuts, None);

        assert_eq!(forest_1.num_trees(), 1);
        assert_eq!(forest_10.num_trees(), 10);
    }

    #[test]
    fn test_early_stopping_triggers() {
        // 7.T4: Early stopping triggers after patience exceeded
        use crate::training::Rmse;

        let (quantized, cuts, labels) = make_regression_data();

        // Create separate eval set with different labels to simulate overfitting
        // This ensures the eval loss will stop improving while train loss continues
        let mut eval_labels = labels.clone();
        // Add noise to eval labels - model will overfit to training data
        for (i, l) in eval_labels.iter_mut().enumerate() {
            *l += ((i % 7) as f32 - 3.0) * 2.0; // Systematic noise
        }

        let params = TrainerParams {
            num_rounds: 100, // Many rounds - early stopping should kick in
            tree_params: TreeParams {
                max_depth: 6, // Deep trees to overfit
                learning_rate: 0.8, // High learning rate to overfit quickly
                min_samples_split: 1,
                min_samples_leaf: 1,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let policy = DepthWisePolicy { max_depth: 6 };
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params)
            .with_early_stopping(Box::new(Rmse), 3); // Patience of 3 rounds

        let forest = trainer.train(
            policy,
            &quantized,
            &labels,
            &cuts,
            Some((&quantized, &eval_labels)),
        );

        // With high learning rate, deep trees, and noisy eval data,
        // should stop when overfitting starts (eval loss increases)
        assert!(
            forest.num_trees() < 100,
            "Expected early stopping, got {} trees",
            forest.num_trees()
        );
    }

    #[test]
    fn test_forest_prediction_quality() {
        // Integration-level test: verify trained forest can predict
        let (quantized, cuts, labels) = make_regression_data();

        let params = TrainerParams {
            num_rounds: 50,
            tree_params: TreeParams {
                max_depth: 4,
                learning_rate: 0.2,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let policy = DepthWisePolicy { max_depth: 4 };
        let mut trainer = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let forest = trainer.train(policy, &quantized, &labels, &cuts, None);

        // The forest should produce reasonable predictions
        assert_eq!(forest.num_trees(), 50);
        assert_eq!(forest.num_groups(), 1);
        // Base score should be mean of labels (around 5.0 for our data)
        let base_scores = forest.base_score();
        assert!(base_scores.len() == 1);
        let bs = base_scores[0];
        // Mean of y = x0 + 0.1 where x0 in [0, 10) with step 0.1
        // Mean should be around (0 + 9.9) / 2 + 0.1 = 5.05
        assert!(
            (bs - 5.0).abs() < 1.0,
            "Base score {} far from expected ~5.0",
            bs
        );
    }

    #[test]
    fn test_leaf_wise_vs_depth_wise() {
        // Compare leaf-wise and depth-wise on same data
        let (quantized, cuts, labels) = make_regression_data();

        let params = TrainerParams {
            num_rounds: 20,
            tree_params: TreeParams {
                max_depth: 4,
                max_leaves: 16,
                learning_rate: 0.3,
                ..Default::default()
            },
            verbosity: Verbosity::Silent,
            ..Default::default()
        };

        let depth_policy = DepthWisePolicy { max_depth: 4 };
        let leaf_policy = LeafWisePolicy { max_leaves: 16 };

        let mut trainer_d = GBTreeTrainer::new(Box::new(SquaredLoss), params.clone());
        let forest_d = trainer_d.train(depth_policy, &quantized, &labels, &cuts, None);

        let mut trainer_l = GBTreeTrainer::new(Box::new(SquaredLoss), params);
        let forest_l = trainer_l.train(leaf_policy, &quantized, &labels, &cuts, None);

        // Both should produce 20 trees
        assert_eq!(forest_d.num_trees(), 20);
        assert_eq!(forest_l.num_trees(), 20);
    }
}
