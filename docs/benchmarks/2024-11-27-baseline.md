# Baseline Benchmarks - 2024-11-27

First benchmark run establishing baseline performance characteristics.

## Environment

- **CPU**: AMD Ryzen 9 9950X (16-core, Zen 5)
- **RAM**: 96GB DDR5
- **OS**: Arch Linux (kernel 6.12)
- **Rust**: 1.83.0 (stable)
- **XGBoost**: 3.0.5 (via xgb crate, C++ backend)

## Models

Generated with `tools/data_generation/scripts/generate_test_cases.py`:

| Model | Trees | Max Depth | Features |
|-------|-------|-----------|----------|
| bench_small | 10 | 6 | 5 |
| bench_medium | 100 | 8 | 50 |
| bench_large | 500 | 10 | 100 |

## Results

### Single-Row Latency (bench_medium model)

| Library | Time | Notes |
|---------|------|-------|
| booste-rs | **858 ns** | Row-by-row traversal |
| XGBoost C++ | 42 µs | Includes DMatrix creation |

**booste-rs is ~49x faster for single-row inference.**

### Batch Prediction (bench_medium model)

| Batch Size | booste-rs | XGBoost C++ | Winner |
|------------|-----------|-------------|--------|
| 100 rows | **97 µs** | 2.7-3.7 ms | booste-rs 28-38x |
| 1,000 rows | 2.24 ms | **584 µs** | XGBoost 3.8x |
| 10,000 rows | 22.4 ms | **3.7 ms** | XGBoost 6x |

### Model Size Scaling (10K rows)

| Model | booste-rs Time |
|-------|----------------|
| Small (10 trees) | 311 µs |
| Medium (100 trees) | 22.3 ms |
| Large (500 trees) | 194 ms |

## Analysis

### Important Methodology Note

**XGBoost caches predictions.** If you call `predict()` twice on the same `DMatrix`, 
the second call returns cached results (~18µs vs ~7ms). Our benchmarks create a fresh 
DMatrix inside each iteration to avoid measuring cached results.

**DMatrix creation overhead is significant:**

| Rows | DMatrix Creation | Total XGBoost Time | Creation % |
|------|------------------|-------------------|------------|
| 1 | 22 µs | 42 µs | **52%** |
| 100 | 44 µs | 2.7 ms | 2% |
| 1,000 | 267 µs | 584 µs | **46%** |
| 10,000 | 2.44 ms | 3.7 ms | **66%** |

This is arguably unfair to XGBoost since real deployments could reuse DMatrix objects.
However, XGBoost's C API has `XGBoosterPredictFromDense()` for "inplace prediction" 
that bypasses DMatrix, but the `xgb` Rust crate doesn't expose it. For now, we accept
this limitation and note that:

1. **For single-row latency**: booste-rs wins regardless (858ns vs 42µs total)
2. **For reusable batch scenarios**: XGBoost's cached predictions would dominate
3. **For fresh-data scenarios**: Our measurements are realistic

Future work could use XGBoost's inplace prediction API or write a custom FFI wrapper.

### Crossover Analysis

booste-rs excels at small batches; XGBoost wins at large batches:

- **< 100 rows**: booste-rs significantly faster (no setup overhead)
- **~500 rows**: Crossover point (approximate)
- **> 1000 rows**: XGBoost faster (SIMD/batching optimizations)

This aligns with our design goals: low-latency single-row inference for real-time applications.

### Why booste-rs wins at small batches

1. **No DMatrix overhead**: XGBoost requires converting data to DMatrix format
2. **Simple hot path**: Direct array traversal with minimal indirection
3. **No thread pool**: Avoid spawning overhead for tiny workloads

### Why XGBoost wins at large batches

1. **SIMD vectorization**: XGBoost uses AVX/SSE for parallel comparisons
2. **Block-based traversal**: Processes multiple rows through same tree simultaneously
3. **Cache optimization**: Designed for batch workloads

### Future Work (M3.4)

Block-based traversal (`BlockVisitor`) should narrow the gap at larger batch sizes
while maintaining our small-batch advantage.

## How to Reproduce

```bash
# Generate benchmark models
cd tools/data_generation
uv run scripts/generate_test_cases.py --benchmarks

# Run all benchmarks
cargo bench

# Run with XGBoost comparison
cargo bench --features bench-xgboost
```
