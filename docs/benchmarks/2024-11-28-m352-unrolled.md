# Benchmark Results: M3.5.2 Predictor Refactor

**Date**: 2024-11-28  
**Milestone**: M3.5.2 Predictor Trait Refactor  
**Rust**: 1.91.1  
**CPU**: (run on developer machine)

## Summary

M3.5.2 unified three predictor implementations into a single `Predictor<T>` generic
over `TreeTraversal`. Key findings:

1. **Blocking alone doesn't help** — `StandardTraversal` with blocking shows no improvement
2. **Unrolling provides 2.9-3.0x speedup** — via level-by-level batch processing
3. **Single-row latency maintained** — ~960ns, still 4x+ faster than XGBoost C++

## Traversal Strategy Comparison

Using `bench_medium` model (100 trees, 50 features):

| Rows | StandardTraversal | StandardTraversal+Block | UnrolledTraversal | Speedup |
|------|-------------------|------------------------|-------------------|---------|
| 100 | 61.6 µs | 60.6 µs (1.02x) | 71.4 µs | 0.86x |
| 1,000 | 2.13 ms | 2.12 ms (1.00x) | 705 µs | **3.0x** |
| 10,000 | 20.9 ms | 21.3 ms (0.98x) | 7.05 ms | **2.97x** |

**Key insight**: The speedup from `UnrolledTraversal` comes from level-by-level
processing where all rows traverse the same tree level together, maximizing
instruction cache reuse. Blocking alone (StandardTraversal) adds slight overhead.

## Batch Size Scaling

Using unified `Predictor` with `StandardTraversal`:

| Batch Size | Time | Throughput |
|------------|------|------------|
| 1 | 1.02 µs | 973K elem/s |
| 10 | 6.63 µs | 1.51M elem/s |
| 100 | 60.5 µs | 1.65M elem/s |
| 1,000 | 2.15 ms | 465K elem/s |
| 10,000 | 21.5 ms | 465K elem/s |

## Model Size Comparison

All using 1,000 rows:

| Model | Trees | Features | Time | Throughput |
|-------|-------|----------|------|------------|
| Small | 10 | 5 | 50.2 µs | 19.9M elem/s |
| Medium | 100 | 50 | 2.12 ms | 471K elem/s |
| Large | 500 | 100 | 13.3 ms | 75K elem/s |

## Single-Row Latency

| Implementation | Time |
|----------------|------|
| booste-rs (StandardTraversal) | 960 ns |
| XGBoost C++ (via xgb crate) | ~4.2 µs |

**Still 4.4x faster** for single-row latency.

## Architecture Changes

### Before (M3.5.1)

- `Predictor` — simple row-at-a-time
- `BlockPredictor` — block-based batching
- `UnrolledPredictor` — unrolled level-by-level

### After (M3.5.2)

- `Predictor<T: TreeTraversal>` — unified predictor
- `StandardTraversal` — per-row traversal (`USES_BLOCK_OPTIMIZATION = false`)
- `UnrolledTraversal<D>` — level-by-level (`USES_BLOCK_OPTIMIZATION = true`)

Code reduction: ~200 lines removed, single code path for batching logic.

## Conclusions

1. **Blocking is not universally beneficial** — only helps when combined with
   level-by-level traversal (UnrolledTraversal)
2. **UnrolledTraversal is the key optimization** — 3x speedup for 1K+ row batches
3. **Compile-time path selection** — `USES_BLOCK_OPTIMIZATION` enables optimal
   code path without runtime overhead
4. **Ready for SIMD** — level-by-level traversal is ideal for SIMD vectorization

## Next Steps

M3.6 SIMD Traversal will add vectorized comparisons to `UnrolledTraversal`,
potentially achieving another 2-4x improvement on batch prediction.
