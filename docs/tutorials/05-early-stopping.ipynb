{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f68d1b",
   "metadata": {},
   "source": [
    "# Tutorial 05: Early Stopping\n",
    "\n",
    "ðŸŸ¡ **Intermediate** â€” Familiarity with ML concepts helpful\n",
    "\n",
    "Learn how to use early stopping to prevent overfitting and find the optimal number of trees.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "1. Understand the overfitting problem\n",
    "2. Use validation-based early stopping\n",
    "3. Visualize training curves\n",
    "4. Choose early stopping parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from boosters.sklearn import GBDTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e44bf",
   "metadata": {},
   "source": [
    "## The Overfitting Problem\n",
    "\n",
    "Without early stopping, adding more trees can lead to overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with some noise\n",
    "X, y = make_regression(n_samples=500, n_features=10, noise=10.0, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85832d4",
   "metadata": {},
   "source": [
    "## Training Without Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with many trees\n",
    "model_no_es = GBDTRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "model_no_es.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, model_no_es.predict(X_train)))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, model_no_es.predict(X_val)))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, model_no_es.predict(X_test)))\n",
    "\n",
    "print(f\"Without early stopping:\")\n",
    "print(f\"  Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"  Val RMSE:   {val_rmse:.4f}\")\n",
    "print(f\"  Test RMSE:  {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93eaee5",
   "metadata": {},
   "source": [
    "## Early Stopping in Action\n",
    "\n",
    "Use early stopping to find the optimal number of iterations.\n",
    "\n",
    "Note: Early stopping support depends on the API version. Here we demonstrate the concept by training multiple models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7dcab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track errors across iterations\n",
    "n_estimators_list = [10, 25, 50, 100, 150, 200, 300, 400, 500]\n",
    "train_errors = []\n",
    "val_errors = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    model = GBDTRegressor(\n",
    "        n_estimators=n_est,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, model.predict(X_train)))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, model.predict(X_val)))\n",
    "    \n",
    "    train_errors.append(train_rmse)\n",
    "    val_errors.append(val_rmse)\n",
    "\n",
    "# Find best iteration\n",
    "best_idx = np.argmin(val_errors)\n",
    "best_n_estimators = n_estimators_list[best_idx]\n",
    "print(f\"Best n_estimators: {best_n_estimators} (val RMSE: {val_errors[best_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd65a8",
   "metadata": {},
   "source": [
    "## Visualize Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234bb7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_list, train_errors, 'b-', label='Train RMSE', marker='o')\n",
    "plt.plot(n_estimators_list, val_errors, 'r-', label='Validation RMSE', marker='s')\n",
    "plt.axvline(x=best_n_estimators, color='g', linestyle='--', label=f'Best: {best_n_estimators}')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Training vs Validation Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcec7ca",
   "metadata": {},
   "source": [
    "## Train with Optimal Number of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa9305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with optimal n_estimators\n",
    "model_optimal = GBDTRegressor(\n",
    "    n_estimators=best_n_estimators,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "model_optimal.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test\n",
    "test_rmse_optimal = np.sqrt(mean_squared_error(y_test, model_optimal.predict(X_test)))\n",
    "\n",
    "print(f\"With optimal trees ({best_n_estimators}):\")\n",
    "print(f\"  Test RMSE: {test_rmse_optimal:.4f}\")\n",
    "print(f\"\\nImprovement over 500 trees: {test_rmse - test_rmse_optimal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26efc4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. âœ… More trees doesn't always mean better performance\n",
    "2. âœ… How to find optimal number of iterations using validation data\n",
    "3. âœ… How to visualize train/validation curves\n",
    "4. âœ… The importance of a held-out validation set\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Tutorial 06: GBLinear & Sparse Data](06-gblinear-sparse.ipynb) â€” Linear boosting\n",
    "- [Tutorial 07: Hyperparameter Tuning](07-hyperparameter-tuning.ipynb) â€” Systematic optimization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
