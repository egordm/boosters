{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb5728d",
   "metadata": {},
   "source": [
    "# Tutorial 11: Sample Weighting\n",
    "\n",
    "ðŸŸ¡ **Intermediate** â€” Familiarity with ML concepts helpful\n",
    "\n",
    "Learn how to use sample weights to handle class imbalance, emphasize recent data, and downweight outliers.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "1. Handle class imbalance with balanced sample weights\n",
    "2. Use temporal weighting for concept drift\n",
    "3. Downweight outliers and noisy samples\n",
    "4. Integrate sample weights with both sklearn and core APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be841e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "import boosters\n",
    "from boosters.sklearn import GBDTClassifier, GBDTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b0a37",
   "metadata": {},
   "source": [
    "## Why Sample Weights?\n",
    "\n",
    "Sample weights allow you to control how much each training example contributes to the model:\n",
    "\n",
    "- **Class imbalance**: Give minority class samples higher weights\n",
    "- **Temporal importance**: Weight recent data more heavily for concept drift\n",
    "- **Data quality**: Downweight known outliers or noisy samples\n",
    "- **Business logic**: Emphasize high-value customers or critical cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb8cda",
   "metadata": {},
   "source": [
    "## 1. Handling Class Imbalance\n",
    "\n",
    "When classes are imbalanced, the model tends to favor the majority class.\n",
    "Sample weights can correct this by giving minority samples more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced dataset (95% class 0, 5% class 1)\n",
    "rng = np.random.default_rng(42)\n",
    "n_samples = 1000\n",
    "n_minority = 50\n",
    "\n",
    "X = rng.standard_normal((n_samples, 5)).astype(np.float32)\n",
    "\n",
    "# Create imbalanced labels\n",
    "y = np.zeros(n_samples, dtype=np.float32)\n",
    "y[:n_minority] = 1.0\n",
    "\n",
    "# Shuffle\n",
    "perm = rng.permutation(n_samples)\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Class distribution in training: {np.bincount(y_train.astype(int))}\")\n",
    "print(f\"Class distribution in test:     {np.bincount(y_test.astype(int))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34838036",
   "metadata": {},
   "source": [
    "### Without Sample Weights\n",
    "\n",
    "The model will likely predict only the majority class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_no_weights = GBDTClassifier(n_estimators=50, verbose=0)\n",
    "clf_no_weights.fit(X_train, y_train)\n",
    "pred_no_weights = clf_no_weights.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (no weights):\")\n",
    "print(classification_report(y_test, pred_no_weights, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e1766",
   "metadata": {},
   "source": [
    "### With Balanced Sample Weights\n",
    "\n",
    "sklearn's `compute_sample_weight` calculates weights inversely proportional to class frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078056d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute balanced weights\n",
    "weights = compute_sample_weight(\"balanced\", y_train)\n",
    "\n",
    "print(f\"Weight for class 0 samples: {weights[y_train == 0][0]:.3f}\")\n",
    "print(f\"Weight for class 1 samples: {weights[y_train == 1][0]:.3f}\")\n",
    "print(f\"Ratio: {weights[y_train == 1][0] / weights[y_train == 0][0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21109d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_weighted = GBDTClassifier(n_estimators=50, verbose=0)\n",
    "clf_weighted.fit(X_train, y_train, sample_weight=weights)\n",
    "pred_weighted = clf_weighted.predict(X_test)\n",
    "\n",
    "print(\"Classification Report (with balanced weights):\")\n",
    "print(classification_report(y_test, pred_weighted, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995b172",
   "metadata": {},
   "source": [
    "## 2. Temporal Weighting for Concept Drift\n",
    "\n",
    "When data distribution changes over time (concept drift), you may want to emphasize recent observations.\n",
    "This is common in:\n",
    "- Financial markets\n",
    "- User behavior prediction\n",
    "- Fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate temporal data with concept drift\n",
    "rng = np.random.default_rng(42)\n",
    "n_samples = 500\n",
    "\n",
    "X = rng.standard_normal((n_samples, 3)).astype(np.float32)\n",
    "\n",
    "# Target relationship changes over time:\n",
    "# Early: y depends mostly on X[:, 0]\n",
    "# Late: y depends mostly on X[:, 1]\n",
    "time_factor = np.linspace(0, 1, n_samples)\n",
    "y = ((1 - time_factor) * X[:, 0] + time_factor * X[:, 1]).astype(np.float32)\n",
    "y += rng.standard_normal(n_samples).astype(np.float32) * 0.1\n",
    "\n",
    "# Split chronologically (not randomly!)\n",
    "train_size = 400\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Training on first {train_size} samples\")\n",
    "print(f\"Testing on last {n_samples - train_size} samples (most recent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bf9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform weights baseline\n",
    "reg_uniform = GBDTRegressor(n_estimators=50, verbose=0)\n",
    "reg_uniform.fit(X_train, y_train)\n",
    "pred_uniform = reg_uniform.predict(X_test)\n",
    "rmse_uniform = np.sqrt(np.mean((pred_uniform - y_test) ** 2))\n",
    "\n",
    "# Exponential decay weights - recent samples have higher weight\n",
    "decay_rate = 3.0  # Higher = more emphasis on recent data\n",
    "temporal_weights = np.exp(decay_rate * np.linspace(0, 1, train_size)).astype(np.float32)\n",
    "\n",
    "reg_weighted = GBDTRegressor(n_estimators=50, verbose=0)\n",
    "reg_weighted.fit(X_train, y_train, sample_weight=temporal_weights)\n",
    "pred_weighted = reg_weighted.predict(X_test)\n",
    "rmse_weighted = np.sqrt(np.mean((pred_weighted - y_test) ** 2))\n",
    "\n",
    "print(f\"RMSE with uniform weights:   {rmse_uniform:.4f}\")\n",
    "print(f\"RMSE with temporal weights:  {rmse_weighted:.4f}\")\n",
    "print(f\"Improvement: {(rmse_uniform - rmse_weighted) / rmse_uniform * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b118e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot weight distribution\n",
    "ax1.plot(temporal_weights)\n",
    "ax1.set_xlabel(\"Sample index (time)\")\n",
    "ax1.set_ylabel(\"Weight\")\n",
    "ax1.set_title(\"Exponential Temporal Weights\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot predictions vs actual\n",
    "ax2.scatter(y_test, pred_uniform, alpha=0.5, label=f\"Uniform (RMSE={rmse_uniform:.3f})\")\n",
    "ax2.scatter(y_test, pred_weighted, alpha=0.5, label=f\"Temporal (RMSE={rmse_weighted:.3f})\")\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\")\n",
    "ax2.set_xlabel(\"Actual\")\n",
    "ax2.set_ylabel(\"Predicted\")\n",
    "ax2.set_title(\"Predictions on Test Set\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464ba3d",
   "metadata": {},
   "source": [
    "## 3. Downweighting Outliers\n",
    "\n",
    "If you know certain samples are outliers or noisy, you can reduce their influence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a930a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "X = rng.standard_normal((200, 4)).astype(np.float32)\n",
    "y = (X[:, 0] + X[:, 1] * 0.5 + rng.standard_normal(200) * 0.1).astype(np.float32)\n",
    "\n",
    "# Inject some outliers\n",
    "outlier_idx = [10, 50, 100, 150]\n",
    "y[outlier_idx] = y[outlier_idx] + 10.0  # Shift by 10 standard deviations\n",
    "\n",
    "# Create weights - downweight outliers\n",
    "weights = np.ones(200, dtype=np.float32)\n",
    "weights[outlier_idx] = 0.01  # Very low weight for known outliers\n",
    "\n",
    "print(f\"Number of outliers: {len(outlier_idx)}\")\n",
    "print(f\"Outlier weight: {weights[outlier_idx[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without outlier handling\n",
    "reg_no_handling = GBDTRegressor(n_estimators=50, verbose=0)\n",
    "reg_no_handling.fit(X, y)\n",
    "pred_no_handling = reg_no_handling.predict(X)\n",
    "\n",
    "# With outlier downweighting\n",
    "reg_downweight = GBDTRegressor(n_estimators=50, verbose=0)\n",
    "reg_downweight.fit(X, y, sample_weight=weights)\n",
    "pred_downweight = reg_downweight.predict(X)\n",
    "\n",
    "# Calculate RMSE on non-outlier samples only\n",
    "mask = ~np.isin(np.arange(len(y)), outlier_idx)\n",
    "rmse_no = np.sqrt(np.mean((pred_no_handling[mask] - y[mask]) ** 2))\n",
    "rmse_dw = np.sqrt(np.mean((pred_downweight[mask] - y[mask]) ** 2))\n",
    "\n",
    "print(f\"RMSE on clean samples (no handling):    {rmse_no:.4f}\")\n",
    "print(f\"RMSE on clean samples (with weights):   {rmse_dw:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd22d28",
   "metadata": {},
   "source": [
    "## 4. Core API with Sample Weights\n",
    "\n",
    "You can also use sample weights directly with the core `Dataset` and `GBDTModel` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82002bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boosters import Dataset, GBDTConfig, GBDTModel, Objective\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "X = rng.standard_normal((200, 4)).astype(np.float32)\n",
    "y = (X[:, 0] + X[:, 1] * 0.5 + rng.standard_normal(200) * 0.1).astype(np.float32)\n",
    "\n",
    "# Create custom weights\n",
    "weights = np.ones(200, dtype=np.float32)\n",
    "weights[np.abs(y) > 2] = 0.1  # Downweight extreme values\n",
    "\n",
    "print(f\"Samples with reduced weight: {np.sum(weights < 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset WITH weights\n",
    "train_data = Dataset(\n",
    "    X, y,\n",
    "    weights=weights,\n",
    "    feature_names=[\"feat_a\", \"feat_b\", \"feat_c\", \"feat_d\"]\n",
    ")\n",
    "\n",
    "config = GBDTConfig(\n",
    "    n_estimators=50,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    objective=Objective.squared(),\n",
    ")\n",
    "\n",
    "model = GBDTModel.train(train_data, config=config)\n",
    "predictions = model.predict(Dataset(X))\n",
    "\n",
    "rmse = np.sqrt(np.mean((predictions.flatten() - y) ** 2))\n",
    "print(f\"Training RMSE: {rmse:.4f}\")\n",
    "print(f\"Feature names stored in model: {model.feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9163a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Sample weights are a powerful tool for:\n",
    "\n",
    "| Use Case | Weight Strategy |\n",
    "|----------|----------------|\n",
    "| Class imbalance | Inverse class frequency (`compute_sample_weight(\"balanced\")`) |\n",
    "| Concept drift | Exponential decay (recent â†’ higher weight) |\n",
    "| Known outliers | Low fixed weight (e.g., 0.01) |\n",
    "| Business importance | Custom weights per sample |\n",
    "\n",
    "### Key Points\n",
    "\n",
    "1. **sklearn API**: Pass `sample_weight` to `.fit()`\n",
    "2. **Core API**: Pass `weights` to `Dataset()`\n",
    "3. Weights are relative â€” they don't need to sum to 1\n",
    "4. Use `compute_sample_weight(\"balanced\")` for quick class balancing\n",
    "5. For temporal weighting, use exponential decay: `exp(rate * normalized_time)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ea51a2",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- [Tutorial 07: Hyperparameter Tuning](07-hyperparameter-tuning.ipynb) â€” Optimize model performance\n",
    "- [Tutorial 08: Explainability](08-explainability.ipynb) â€” Understand model predictions\n",
    "- [User Guide: Recipes](../user-guide/recipes.rst) â€” Common patterns and solutions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
