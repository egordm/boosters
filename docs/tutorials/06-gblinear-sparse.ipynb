{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3f2869",
   "metadata": {},
   "source": [
    "# Tutorial 06: GBLinear & Sparse Data\n",
    "\n",
    "ðŸŸ¡ **Intermediate** â€” Familiarity with ML concepts helpful\n",
    "\n",
    "Learn how to use GBLinear for linear gradient boosting, especially useful for high-dimensional sparse data.\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "1. When to use GBLinear vs GBDT\n",
    "2. Train a GBLinear model\n",
    "3. Access linear coefficients\n",
    "4. Work with sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from boosters.sklearn import GBLinearRegressor, GBDTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc109717",
   "metadata": {},
   "source": [
    "## GBDT vs GBLinear\n",
    "\n",
    "| Aspect | GBDT | GBLinear |\n",
    "|--------|------|----------|\n",
    "| Relationships | Non-linear | Linear only |\n",
    "| Inference | O(trees Ã— depth) | O(features) |\n",
    "| Sparse data | OK | Excellent |\n",
    "| Interpretability | Feature importance | Direct coefficients |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc35a6",
   "metadata": {},
   "source": [
    "## Generate Linear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aedcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with linear relationships\n",
    "X, y, coef = make_regression(\n",
    "    n_samples=1000,\n",
    "    n_features=50,\n",
    "    n_informative=10,  # Only 10 features are actually useful\n",
    "    noise=5.0,\n",
    "    coef=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Features: {X.shape[1]}, Informative: 10\")\n",
    "print(f\"True coefficients (non-zero): {np.sum(coef != 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47532242",
   "metadata": {},
   "source": [
    "## Train GBLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GBLinear model\n",
    "model_linear = GBLinearRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.5,\n",
    "    l2=0.1,  # L2 regularization (lambda)\n",
    ")\n",
    "model_linear.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_linear = model_linear.predict(X_test)\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_test, y_pred_linear))\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "print(f\"GBLinear Performance:\")\n",
    "print(f\"  RMSE: {rmse_linear:.4f}\")\n",
    "print(f\"  RÂ²:   {r2_linear:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d4b65",
   "metadata": {},
   "source": [
    "## Compare with GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GBDT for comparison\n",
    "model_tree = GBDTRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = model_tree.predict(X_test)\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
    "r2_tree = r2_score(y_test, y_pred_tree)\n",
    "\n",
    "print(f\"\\nGBDT Performance:\")\n",
    "print(f\"  RMSE: {rmse_tree:.4f}\")\n",
    "print(f\"  RÂ²:   {r2_tree:.4f}\")\n",
    "\n",
    "print(f\"\\nFor linear data, GBLinear is {'better' if rmse_linear < rmse_tree else 'comparable'}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1e1cb",
   "metadata": {},
   "source": [
    "## Access Linear Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103145e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learned coefficients\n",
    "learned_coef = model_linear.coef_\n",
    "intercept = model_linear.intercept_\n",
    "\n",
    "print(f\"Intercept: {float(intercept[0]):.4f}\")\n",
    "print(f\"Coefficients shape: {learned_coef.shape}\")\n",
    "print(f\"\\nTop 5 coefficients (by magnitude):\")\n",
    "top_indices = np.argsort(np.abs(learned_coef))[-5:]\n",
    "for idx in reversed(top_indices):\n",
    "    print(f\"  Feature {idx}: {learned_coef[idx]:.4f} (true: {coef[idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fff8ac",
   "metadata": {},
   "source": [
    "## Compare Learned vs True Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431aef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coefficient recovery\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(coef)), coef, alpha=0.7)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('True Coefficients')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(learned_coef)), learned_coef, alpha=0.7, color='orange')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Learned Coefficients')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between true and learned\n",
    "correlation = np.corrcoef(coef, learned_coef)[0, 1]\n",
    "print(f\"Correlation between true and learned coefficients: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d694a0",
   "metadata": {},
   "source": [
    "## Working with Sparse Data\n",
    "\n",
    "GBLinear handles sparse data efficiently. The sklearn wrapper accepts scipy sparse matrices and converts them internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dcd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse data (90% zeros)\n",
    "X_sparse = X.copy()\n",
    "mask = np.random.random(X_sparse.shape) < 0.9\n",
    "X_sparse[mask] = 0\n",
    "X_sparse_csr = sparse.csr_matrix(X_sparse)\n",
    "\n",
    "print(f\"Dense shape: {X_sparse.shape}\")\n",
    "print(f\"Sparsity: {100 * mask.sum() / mask.size:.1f}%\")\n",
    "print(f\"Memory: {X_sparse.nbytes / 1e6:.2f} MB (dense) vs {(X_sparse_csr.data.nbytes + X_sparse_csr.indices.nbytes + X_sparse_csr.indptr.nbytes) / 1e6:.2f} MB (sparse)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBLinear handles sparse data - convert to dense for sklearn wrapper\n",
    "# (the underlying binned representation is memory-efficient)\n",
    "X_train_sp = sparse.csr_matrix(X_train).toarray()  # Convert to dense\n",
    "X_test_sp = sparse.csr_matrix(X_test).toarray()\n",
    "\n",
    "model_sparse = GBLinearRegressor(n_estimators=100, learning_rate=0.5)\n",
    "model_sparse.fit(X_train_sp, y_train)\n",
    "\n",
    "y_pred_sparse = model_sparse.predict(X_test_sp)\n",
    "rmse_sparse = np.sqrt(mean_squared_error(y_test, y_pred_sparse))\n",
    "\n",
    "print(f\"Sparse-to-dense input RMSE: {rmse_sparse:.4f}\")\n",
    "print(\"Note: Internal binned storage is efficient for sparse patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b06b98",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. âœ… When to choose GBLinear over GBDT\n",
    "2. âœ… How to train and use GBLinear\n",
    "3. âœ… How to access and interpret linear coefficients\n",
    "4. âœ… How GBLinear works with sparse data\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- [Tutorial 07: Hyperparameter Tuning](07-hyperparameter-tuning.ipynb) â€” Optimize your models\n",
    "- [Tutorial 08: Explainability](08-explainability.ipynb) â€” Interpret model predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
