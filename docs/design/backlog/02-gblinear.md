# Epic: GBLinear Support

**Status**: ğŸ”„ Active (feature parity work)
**Priority**: High  
**RFCs**: [0008](../rfcs/0008-gblinear-inference.md), [0009](../rfcs/0009-gblinear-training.md)

Add support for XGBoost's linear booster â€” both inference and training.
Validates training infrastructure before GBTree training.

---

## Completed Stories

### Story 1: GBLinear Inference âœ“

- [x] 1.1 `LinearModel` struct with `Box<[f32]>` weight storage
- [x] 1.2 Weight indexing: `weights[feature * num_groups + group]`
- [x] 1.3 `predict_row()` and `predict_batch()` methods
- [x] 1.4 `par_predict_batch()` with Rayon
- [x] 1.5 `Booster::Linear` variant in model enum
- [x] 1.6 XGBoost JSON parser for `gblinear` section
- [x] 1.7 Integration tests vs Python XGBoost

### Story 2: Training Infrastructure âœ“

- [x] 2.1 `GradientPair` struct (grad, hess)
- [x] 2.2 `Loss` trait â€” compute gradients from predictions + labels
- [x] 2.3 Common losses: squared error, logistic, softmax
- [x] 2.4 `Metric` trait â€” evaluate model quality
- [x] 2.5 Common metrics: RMSE, MAE, logloss, AUC, accuracy
- [x] 2.6 `EarlyStopping` callback
- [x] 2.7 `TrainingLogger` with verbosity levels

### Story 3: GBLinear Training âœ“

- [x] 3.1 `CSCMatrix` â€” column-sparse format for efficient column access
- [x] 3.2 `CSCMatrix::from_dense()` and column iteration
- [x] 3.3 Coordinate descent update with elastic net regularization
- [x] 3.4 Parallel updater â€” all features with stale gradients (default)
- [x] 3.5 Sequential updater â€” features in order with stale gradients
- [x] 3.6 `CyclicSelector` and `ShuffleSelector` for feature order
- [x] 3.7 `LinearTrainer` high-level API
- [x] 3.8 Integration tests comparing to XGBoost

### Story 4: Matrix Layout Refactor âœ“

- [x] 4.1 Add `Layout` trait with `RowMajor` and `ColMajor` implementations
- [x] 4.2 Refactor `DenseMatrix` to `DenseMatrix<T, L: Layout = RowMajor>`
- [x] 4.3-4.8 Full layout support with conversions and iterators

### Story 5: Training Validation âœ“

- [x] 5.1-5.5 Full validation vs XGBoost (weight correlation > 0.9, good test RMSE)

### Story 6: Benchmarks & Optimization âœ“

- [x] 6.1-6.6 Performance validated and documented

---

## Active Stories (Feature Parity)

### Story 7: Fix Multiclass Training ğŸ”´ HIGH

**Goal**: Multiclass classification currently broken â€” all groups get identical gradients.

**Problem**: In `LinearTrainer::compute_gradients()`, we use the same gradient
for all output groups instead of per-class softmax gradients.

- [ ] 7.1 Update `compute_gradients` to handle multiclass properly
- [ ] 7.2 Use `SoftmaxLoss::compute_multiclass_gradient()` for each sample
- [ ] 7.3 Store gradients per (sample, class) pair
- [ ] 7.4 Update each group's weights with group-specific gradients
- [ ] 7.5 Enable `train_multiclass_classification` test
- [ ] 7.6 Validate vs XGBoost multiclass

---

### Story 8: Quantile Regression ğŸŸ¡ MEDIUM

**Goal**: Add quantile loss for uncertainty quantification.

Pinball loss: `L = Î±(y-Å·)âº + (1-Î±)(Å·-y)âº`

- [ ] 8.1 Implement `QuantileLoss` with configurable Î±
- [ ] 8.2 Gradient: `grad = (1-Î±) if pred >= label else -Î±`
- [ ] 8.3 Add Python test case generation for quantile regression
- [ ] 8.4 Integration test vs XGBoost `reg:quantileerror`
- [ ] 8.5 Document multi-quantile training (use num_groups = num_quantiles)

---

### Story 9: Additional Loss Functions ğŸŸ¢ LOW

**Goal**: Add commonly used loss functions for feature parity.

- [ ] 9.1 `HuberLoss` â€” robust regression (grad clipped for large residuals)
- [ ] 9.2 `HingeLoss` â€” SVM-style binary classification
- [ ] 9.3 `PseudoHuberLoss` â€” smooth approximation of Huber
- [ ] 9.4 Integration tests for each

---

### Story 10: Additional Feature Selectors ğŸŸ¢ LOW

**Goal**: XGBoost-compatible feature selection strategies.

- [ ] 10.1 `GreedySelector` â€” select feature with largest gradient magnitude
- [ ] 10.2 `ThriftySelector` â€” approximate greedy (sort by magnitude, iterate)
- [ ] 10.3 `RandomSelector` â€” with replacement
- [ ] 10.4 Benchmark feature selector impact

---

## Feature Parity Checklist

### Loss Functions

| Objective | XGBoost | booste-rs | Story |
|-----------|---------|-----------|-------|
| `reg:squarederror` | âœ… | âœ… | Done |
| `reg:quantileerror` | âœ… | âŒ | 8 |
| `reg:pseudohubererror` | âœ… | âŒ | 9 |
| `binary:logistic` | âœ… | âœ… | Done |
| `binary:hinge` | âœ… | âŒ | 9 |
| `multi:softmax` | âœ… | âš ï¸ Broken | 7 |

### Feature Selectors

| Selector | XGBoost | booste-rs | Story |
|----------|---------|-----------|-------|
| Cyclic | âœ… | âœ… | Done |
| Shuffle | âœ… | âœ… | Done |
| Greedy | âœ… | âŒ | 10 |
| Thrifty | âœ… | âŒ | 10 |
| Random | âœ… | âŒ | 10 |

---

## Success Criteria

1. âœ… Load XGBoost GBLinear JSON models and predict correctly
2. âœ… Train models matching Python XGBoost quality (metrics within 5%)
3. âœ… Training performance equal to or faster than XGBoost
4. âœ… Training infrastructure (losses, metrics, callbacks) is reusable
5. âœ… Early stopping and logging work correctly
6. âœ… Trained model predictions correlate highly with XGBoost predictions
7. â¬œ Multiclass classification works correctly
8. â¬œ Quantile regression supported
