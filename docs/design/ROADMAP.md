# booste-rs Roadmap

## Philosophy

**Slice-wise implementation**: Build thin vertical slices that work end-to-end, then expand.

**Guiding principle**: At each milestone, we should be able to load a real model and produce correct predictions.

---

## Current Focus

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GBTree Inference                       âœ… COMPLETE             â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•             â”‚
â”‚  Load XGBoost JSON models, predict with 3x+ speedup vs C++      â”‚
â”‚                                                                  â”‚
â”‚  GBLinear Support                       â—„â”€â”€ ACTIVE              â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                           â”‚
â”‚  [ ] Story 1: GBLinear Inference                                â”‚
â”‚  [ ] Story 2: Training Infrastructure                           â”‚
â”‚  [ ] Story 3: GBLinear Training                                 â”‚
â”‚  [ ] Story 4: Benchmarks                                        â”‚
â”‚                                                                  â”‚
â”‚  Future (backlog)                                                â”‚
â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                                                â”‚
â”‚  - Sparse data, LightGBM, GBTree training, Python bindings      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Epics

| Epic | Status | Summary |
|------|--------|---------|
| [GBTree Inference](backlog/01-gbtree-inference.md) | âœ… Complete | Tree inference, 3x faster than XGBoost C++ |
| [GBLinear](backlog/02-gblinear.md) | ğŸ”„ Active | Linear booster inference + training |
| [Future](backlog/99-future.md) | ğŸ“‹ Backlog | Sparse data, LightGBM, bindings, etc. |

---

## Performance Summary

Current benchmark results (Apple M1 Pro, vs XGBoost C++):

| Metric | booste-rs | XGBoost C++ | Speedup |
|--------|-----------|-------------|---------|
| Single-row latency | 1.24Âµs | 11.6Âµs | **9.4x** |
| 10K batch (8 threads) | 1.58ms | 5.0ms | **3.2x** |

See [benchmarks](../benchmarks/) for details.

---

## RFCs

| RFC | Status | Topic |
|-----|--------|-------|
| [0001](rfcs/0001-forest-data-structures.md) | Implemented | Forest structures |
| [0002](rfcs/0002-tree-data-structures.md) | Implemented | Tree structures |
| [0003](rfcs/0003-visitor-and-traversal.md) | Implemented | Traversal & prediction |
| [0004](rfcs/0004-dmatrix.md) | Implemented | Data input |
| [0007](rfcs/0007-serialization.md) | Active | XGBoost loading |
| [0008](rfcs/0008-gblinear-inference.md) | Approved | Linear inference |
| [0009](rfcs/0009-gblinear-training.md) | Approved | Linear training |

---

## Test Data

Reference predictions generated from Python XGBoost:

```bash
cd tools/data_generation && uv run python scripts/generate_test_cases.py
```

---

## Notes

- **Don't over-engineer early**: Get something working, then refactor
- **Test against Python**: Every story should have validation tests
- **Feature flags**: Keep optional stuff behind features
- **Document as you go**: Update RFCs if implementation diverges
